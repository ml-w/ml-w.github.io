<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.537">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="MLun Wong">
<meta name="dcterms.date" content="2024-02-04">

<title>Computer vision, biostatistics, and AI in Radiology - LVM - General Info</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<link href="../../../../posts/deep-learning/multi-modal/base-modules/02-CLIP.html" rel="next">
<link href="../../../../posts/deep-learning/explainable-ai/01-xai-npc-bn.html" rel="prev">
<link href="../../../../favicon.png" rel="icon" type="image/png">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="dark">
<link href="../../../../site_libs/bootstrap/bootstrap-dark.min.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script src="../../../../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../../../../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="../../../../site_libs/quarto-diagram/mermaid.min.js"></script>
<script src="../../../../site_libs/quarto-diagram/mermaid-init.js"></script>
<link href="../../../../site_libs/quarto-diagram/mermaid.css" rel="stylesheet">
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      // ...your configuration options...
      "HTML-CSS": { fonts: ["TeX"] }
    });
</script>
    
<link href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&amp;display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css2?family=Georgia:wght@400;700&amp;display=swap" rel="stylesheet">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../../../styles.css">
</head>

<body class="nav-sidebar floating nav-fixed slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../../index.html">
    <span class="navbar-title">Computer vision, biostatistics, and AI in Radiology</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link active" href="../../../../about.html" aria-current="page"> 
<span class="menu-text">About Me</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/ml-w"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://scholar.google.com/citations?user=JNKeB6cAAAAJ&amp;hl=en"> <i class="bi bi-mortarboard" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="http://www.diir.cuhk.edu.hk/profile/mr-wong-lun-matthew"> <i class="bi bi-person-circle" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item">Posts</li><li class="breadcrumb-item"><a href="../../../../posts/deep-learning/explainable-ai/index.html">Deep Learning</a></li><li class="breadcrumb-item">Multi Modal</li><li class="breadcrumb-item"><a href="../../../../posts/deep-learning/multi-modal/base-modules/01-general-info.html">Base Modules</a></li><li class="breadcrumb-item"><a href="../../../../posts/deep-learning/multi-modal/base-modules/01-general-info.html">LVM - General Info</a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../about.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About Me</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Posts</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
 <span class="menu-text">Deep Learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth2 show">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../../posts/deep-learning/explainable-ai/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Explainable AI (XAI)</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth3 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../posts/deep-learning/explainable-ai/01-xai-npc-bn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Explainable artificial intelligence for automatic detection of early nasopharyngeal carcinoma on MRI</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
 <span class="menu-text">Multi Modal</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth3 show">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
 <span class="menu-text">Base Modules</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth4 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../posts/deep-learning/multi-modal/base-modules/01-general-info.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">LVM - General Info</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../posts/deep-learning/multi-modal/base-modules/02-CLIP.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">LVM - CLIP</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../posts/deep-learning/multi-modal/base-modules/03-BLIP.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">LVM - BLIP-1 &amp; BLIP-2</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="false">
 <span class="menu-text">Hallucinations</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth4 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../posts/deep-learning/multi-modal/hallucinations/01_hallucination.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Narrative review - Reducing hallucination in MMLLM</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../posts/deep-learning/multi-modal/hallucinations/02_woodpecker.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">[Review] Reducing hallucination: Woodpecker</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="false">
 <span class="menu-text">Torch</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth3 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../posts/deep-learning/torch/torchio-ddp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">PyTorch DDP with torchio</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" aria-expanded="false">
 <span class="menu-text">Medical Imaging</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../posts/medical-imaging/feature-extraction-from-image.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Texture analysis of medical images</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../posts/medical-imaging/image-orientation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Medical imaging orientation in VTK and ITK</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../posts/medical-imaging/imaging-mri.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">MRI - Spin relaxation simulation</span></a>
  </div>
</li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" aria-expanded="false">
 <span class="menu-text">Intensity Normalization</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth3 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../posts/medical-imaging/intensity-normalization/01_intensity-normalization-toc.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Intensity normalization (Part 1) - General Info</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../posts/medical-imaging/intensity-normalization/02_intensity-normalization-mri.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Intensity normalization (Part 2) - MRI</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" aria-expanded="false">
 <span class="menu-text">MISC</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-10" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../posts/MISC/headless-server.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Setting up a headless server</span></a>
  </div>
</li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../../../../posts/MISC/fourier-transform/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Imaging - Convolution between an image and a filtering kernel</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-11" class="collapse list-unstyled sidebar-section depth3 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../posts/MISC/fourier-transform/discrete-fourier-transform.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Discrete Fourier Transform</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" aria-expanded="false">
 <span class="menu-text">Stochastic</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-12" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../posts/stochastic/incremental-process.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Stochastic - Python Example of a Random Walk Process</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../posts/stochastic/monte-carlo.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Stochastic - Particle Filtering and Markov Chain Monte Carlo</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../posts/stochastic/random-variables-and-distribution.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Random variables, Probability and Stochastic system</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../../posts/stochastic/simple-random-walk.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Stochastic - Python Example of a Random Walk Process</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#background" id="toc-background" class="nav-link active" data-scroll-target="#background">Background</a></li>
  <li><a href="#transformer" id="toc-transformer" class="nav-link" data-scroll-target="#transformer">Transformer</a>
  <ul class="collapse">
  <li><a href="#self-attention-mechanism" id="toc-self-attention-mechanism" class="nav-link" data-scroll-target="#self-attention-mechanism">Self attention mechanism</a></li>
  <li><a href="#multi-head-attention" id="toc-multi-head-attention" class="nav-link" data-scroll-target="#multi-head-attention">Multi-head attention</a>
  <ul class="collapse">
  <li><a href="#self-attention-sa" id="toc-self-attention-sa" class="nav-link" data-scroll-target="#self-attention-sa">Self-attention (SA)</a></li>
  <li><a href="#cross-attention-ca" id="toc-cross-attention-ca" class="nav-link" data-scroll-target="#cross-attention-ca">Cross-attention (CA)</a></li>
  </ul></li>
  <li><a href="#postion-encoding" id="toc-postion-encoding" class="nav-link" data-scroll-target="#postion-encoding">Postion encoding</a></li>
  </ul></li>
  <li><a href="#bert" id="toc-bert" class="nav-link" data-scroll-target="#bert">BERT</a>
  <ul class="collapse">
  <li><a href="#purpose-of-bert" id="toc-purpose-of-bert" class="nav-link" data-scroll-target="#purpose-of-bert">Purpose of BERT</a></li>
  <li><a href="#application-from-bert" id="toc-application-from-bert" class="nav-link" data-scroll-target="#application-from-bert">Application from BERT</a></li>
  </ul></li>
  <li><a href="#gpt-2" id="toc-gpt-2" class="nav-link" data-scroll-target="#gpt-2">GPT-2</a>
  <ul class="collapse">
  <li><a href="#training-gpt" id="toc-training-gpt" class="nav-link" data-scroll-target="#training-gpt">Training GPT</a></li>
  <li><a href="#modifications-in-gpt-2" id="toc-modifications-in-gpt-2" class="nav-link" data-scroll-target="#modifications-in-gpt-2">Modifications in GPT-2</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item">Posts</li><li class="breadcrumb-item"><a href="../../../../posts/deep-learning/explainable-ai/index.html">Deep Learning</a></li><li class="breadcrumb-item">Multi Modal</li><li class="breadcrumb-item"><a href="../../../../posts/deep-learning/multi-modal/base-modules/01-general-info.html">Base Modules</a></li><li class="breadcrumb-item"><a href="../../../../posts/deep-learning/multi-modal/base-modules/01-general-info.html">LVM - General Info</a></li></ol></nav>
<div class="quarto-title">
<h1 class="title">LVM - General Info</h1>
  <div class="quarto-categories">
    <div class="quarto-category">transformer</div>
    <div class="quarto-category">bert</div>
    <div class="quarto-category">NLP</div>
    <div class="quarto-category">review</div>
    <div class="quarto-category">lvm</div>
  </div>
  </div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>MLun Wong </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">February 4, 2024</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="background" class="level1">
<h1>Background</h1>
<p>Language-visual model (LVM) has quite a general scope covering any model that deal with both language and visual information, but let’s not complicate things and only look at the possible combination of outputs:</p>
<ul>
<li>Text to image ➠ E.g., Stable diffusion</li>
<li>Image to text ➠ E.g., Imaging captioning</li>
<li><del>Text to Image + text</del></li>
<li><del>Image to Image + text</del></li>
<li>Text + Image to Text ➠ E.g., Radiology reporting.</li>
<li>Text + Image to Image ➠ E.g., Stable diffusion, Animation, Style changes.</li>
</ul>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p>Two combinations was crossed out just because no compelling examples came to my mind, but I am sure there’s some applications out there that fits the profile.</p>
</div>
</div>
</div>
<p>To understand how these applications were achieved, some milestone modules needs to be properly introduced, which is the purpose of this post.</p>
<hr>
</section>
<section id="transformer" class="level1 page-columns page-full">
<h1>Transformer</h1>
<p>Transformer is probably one of the most important milestone in modern language AI models, proposed by scientist from Google <span class="citation" data-cites="vaswani2017">(<a href="#ref-vaswani2017" role="doc-biblioref">Vaswani et al. 2017</a>)</span>. It can be modulised into an encoder and a decoder, which is essentially stacks of linear units with non-linear activations. Now there’s tons of tutorials out there covering transformer, so I am not going to repeat all of the details. Instead, I plan only to cover a few items and point you to one that I think is pretty good <a href="https://www.youtube.com/watch?v=4Bdc55j80l8">here</a>.</p>

<div class="no-row-height column-margin column-container"><div class="">
<div id="fig-transformer" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-transformer-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/transformer-struct.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" data-glightbox="description: .lightbox-desc-1"><img src="images/transformer-struct.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-transformer-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Architecture of transformer extracted from <span class="citation" data-cites="vaswani2017">(<a href="#ref-vaswani2017" role="doc-biblioref">Vaswani et al. 2017</a>)</span>.
</figcaption>
</figure>
</div>
</div></div><section id="self-attention-mechanism" class="level2">
<h2 class="anchored" data-anchor-id="self-attention-mechanism">Self attention mechanism</h2>
<p>At the heart of the Transformer is the <strong>self-attention mechanism</strong>, which computes attention scores to capture the relevance of all other tokens in the input sequence for each token. Considering the sentence “<em>I am a boy</em>,” the Transformer doesn’t simply assign weights to “<em>I</em>” related to “<em>am</em>”, “<em>a</em>”, and “<em>boy</em>”. Instead, it evaluates how each word should attend to every other word in the sentence, allowing the model to contextualize each word, such as understanding “<em>boy</em>” in relation to ““. This contextualization is essential for nuanced language understanding.</p>
</section>
<section id="multi-head-attention" class="level2">
<h2 class="anchored" data-anchor-id="multi-head-attention">Multi-head attention</h2>
<p>Attention heads in transformer refers to the attention mapping mentioned about. Multi-head attention simply means there are multiple mappings being constructed to attend to different “relevance” between the tokens. In my previous example, “<em>I</em>” and “<em>boy</em>” would have different semantic connection as “<em>I</em>” and “<em>am</em>”, and therefore result in a different attention weight.</p>
<section id="self-attention-sa" class="level3">
<h3 class="anchored" data-anchor-id="self-attention-sa">Self-attention (SA)</h3>
<p>Self-attention is a mechanism that allows each position in the input sequence to attend to all positions within the same sequence. In transformer blocks, this is used to compute a representation of the sequence where the contribution of other tokens to the representation of each token is determined by the attention scores.</p>
<p>The process involves three sets of learned weights: query <span class="math inline">\(Q\)</span>, key <span class="math inline">\(K\)</span>, and value <span class="math inline">\(V\)</span> matrices. For a given token, the model computes its query and then calculates attention scores by taking the dot product of this query with the keys of all tokens in the sequence, including itself. These scores are then normalized using a softmax function, and this distribution is used to create a weighted sum of the values, resulting in the final self-attended output for that token.</p>
<p>Self-attention allows the model to integrate information from the entire sequence, making it powerful for tasks such as language modeling, where the relevance of context can vary greatly based on the content.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Technical details
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Here are some additional details of <span class="math inline">\(Q,V,K\)</span> vectors in SA module</p>
<p><span class="math display">\[ \begin{array}{cll} Q\in\mathbb{R}^{d_k}: &amp;\text{query, to match others} &amp; =\mathscr{U}\cdot W^q \\ K\in\mathbb{R}^{d_k}: &amp;\text{key, to be matched} &amp;= \mathscr{U}\cdot W^k \\ V\in\mathbb{R}^{d_v}: &amp; \text{values, information to be extracted} &amp; =\mathscr{U}\cdot W^v \end{array} \]</span></p>
<p>Attention matrices are then formed by:</p>
<p><span class="math display">\[ \alpha=\frac{Q^TK}{\sqrt{d}} \]</span></p>
<p>where d is the dim of <span class="math inline">\(Q\)</span> and <span class="math inline">\(K\)</span>. Self-attention <span class="math inline">\(\hat{\alpha}\)</span> is obtained from the soft-max of this attention matrices:</p>
<p><span class="math display">\[ \hat{\alpha}_{m,i}=\frac{\exp(\alpha_{m,i})}{\sum_j \exp(\alpha_{m,j})} \]</span></p>
<p>where <span class="math inline">\(m\)</span> denotes the row index and <span class="math inline">\(i\)</span> denotes the column index. This attention is a coefficient prepared for <span class="math inline">\(V\)</span>. The output sequence is then <span class="math inline">\(B=\hat{\alpha}V^T\)</span>, or</p>
<p><span class="math display">\[
B=\text{Attention}(Q, K, V)=V\cdot \text{softmax}\left(\frac{Q^TK}{\sqrt{d_k}} \right)
\]</span></p>
<section id="multi-head-sa" class="level4">
<h4 class="anchored" data-anchor-id="multi-head-sa">Multi-head SA</h4>
<p>For multi-head, it just mean there’s more than one attention mapper such that the hidden layer is obtained by:</p>
<p><span class="math display">\[
\begin{aligned}
\text{Multi-head}(Q, K, V)=\text{concat}(H_1, \dots,H_h)W^O \\
\text{where }H_i =\text{Attention}(QW^q_i,KW^k_i, VW^vi) \\
\end{aligned}
\]</span></p>
<p>where there’s an additional dimension <span class="math inline">\(d_k\)</span> for number of attention heads.</p>
<p><span class="math display">\[
\begin{align*}
W^q_i &amp;\in \mathbb{R}^{d_{model}\times d_k} \\
W^k_i &amp;\in \mathbb{R}^{d_{model}\times d_k} \\
W^v_i &amp;\in \mathbb{R}^{d_{model}\times d_v} \\
W^O &amp;\in \mathbb{R}^{hd_{model}\times d_v}
\end{align*}
\]</span></p>
</section>
</div>
</div>
</div>
</section>
<section id="cross-attention-ca" class="level3">
<h3 class="anchored" data-anchor-id="cross-attention-ca">Cross-attention (CA)</h3>
<p>Cross-attention is used when there are two different sequences, and the goal is to let one sequence attend to the other. This mechanism is central to the encoder-decoder structure in transformer models, where the decoder attends to the output of the encoder.</p>
<p>Similar to self-attention, cross-attention also uses <span class="math inline">\((Q, K, V)\)</span> However, in this case, <span class="math inline">\(Q\)</span> come from one sequence (typically the decoder), and <span class="math inline">\((K, V)\)</span> come from another (typically from the encoder). The decoder’s queries are used to attend to the encoder’s keys and values, allowing the decoder to focus on relevant parts of the input sequence when generating each token of the output sequence.</p>
<p>Cross-attention is particularly useful in <strong>machine translation</strong>, where the model needs to consider information from an input sentence when constructing a sentence in the target language. It is also useful in mult-modal situation where you want different encoded sequences attend to each other int the low-dimensional latent space.</p>
<div id="fig-cross-attension" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cross-attension-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/ca-struct-example.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" data-glightbox="description: .lightbox-desc-2"><img src="images/ca-struct-example.png" class="img-fluid figure-img" width="652"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cross-attension-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Example of cross-attention layer extract from <span class="citation" data-cites="li2021">(<a href="#ref-li2021" role="doc-biblioref">Li et al. 2021</a>)</span> ([<a href="http://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA</a>]). Notice how they use cross-attention to embed both textual and visual information.
</figcaption>
</figure>
</div>
</section>
</section>
<section id="postion-encoding" class="level2">
<h2 class="anchored" data-anchor-id="postion-encoding">Postion encoding</h2>
<p>One interesting details about transformer is it uses the sine and cosine function for position encoding. In particular, the position of tokens in the vector is encoded by adding a fix sequence obtained by:</p>
<p><span class="math display">\[
\begin{align}
\operatorname{PE}_{\tt pos, 2i} &amp;= \sin(\text{pos} / 10000^{2i/d_{\tt model}}) \\
\operatorname{PE}_{\tt pos, 2i+1} &amp;=\cos(\text{pos} / 10000^{2i/d_{\tt model}})  
\end{align}
\]</span></p>
<p>where <span class="math inline">\(\tt pos\)</span> is the sequence position of the token, <span class="math inline">\(i\)</span> is the channel, <span class="math inline">\(d_{\tt model}\)</span> is the total dimension of the embedding space. The position information is embedded just by adding the resultant vector from the equation above to the input vector (which is already embedded by the input embedding step).</p>
<p>Citing the authors explaination:</p>
<blockquote class="blockquote">
<p>We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset <span class="math inline">\(k, PE_{pos+k}\)</span> can be represented as a linear function of <span class="math inline">\(PE_{pos}\)</span></p>
</blockquote>
<p>This is quite interesting as the more intuitive way would just be adding equally spaced embeddings, but the author mentioned this choice is an empirical decision that returned better results. They did not really went into detail why this is the case, but I think this <a href="https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/">post online</a> summarized it pretty comprehensibly.</p>
</section>
</section>
<section id="bert" class="level1">
<h1>BERT</h1>
<p>Bidirectional Encoder Representations from Transformers, or BERT, is quite influence in the development of language models. It is once again come from Google scientist, that is an extension of transformer <span class="citation" data-cites="devlin2018">(<a href="#ref-devlin2018" role="doc-biblioref">Devlin et al. 2018</a>)</span>. It is essentially the encoder segment of the Transformer, with slight variations in hyperparameters, such as the number of multi-head attention. Also the position encoding is accompanied with segment encoding to give the AI somewhat “formats” of input, enabling also the grasp of phrases instead of just words. True to its name, BERT functions as an encoder, translating text into a latent semantic space—a crucial step for subsequent processing.</p>
<div id="fig-bert" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bert-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/bert-struct.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3" data-glightbox="description: .lightbox-desc-3"><img src="images/bert-struct.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bert-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Summary of training BERT to understand language. Extracted from <span class="citation" data-cites="devlin2018">(<a href="#ref-devlin2018" role="doc-biblioref">Devlin et al. 2018</a>)</span>
</figcaption>
</figure>
</div>
<section id="purpose-of-bert" class="level2">
<h2 class="anchored" data-anchor-id="purpose-of-bert">Purpose of BERT</h2>
<p>BERT plays the role as the “dictionary” for downstream applications by providing an effective way to learn representations of corpus from a bunch of unlabeled text, i.e., allowing the computer to understand human language. The original author showed that by pre-training BERT on a corpus, and then fine-tunning it to do various downstream tasks (by adding an output layer), BERT showed state-of-the-art performance at the time of their publication, which is quite astonishing taking the number of trainable parameters into consideration.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-bert-usage" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bert-usage-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div>
<pre class="mermaid mermaid-js" data-label="fig-bert-usage">flowchart LR
  data[(Big un-labelled &lt;br&gt;text dataset)]
  subgraph B[BERT model]
    direction LR
    BERT --&gt; |Attached to|ol[Output Layer]
  end
    data --&gt; |Pre-train|BERT
  data2[(Labelled targets)] --&gt; |Fine-tune|B
  %% data2 --&gt; |fine-tune|BERT &amp; ol
</pre>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bert-usage-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Simplified workflow to train BERT for customized tasks
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="application-from-bert" class="level2">
<h2 class="anchored" data-anchor-id="application-from-bert">Application from BERT</h2>
<p>After BERT is pre-trained, it can be quickly adapted to a certain task which uses the pre-trained corpus by attaching BERT to an output layer. For example, BERT can be configured to extract an answer of a question from a paragraph of text (Question answering model) by adding an output layer that convert the transformer outputs into 2-channel logits representing probability as starting index, and ending index.</p>
<p>Taking <code>Huggingface</code>’s example implementation of BERT QA model <code>transformers.BertForQuestionAnswering</code></p>
<div class="sourceCode" id="annotated-cell-1"><pre class="sourceCode numberSource python code-annotation-code number-lines code-with-copy code-annotated"><code class="sourceCode python"><span id="annotated-cell-1-1"><a href="#annotated-cell-1-1"></a>BertForQuestionAnswering(</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-1" data-target-annotation="1" onclick="event.preventDefault();">1</a><span id="annotated-cell-1-2" class="code-annotation-target"><a href="#annotated-cell-1-2"></a>  (bert): BertModel(</span>
<span id="annotated-cell-1-3"><a href="#annotated-cell-1-3"></a>    (embeddings): BertEmbeddings(...)</span>
<span id="annotated-cell-1-4"><a href="#annotated-cell-1-4"></a>    (encoder): BertEncoder(</span>
<span id="annotated-cell-1-5"><a href="#annotated-cell-1-5"></a>      (layer): ModuleList(</span>
<span id="annotated-cell-1-6"><a href="#annotated-cell-1-6"></a>        (<span class="dv">0</span><span class="op">-</span><span class="dv">11</span>): <span class="dv">12</span> x BertLayer(...)</span>
<span id="annotated-cell-1-7"><a href="#annotated-cell-1-7"></a>        )</span>
<span id="annotated-cell-1-8"><a href="#annotated-cell-1-8"></a>      )</span>
<span id="annotated-cell-1-9"><a href="#annotated-cell-1-9"></a>    )</span>
<span id="annotated-cell-1-10"><a href="#annotated-cell-1-10"></a>  )</span>
<a class="code-annotation-anchor" data-target-cell="annotated-cell-1" data-target-annotation="2" onclick="event.preventDefault();">2</a><span id="annotated-cell-1-11" class="code-annotation-target"><a href="#annotated-cell-1-11"></a>  (qa_outputs): Linear(in_features<span class="op">=</span><span class="dv">768</span>, out_features<span class="op">=</span><span class="dv">2</span>, bias<span class="op">=</span><span class="va">True</span>)</span>
<span id="annotated-cell-1-12"><a href="#annotated-cell-1-12"></a>)</span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<dl class="code-annotation-container-grid">
<dt data-target-cell="annotated-cell-1" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-1" data-code-lines="2" data-code-annotation="1">BERT model pretrained with any corpus</span>
</dd>
<dt data-target-cell="annotated-cell-1" data-target-annotation="2">2</dt>
<dd>
<span data-code-cell="annotated-cell-1" data-code-lines="11" data-code-annotation="2">Output layer customized for different applications</span>
</dd>
</dl>
</section>
</section>
<section id="gpt-2" class="level1">
<h1>GPT-2</h1>
<p>GPT is a generative transformer which throws out text on input.</p>
<p>The first generation of GPT came out back in 2018 <span class="citation" data-cites="radford2018">(<a href="#ref-radford2018" role="doc-biblioref">Radford et al. 2018</a>)</span>, which was back than also a model based on transformer. The model was developed by scientists in OpenAI, and it was essentially a transformer decoder with multiple layers of multi-head attention. The novelty lies on the way they formulate their output by also adding the context of “task” into the equation. Their rationale being that, same input entails different output if the implicit task is different.</p>
<div id="fig-gpt1-struct" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gpt1-struct-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="images/GPT-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4" data-glightbox="description: .lightbox-desc-4"><img src="images/GPT-1.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-gpt1-struct-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Outline of first generation of GPT structure and principles. See how the input text is being used not just for text generation, but also task classification. The authors trained the input encoder to Image is extracted from <span class="citation" data-cites="radford2018">(<a href="#ref-radford2018" role="doc-biblioref">Radford et al. 2018</a>)</span>.
</figcaption>
</figure>
</div>
<section id="training-gpt" class="level2">
<h2 class="anchored" data-anchor-id="training-gpt">Training GPT</h2>
<p>The first version of GPT is trained two steps: 1) unsupervised next-word prediction, 2) supervised fine-tuning. The first step is pretty standard so lets not go into details. The second step involving feeding the network with the text tokens that has been labeled. The text tokens are constructed based on the task, and you can see in <a href="#fig-gpt1-struct" class="quarto-xref">Figure&nbsp;5</a> how some of the tasks were formulated. The GPT, receiving the input, would not only have to output tokens, but also predict the task based on the input prompt. This means that the network has an extra layer of thought process before generating the output prompts that might not only modify the output prompt style, but also a window to replace the output model with another set of weights for some very specific tasks, such as code writing.</p>
</section>
<section id="modifications-in-gpt-2" class="level2">
<h2 class="anchored" data-anchor-id="modifications-in-gpt-2">Modifications in GPT-2</h2>
<p>Open-AI was not being incredibly transparent for the modifications they made in GPT-2. Based on the paper they released <span class="citation" data-cites="radford2019">(<a href="#ref-radford2019" role="doc-biblioref">Radford et al. 2019</a>)</span>, the only structural changes are model size and they moved the layer norm layer to the inputs of each sub-block, instead of after masked multi-head attention, and finally they prune the pre-train data used. They also trained with different sizes to observe the performance changes.</p>


<div class="hidden" aria-hidden="true">
<span class="glightbox-desc lightbox-desc-1">Figure&nbsp;1: Architecture of transformer extracted from <span class="citation" data-cites="vaswani2017">(<a href="#ref-vaswani2017" role="doc-biblioref">Vaswani et al. 2017</a>)</span>.</span>
<span class="glightbox-desc lightbox-desc-2">Figure&nbsp;2: Example of cross-attention layer extract from <span class="citation" data-cites="li2021">(<a href="#ref-li2021" role="doc-biblioref">Li et al. 2021</a>)</span> ([<a href="http://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA</a>]). Notice how they use cross-attention to embed both textual and visual information.</span>
<span class="glightbox-desc lightbox-desc-3">Figure&nbsp;3: Summary of training BERT to understand language. Extracted from <span class="citation" data-cites="devlin2018">(<a href="#ref-devlin2018" role="doc-biblioref">Devlin et al. 2018</a>)</span></span>
<span class="glightbox-desc lightbox-desc-4">Figure&nbsp;5: Outline of first generation of GPT structure and principles. See how the input text is being used not just for text generation, but also task classification. The authors trained the input encoder to Image is extracted from <span class="citation" data-cites="radford2018">(<a href="#ref-radford2018" role="doc-biblioref">Radford et al. 2018</a>)</span>.</span>
</div>

</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-devlin2018" class="csl-entry" role="listitem">
Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. <span>“BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding.”</span> <a href="https://doi.org/10.48550/ARXIV.1810.04805">https://doi.org/10.48550/ARXIV.1810.04805</a>.
</div>
<div id="ref-li2021" class="csl-entry" role="listitem">
Li, Peizhao, Jiuxiang Gu, Jason Kuen, Vlad I. Morariu, Handong Zhao, Rajiv Jain, Varun Manjunatha, and Hongfu Liu. 2021. <span>“SelfDoc: Self-Supervised Document Representation Learning.”</span> <a href="https://doi.org/10.48550/ARXIV.2106.03331">https://doi.org/10.48550/ARXIV.2106.03331</a>.
</div>
<div id="ref-radford2018" class="csl-entry" role="listitem">
Radford, Alec, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018. <span>“Improving Language Understanding by Generative Pre-Training.”</span>
</div>
<div id="ref-radford2019" class="csl-entry" role="listitem">
Radford, Alec, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. <span>“Language Models Are Unsupervised Multitask Learners.”</span> <em>OpenAI Blog</em> 1 (8): 9.
</div>
<div id="ref-vaswani2017" class="csl-entry" role="listitem">
Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. <span>“Attention Is All You Need.”</span> <a href="https://doi.org/10.48550/ARXIV.1706.03762">https://doi.org/10.48550/ARXIV.1706.03762</a>.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = true;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      return note.innerHTML;
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../../../posts/deep-learning/explainable-ai/01-xai-npc-bn.html" class="pagination-link  aria-label=" explainable="" artificial="" intelligence="" for="" automatic="" detection="" of="" early="" nasopharyngeal="" carcinoma="" on="" mri"="">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Explainable artificial intelligence for automatic detection of early nasopharyngeal carcinoma on MRI</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../../../posts/deep-learning/multi-modal/base-modules/02-CLIP.html" class="pagination-link" aria-label="LVM - CLIP">
        <span class="nav-page-text">LVM - CLIP</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>© MLun Wong, 2024</p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>
<script>var lightboxQuarto = GLightbox({"selector":".lightbox","openEffect":"zoom","loop":false,"descPosition":"bottom","closeEffect":"zoom"});
window.onload = () => {
  lightboxQuarto.on('slide_before_load', (data) => {
    const { slideIndex, slideNode, slideConfig, player, trigger } = data;
    const href = trigger.getAttribute('href');
    if (href !== null) {
      const imgEl = window.document.querySelector(`a[href="${href}"] img`);
      if (imgEl !== null) {
        const srcAttr = imgEl.getAttribute("src");
        if (srcAttr && srcAttr.startsWith("data:")) {
          slideConfig.href = srcAttr;
        }
      }
    } 
  });

  lightboxQuarto.on('slide_after_load', (data) => {
    const { slideIndex, slideNode, slideConfig, player, trigger } = data;
    if (window.Quarto?.typesetMath) {
      window.Quarto.typesetMath(slideNode);
    }
  });

};
          </script>




</body></html>