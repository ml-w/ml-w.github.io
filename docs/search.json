[
  {
    "objectID": "posts/stochastic/simple-random-walk.html",
    "href": "posts/stochastic/simple-random-walk.html",
    "title": "Stochastic - Python Example of a Random Walk Process",
    "section": "",
    "text": "Simple random walk is the first step towards learning stochastic process. Essentially, we model a particle (of desired dimension) walking randomly as time (steps) goes by. Both the stride and the direction is random, but it follows a certain distribution.\n\n\nFirst, let us define the problem formally.\n\nTo implement a 1-D simulation of random walk \\(S(t)\\) within period \\(t \\in T = \\{0, 1, 2, \\dots, N\\}\\) in sample space \\(\\omega \\in \\mathbb{W}\\), with discrete stochastic process \\(X_T = \\{X_1, X_2, \\dots, X_n \\}\\) called steps of the random&gt; walk with the constrain \\(\\text{min} \\leq X(t) \\leq \\text{max}\\).\n\n\n\n\nThe random walk can be formally defined as follow:\n\\[S(t) = S_0 + \\sum_{t=1}^{n} X_t\\]\n\\(S_0\\) represents the initial value or start point of the random walk. Also, select that each elements of \\(X_T\\) can take on integer values between -5 and 5. Implementation\nThis simulation is equivalent to plotting \\(S(t)\\) against \\(t\\).\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate random numbers within the range -5 to 5\n# Note that randint(-5, high=6) generate range -5 to 5\nN =5100 ; MIN_STEP = -5; MAX_STEP = 6; S_0 = 0; # Define parameters of the simulation\nX_T = np.random.randint(MIN_STEP , high=MAX_STEP , size=N+1) # Generate the discrete stochastic process\nt = np.linspace(0, N, N+1) # Time domain\nS = [S_0 + np.sum(X_T[0:i]) for i in range(N+1)] # Calculate each S(t) of the random walk\nplt.plot(t, S, '-') # Plot\n\n\n\n\n\n\n\n\n\n\n\n\nHere’s an example of a random flight of 1000 steps sampled from a process similar to above, but in 2-dimension.\n\n\n\n\n\n\nIn this example, we select that each elements \\(X(t) \\in X_T, t \\in T = \\{0, 1, 2, \\dots, n\\}\\) to follows \\(-5 \\leq X(t) \\leq 5 \\text{ } \\forall \\text{ } t \\in T\\). It is, however, possible to introduce various other constrain to the process w.r.t. the application of your application. By modeling this distribution based on a known particle’s path, one might be able to estimate its future movement. An even more sophisticated method to sample the path is to decide the distribution again based on the location of the particle and its surrounding environment. In real life, physical phenomenon, such as Brownian motion, can also be described by random walk.\n\n\n\nIt is also worthwhile to note that both \\(S\\) and \\(X_T=\\{T_1, T_2, \\dots, T_n\\}\\) fulfills the definition of stochastic process with the state space being different. In otherwords, stochastic process is commutative in nature."
  },
  {
    "objectID": "posts/stochastic/simple-random-walk.html#defining-the-problem",
    "href": "posts/stochastic/simple-random-walk.html#defining-the-problem",
    "title": "Stochastic - Python Example of a Random Walk Process",
    "section": "",
    "text": "First, let us define the problem formally.\n\nTo implement a 1-D simulation of random walk \\(S(t)\\) within period \\(t \\in T = \\{0, 1, 2, \\dots, N\\}\\) in sample space \\(\\omega \\in \\mathbb{W}\\), with discrete stochastic process \\(X_T = \\{X_1, X_2, \\dots, X_n \\}\\) called steps of the random&gt; walk with the constrain \\(\\text{min} \\leq X(t) \\leq \\text{max}\\)."
  },
  {
    "objectID": "posts/stochastic/simple-random-walk.html#formulation",
    "href": "posts/stochastic/simple-random-walk.html#formulation",
    "title": "Stochastic - Python Example of a Random Walk Process",
    "section": "",
    "text": "The random walk can be formally defined as follow:\n\\[S(t) = S_0 + \\sum_{t=1}^{n} X_t\\]\n\\(S_0\\) represents the initial value or start point of the random walk. Also, select that each elements of \\(X_T\\) can take on integer values between -5 and 5. Implementation\nThis simulation is equivalent to plotting \\(S(t)\\) against \\(t\\).\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate random numbers within the range -5 to 5\n# Note that randint(-5, high=6) generate range -5 to 5\nN =5100 ; MIN_STEP = -5; MAX_STEP = 6; S_0 = 0; # Define parameters of the simulation\nX_T = np.random.randint(MIN_STEP , high=MAX_STEP , size=N+1) # Generate the discrete stochastic process\nt = np.linspace(0, N, N+1) # Time domain\nS = [S_0 + np.sum(X_T[0:i]) for i in range(N+1)] # Calculate each S(t) of the random walk\nplt.plot(t, S, '-') # Plot"
  },
  {
    "objectID": "posts/stochastic/simple-random-walk.html#applications",
    "href": "posts/stochastic/simple-random-walk.html#applications",
    "title": "Stochastic - Python Example of a Random Walk Process",
    "section": "",
    "text": "In this example, we select that each elements \\(X(t) \\in X_T, t \\in T = \\{0, 1, 2, \\dots, n\\}\\) to follows \\(-5 \\leq X(t) \\leq 5 \\text{ } \\forall \\text{ } t \\in T\\). It is, however, possible to introduce various other constrain to the process w.r.t. the application of your application. By modeling this distribution based on a known particle’s path, one might be able to estimate its future movement. An even more sophisticated method to sample the path is to decide the distribution again based on the location of the particle and its surrounding environment. In real life, physical phenomenon, such as Brownian motion, can also be described by random walk."
  },
  {
    "objectID": "posts/stochastic/simple-random-walk.html#nature-of-s-and-x_t",
    "href": "posts/stochastic/simple-random-walk.html#nature-of-s-and-x_t",
    "title": "Stochastic - Python Example of a Random Walk Process",
    "section": "",
    "text": "It is also worthwhile to note that both \\(S\\) and \\(X_T=\\{T_1, T_2, \\dots, T_n\\}\\) fulfills the definition of stochastic process with the state space being different. In otherwords, stochastic process is commutative in nature."
  },
  {
    "objectID": "posts/stochastic/incremental-process.html",
    "href": "posts/stochastic/incremental-process.html",
    "title": "Stochastic - Python Example of a Random Walk Process",
    "section": "",
    "text": "An independent incremental process is a type of stochastic process where the increments (or differences) between values at different times are independent random variables. This means that the value of an increment at one time point does not influence the value of another increment at a different time point. In other words, what happens from one step to the next has no memory of what has happened before.\n\n\n\nFor a stochastic process \\(X_T\\) with sample space \\(t_1, t_2, \\dots, t_n \\in T\\) such that \\(t_1 &lt; t_2 &lt; \\dots &lt; t_n\\) then\n\\[\\displaystyle X_{t2} - X_{t1} , X_{t3} - X_{t2} , \\dots , X_{tn} - X_{tn-1}\\]\nare independent.\n\n\n\n\n\nA stationary incremental process, on the other hand, is a stochastic process where the statistical properties of the increments are time-invariant. This means that the probability distribution of the increments does not change over time. For instance, if you take any two increments of the same length at different points in the process, they should have the same probability distribution.\n\n\n\nFor an independent incremental process \\(X_T\\), it is said to be stationary if it satisfy:\n\\[\\displaystyle \\{X_{i + h} - X_{i-1 + h}\\} = \\{X_{i} - X_{i-1}\\};\\forall i, h\\in\\mathbb{Z}^+\\]\nMore formally, \\(\\forall t, s, h \\in \\mathbb{Z}^+\\), where \\(0 \\leq s &lt; t\\), the following holds:\n\\[F_{X_{t+h} - X_{s+h}}(x) = F_{X_t - X_s}(x)\\]\nwhere \\(F_{X_t - X_s}(x)\\) is the cumulative distribution function of the increment \\(X_{t} - X(s)\\).\n\n\n\n\n\n\n\nThe simple random walk process covered here is a classical example of incremental process. Whether or not it is independent and stationary is dictated by whether each steps are independent of each other, and if the distribution from which the steps are sampled is identical for every step.\n\n\n\n\n\n\nA process can be a stationary incremental process without being an independent incremental process. Consider a moving average (MA) process in time series analysis, which is often used to model time series data. In an MA process, the current value is derived from a combination of current and past random shocks (errors), with the random shocks being independent and identically distributed. The increments (changes) in the process could have a constant variance, making them stationary, but they are not independent because the value at time \\(t\\) is directly influenced by the random shock at time \\(t-1\\).\nLet \\(X(t)\\) be the MA and \\(\\epsilon(t)\\) be the shock at time \\(t\\), both being random variables and \\(\\epsilon(t) \\in \\mathscr{N}(0, 1)\\),\n\\[ X(t) = \\epsilon(t) + \\theta \\cdot \\epsilon(t-1) \\]\nEvidently \\(X(t)\\) depends on \\(t\\) because it has memory of \\(X(t-1)\\).\nHere’s a simple example of an MA(1) process, which is a moving average process of order 1:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\nn = 100  # number of points\nerrors = np.random.normal(0, 1, n)  # independent, identically distributed errors\ntheta = 0.5  # parameter of the MA(1) process\n\n# MA(1) process\nY = np.empty(n)\nY[0] = errors[0]\nfor t in range(1, n):\n    Y[t] = errors[t] + theta * errors[t-1]  # dependent increments\n\n# Y[t] - Y[t-1] are the increments and are stationary but not independent because Y[t] depends on errors[t-1]\n\nfig, ax = plt.subplots(1, 1, figsize=(7, 5))\nax.plot(Y)\nplt.show()\n\n\n\n\n\n\n\n\nIn this MA(1) model, the increments are stationary because the distribution of Y[t] - Y[t-1] does not change over time. Each increment has a mean of zero and a constant variance (which can be calculated based on the variance of the errors and the parameter theta). However, the increments are not independent because each Y[t] is dependent on the error term at t-1 (errors[t-1]).\nYou can see how the path tends to return to the average (\\(\\mu = 0\\)) if it has deviate from it.\nThis demonstrates that a process can have stationary increments with a consistent statistical distribution while still exhibiting dependence between those increments."
  },
  {
    "objectID": "posts/stochastic/incremental-process.html#independent-incremental-process",
    "href": "posts/stochastic/incremental-process.html#independent-incremental-process",
    "title": "Stochastic - Python Example of a Random Walk Process",
    "section": "",
    "text": "An independent incremental process is a type of stochastic process where the increments (or differences) between values at different times are independent random variables. This means that the value of an increment at one time point does not influence the value of another increment at a different time point. In other words, what happens from one step to the next has no memory of what has happened before.\n\n\n\nFor a stochastic process \\(X_T\\) with sample space \\(t_1, t_2, \\dots, t_n \\in T\\) such that \\(t_1 &lt; t_2 &lt; \\dots &lt; t_n\\) then\n\\[\\displaystyle X_{t2} - X_{t1} , X_{t3} - X_{t2} , \\dots , X_{tn} - X_{tn-1}\\]\nare independent."
  },
  {
    "objectID": "posts/stochastic/incremental-process.html#stationary-independent-incremental-process",
    "href": "posts/stochastic/incremental-process.html#stationary-independent-incremental-process",
    "title": "Stochastic - Python Example of a Random Walk Process",
    "section": "",
    "text": "A stationary incremental process, on the other hand, is a stochastic process where the statistical properties of the increments are time-invariant. This means that the probability distribution of the increments does not change over time. For instance, if you take any two increments of the same length at different points in the process, they should have the same probability distribution.\n\n\n\nFor an independent incremental process \\(X_T\\), it is said to be stationary if it satisfy:\n\\[\\displaystyle \\{X_{i + h} - X_{i-1 + h}\\} = \\{X_{i} - X_{i-1}\\};\\forall i, h\\in\\mathbb{Z}^+\\]\nMore formally, \\(\\forall t, s, h \\in \\mathbb{Z}^+\\), where \\(0 \\leq s &lt; t\\), the following holds:\n\\[F_{X_{t+h} - X_{s+h}}(x) = F_{X_t - X_s}(x)\\]\nwhere \\(F_{X_t - X_s}(x)\\) is the cumulative distribution function of the increment \\(X_{t} - X(s)\\)."
  },
  {
    "objectID": "posts/stochastic/incremental-process.html#random-walk",
    "href": "posts/stochastic/incremental-process.html#random-walk",
    "title": "Stochastic - Python Example of a Random Walk Process",
    "section": "",
    "text": "The simple random walk process covered here is a classical example of incremental process. Whether or not it is independent and stationary is dictated by whether each steps are independent of each other, and if the distribution from which the steps are sampled is identical for every step."
  },
  {
    "objectID": "posts/IT/headless-server.html",
    "href": "posts/IT/headless-server.html",
    "title": "Setting up a headless server",
    "section": "",
    "text": "This post is compiled from a list of steps which makes it possible to set up a headless server with nvidia GPU driver and glx support. The system is tested on Ubuntu 16.04 with nvidia-375.2 driver installed manually. Setting up XRDP"
  },
  {
    "objectID": "posts/IT/headless-server.html#prerequisite",
    "href": "posts/IT/headless-server.html#prerequisite",
    "title": "Setting up a headless server",
    "section": "Prerequisite",
    "text": "Prerequisite\n\nlibtool\nautoconf\nautomake\ncmake-curses-gui"
  },
  {
    "objectID": "posts/IT/headless-server.html#installing-libjpeg-turbo8",
    "href": "posts/IT/headless-server.html#installing-libjpeg-turbo8",
    "title": "Setting up a headless server",
    "section": "Installing libjpeg-turbo8",
    "text": "Installing libjpeg-turbo8\nClone source from Github then cd into directory.\nautoreconf\n./configure -prefix=/install/path\nmake -j 4\nmake install\nsudo ldconfig /install/path/lib\n\n\n\n\n\n\nNote\n\n\n\nNote that libtool is installed in anaconda, so you should use $ which libtool command to check which binary you are using if you get a version mismatch error here."
  },
  {
    "objectID": "posts/IT/headless-server.html#installing-virtual-gl",
    "href": "posts/IT/headless-server.html#installing-virtual-gl",
    "title": "Setting up a headless server",
    "section": "Installing Virtual GL",
    "text": "Installing Virtual GL\nClone source from Github then checkout 2.5.2 version with command $ git checkout -tag 2.5.2. It is recommended to use ccmake to config the project.\ncd /virutalgl/source\nmkdir build\ncd build;ccmake ..\nSet the following flags:\nCMAKE_CXX_FLAG=-L/Path/To/libjpeg-turbo8/lib\nCMAKE_C_FLAG=-L/Path/To/libjpeg-turbo8/lib\nCMAKE_CXX_FLAGS_RELEASE=-O3 -DNDEBUG -fPIC\nCMAKE_BUILD_TYPE=Release\nThen generate project and build, you should finish without any errors. Setup VirtualGL\nThe display of X server is configured by the file /etc/X11/xorg.conf, an example is given below:\n# nvidia-xconfig: X configuration file generated by nvidia-xconfig\n# nvidia-xconfig:  version 375.66  (buildmeister@swio-display-x86-rhel47-06)  Mon May  1 15:45:32 PDT 2017\nSection \"DRI\"\n        Mode 0666\nEndSection\n\n\nSection \"ServerLayout\"\n    Identifier     \"Layout0\"\n    Screen      0  \"Screen0\" 0 0\n    InputDevice    \"Keyboard0\" \"CoreKeyboard\"\n    InputDevice    \"Mouse0\" \"CorePointer\"\nEndSection\n\nSection \"Files\"\nEndSection\n\nSection \"InputDevice\"\n\n    # generated from default\n    Identifier     \"Mouse0\"\n    Driver         \"mouse\"\n    Option         \"Protocol\" \"auto\"\n    Option         \"Device\" \"/dev/psaux\"\n    Option         \"Emulate3Buttons\" \"no\"\n    Option         \"ZAxisMapping\" \"4 5\"\nEndSection\n\nSection \"InputDevice\"\n\n    # generated from default\n    Identifier     \"Keyboard0\"\n    Driver         \"kbd\"\nEndSection\n\nSection \"Monitor\"\n    Identifier     \"Monitor0\"\n    VendorName     \"Unknown\"\n    ModelName      \"Unknown\"\n    HorizSync       28.0 - 33.0\n    VertRefresh     43.0 - 72.0\n    Option         \"DPMS\"\nEndSection\n\nSection \"Device\"\n    Identifier     \"Device0\"\n    Driver         \"nvidia\"\n    VendorName     \"NVIDIA Corporation\"\n    BoardName      \"TITAN Xp\"\n    Option \"Coolbits\" \"5\"\nEndSection\n\nSection \"Screen\"\n    Identifier     \"Screen0\"\n    Device         \"Device0\"\n    Monitor        \"Monitor0\"\n    DefaultDepth    24\n    Option         \"UseDisplayDevice\" \"None\" #This is required for headless computer\n    SubSection     \"Display\"\n        Virtual     1920 1080\n        Depth       24\n    EndSubSection\nEndSection"
  },
  {
    "objectID": "posts/IT/headless-server.html#ipython-input-doesnt-have-any-keyboard-input",
    "href": "posts/IT/headless-server.html#ipython-input-doesnt-have-any-keyboard-input",
    "title": "Setting up a headless server",
    "section": "Ipython input doesn’t have any keyboard input",
    "text": "Ipython input doesn’t have any keyboard input\nThis happens because keyboard was not exported to X11. Export QT_XKB_CONFIG_ROOT=/usr/share/X11/xkb"
  },
  {
    "objectID": "posts/IT/headless-server.html#kernel-version-mis-match",
    "href": "posts/IT/headless-server.html#kernel-version-mis-match",
    "title": "Setting up a headless server",
    "section": "Kernel version mis-match",
    "text": "Kernel version mis-match\nTry rebooting, this happens when you update the driver"
  },
  {
    "objectID": "posts/fourier-transform/discrete-fourier-transform.html",
    "href": "posts/fourier-transform/discrete-fourier-transform.html",
    "title": "Discrete Fourier Transform",
    "section": "",
    "text": "For every continuous function \\(f(x)\\), the sampling process can be expressed as:\n\\[ \\bar{f}(x) = \\sum_{k=-\\infty}^{\\infty} f(x) \\cdot \\delta(x - kT) = f(nT) \\cdot \\text{III}_T(x) \\tag{1} \\]\nwhere \\(\\bar{f}(x)\\) is the sampled discrete function. The upper equation can be seen as a convolution.\n\n\n\nThen the Fourier transform of this equation would be\n\\[ \\mathscr{F}[\\bar{f}(x)] = \\hat{f}(k) * \\left[ \\frac{1}{T} \\text{III}\\_{1/T}(k) \\right]=\\frac{1}{T} \\sum\\_{n=-\\infty}^{\\infty} \\hat{f}(k - \\frac{n}{T}) \\tag{2.1}\\]\nwhere \\(\\hat{f}(k) = \\mathscr{F} [f(x)]\\).\nEq.[2.1] actually looks like copies of \\(\\hat{f}(k)\\) at \\(1/T\\) intervals. So you can say that sampling in the x-domain is equivalent to shift and paste in the k-domain. One problem is that if your $\\hat{f}(k) $ has components with higher then a certain frequency, it will overlap with the shifted \\(\\hat{f}(k - 1/T)\\). This overlap is un-resolvable and would create what we called Aliasing artifacts which we will talk about this later.\n\n\n\nNow moving back a little bit to Eq.[2.1]. I real life you won’t have infinite points. Suppose you sample $ N $ points from $ f(x)$ at a frequency \\(v\\) within the domain \\(x\\in [0, N-1]\\). The normalized DFT is defined by:\n\\[ X_k = \\sum\\_{n=0}^{N-1}x_n e^{-i 2 \\pi  k n / N } \\]\n\\[ x_n = \\frac{1}{N}\\sum\\_{n=0}^{N-1}X_k e^{i 2 \\pi  k n/N } \\]\nwhere \\(x_n, X_k\\) is the n-th sample and the k-th coefficient respectively. So which frequencies does the k-th bin represents?\nFor a simple sinusoidal function $f(x) = \\cos (2\\pi \\cdot 3x) $ we can easily identify the frequency of this function is 3. Similar for \\(f(x) = \\exp (i 2 \\pi 3 x)\\). Now consider the following expansion of the inverse DFT:\n\\[ \\displaystyle x_n = \\frac{1}{N} \\left[ X_0 +  X_1 \\cdot e^{i 2 \\pi n \\frac{1}{N} } + X_2 \\cdot e^{i 2 \\pi  n \\frac{2}{N} }  + \\cdots \\right] \\tag{2.2} \\]\nOne can immediately identify \\(X_k\\) is the coefficient of a \\[ k/N\\] frequency function. The resolution of the frequency domain is therefore \\(1/N\\), for example if you sample 512 points from 0mm to 511mm, each bin in the k-space resolves to \\(\\frac{1}{512} mm^{-1}\\) with the bound \\([0, 1]mm^{-1}\\),\nNow things gets trickier if you are not using 1mm as your sampling interval, but this problem is essentially an axis re-scale problem. Say you define \\(2y = x\\) then \\(f(x) = f(2y)\\), very straight forward. Now imagine you sample 512 points from 0 to 255mm. Just re-scale our previous result by substituting \\(n \\rightarrow 2n'\\):\n\\[ x\\_{n} = x\\_{2n'} = \\frac{1}{N} \\sum\\_{n'=0}^{N-1} X_k e^{i 2 \\pi  k \\cdot 2n'/N } \\]\nBy the same logic, the k-th bin corresponds to \\(\\frac{2k}{N}\\). Notice how the increase in x-space sampling rate reduce the k-space resolution while increase the k-space range. This is actually very logical, increasing your sampling rate allows one to discovers higher frequencies component. Imaging you sample at 1Hz, you will never know there are a 5Hz component in the sampling target.\n\nWe therefore draw the conclusion about x-space vs k-space resolution conversion:\n\\[ \\Delta k = \\frac{\\Delta x}{N} \\tag{2.3}\\]\nwhere $ N $ is the number of sampled points, \\(\\Delta k, \\Delta x\\) are the resolution of k and x-space respectively. Note that in k-space, the range always starts from 0 regardless of the range of $ x$ sampled.\n\n\n\nKnowing how to calculate the resolution, the next logical question is what is the minimum x-space sampling one should use to sample a function for DFT so that the inverse DFT recovers the original signal?\nRemember I mentioned Aliasing Artifacts mentioned in previous section?\n\nThis is what happens if you don’t sample enough data points, the recovered signal is flawed.\nTo look at the reason behind, study Eq.[2.1] again, seeing it has a period of \\(1/T\\) (\\(T\\) represents \\(\\Delta x\\) in Eq.[2.3]) and recall that if \\(\\hat{f}(k)\\) has components in bins greater then \\(\\frac{1}{2 \\Delta x}\\), overlapping of frequencies coefficient occurs and information is lost as we cannot resolve the overlapped frequencies.\nNyquist Theory states that in order to prevent this, knowing that the highest frequencies component of the sample is \\(B\\), one has to sample at some frequencies:\n\\[ F_s &gt; \\frac{1}{2 B} \\tag{3.1}\\]"
  },
  {
    "objectID": "posts/fourier-transform/discrete-fourier-transform.html#sampling-from-continuous-function",
    "href": "posts/fourier-transform/discrete-fourier-transform.html#sampling-from-continuous-function",
    "title": "Discrete Fourier Transform",
    "section": "",
    "text": "For every continuous function \\(f(x)\\), the sampling process can be expressed as:\n\\[ \\bar{f}(x) = \\sum_{k=-\\infty}^{\\infty} f(x) \\cdot \\delta(x - kT) = f(nT) \\cdot \\text{III}_T(x) \\tag{1} \\]\nwhere \\(\\bar{f}(x)\\) is the sampled discrete function. The upper equation can be seen as a convolution."
  },
  {
    "objectID": "posts/fourier-transform/discrete-fourier-transform.html#discrete-fourier-transform",
    "href": "posts/fourier-transform/discrete-fourier-transform.html#discrete-fourier-transform",
    "title": "Discrete Fourier Transform",
    "section": "",
    "text": "Then the Fourier transform of this equation would be\n\\[ \\mathscr{F}[\\bar{f}(x)] = \\hat{f}(k) * \\left[ \\frac{1}{T} \\text{III}\\_{1/T}(k) \\right]=\\frac{1}{T} \\sum\\_{n=-\\infty}^{\\infty} \\hat{f}(k - \\frac{n}{T}) \\tag{2.1}\\]\nwhere \\(\\hat{f}(k) = \\mathscr{F} [f(x)]\\).\nEq.[2.1] actually looks like copies of \\(\\hat{f}(k)\\) at \\(1/T\\) intervals. So you can say that sampling in the x-domain is equivalent to shift and paste in the k-domain. One problem is that if your $\\hat{f}(k) $ has components with higher then a certain frequency, it will overlap with the shifted \\(\\hat{f}(k - 1/T)\\). This overlap is un-resolvable and would create what we called Aliasing artifacts which we will talk about this later."
  },
  {
    "objectID": "posts/fourier-transform/discrete-fourier-transform.html#x-vs-k-space-resolution-conversion",
    "href": "posts/fourier-transform/discrete-fourier-transform.html#x-vs-k-space-resolution-conversion",
    "title": "Discrete Fourier Transform",
    "section": "",
    "text": "Now moving back a little bit to Eq.[2.1]. I real life you won’t have infinite points. Suppose you sample $ N $ points from $ f(x)$ at a frequency \\(v\\) within the domain \\(x\\in [0, N-1]\\). The normalized DFT is defined by:\n\\[ X_k = \\sum\\_{n=0}^{N-1}x_n e^{-i 2 \\pi  k n / N } \\]\n\\[ x_n = \\frac{1}{N}\\sum\\_{n=0}^{N-1}X_k e^{i 2 \\pi  k n/N } \\]\nwhere \\(x_n, X_k\\) is the n-th sample and the k-th coefficient respectively. So which frequencies does the k-th bin represents?\nFor a simple sinusoidal function $f(x) = \\cos (2\\pi \\cdot 3x) $ we can easily identify the frequency of this function is 3. Similar for \\(f(x) = \\exp (i 2 \\pi 3 x)\\). Now consider the following expansion of the inverse DFT:\n\\[ \\displaystyle x_n = \\frac{1}{N} \\left[ X_0 +  X_1 \\cdot e^{i 2 \\pi n \\frac{1}{N} } + X_2 \\cdot e^{i 2 \\pi  n \\frac{2}{N} }  + \\cdots \\right] \\tag{2.2} \\]\nOne can immediately identify \\(X_k\\) is the coefficient of a \\[ k/N\\] frequency function. The resolution of the frequency domain is therefore \\(1/N\\), for example if you sample 512 points from 0mm to 511mm, each bin in the k-space resolves to \\(\\frac{1}{512} mm^{-1}\\) with the bound \\([0, 1]mm^{-1}\\),\nNow things gets trickier if you are not using 1mm as your sampling interval, but this problem is essentially an axis re-scale problem. Say you define \\(2y = x\\) then \\(f(x) = f(2y)\\), very straight forward. Now imagine you sample 512 points from 0 to 255mm. Just re-scale our previous result by substituting \\(n \\rightarrow 2n'\\):\n\\[ x\\_{n} = x\\_{2n'} = \\frac{1}{N} \\sum\\_{n'=0}^{N-1} X_k e^{i 2 \\pi  k \\cdot 2n'/N } \\]\nBy the same logic, the k-th bin corresponds to \\(\\frac{2k}{N}\\). Notice how the increase in x-space sampling rate reduce the k-space resolution while increase the k-space range. This is actually very logical, increasing your sampling rate allows one to discovers higher frequencies component. Imaging you sample at 1Hz, you will never know there are a 5Hz component in the sampling target.\n\nWe therefore draw the conclusion about x-space vs k-space resolution conversion:\n\\[ \\Delta k = \\frac{\\Delta x}{N} \\tag{2.3}\\]\nwhere $ N $ is the number of sampled points, \\(\\Delta k, \\Delta x\\) are the resolution of k and x-space respectively. Note that in k-space, the range always starts from 0 regardless of the range of $ x$ sampled."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About Me",
    "section": "Education",
    "text": "Education\n\n\n\n\n\n\n\nYear\nQualification\n\n\n\n\n2015\nBachelor of Science (BSc), \nma Physics, mi Computer Science\nThe Chinese University of Hong Kong, HKSAR\n\n\n2018\nMaster of Philosophy (MPhil)\nDepartment of Imaging and Interventional Radiology\nThe Chinese University of Hong Kong, HKSAR\n\n\n2021\nDoctor of Philosophy (PhD)\nDepartment of Imaging and Interventional Radiology\nThe Chinese University of Hong Kong, HKSAR\n\n\n\n\nSelected Publications\n\nWong, L. M., et al. “Radiomics for Discrimination between Early-Stage Nasopharyngeal Carcinoma and Benign Hyperplasia with Stable Feature Selection on MRI. Cancers (Basel). 2022;14(14).\nWong, L. M., et al. “A convolutional neural network combined with positional and textural attention for the fully automatic delineation of primary nasopharyngeal carcinoma on non-contrast-enhanced MRI.” Quantitative Imaging in Medicine and Surgery 11.9 (2021): 3932-3944.\nWong, L. M., et al. “Convolutional neural network in nasopharyngeal carcinoma: how good is automatic delineation for primary tumor on a non-contrast-enhanced fat-suppressed T2-weighted MRI?” Japanese Journal of Radiology (2021): 1-9.\nWong, L. M., et al. “Convolutional neural network for discriminating nasopharyngeal carcinoma and benign hyperplasia on MRI.” European Radiology (2020): 1-8.\nWong, L. M., Shi, L., Xiao, F., & Griffith, J. F. “Fully automated segmentation of wrist bones on T2-weighted fat-suppressed MR images in early rheumatoid arthritis.” Quantitative imaging in medicine and surgery 9.4 (2019): 57.\nHung, K. F., …, Wong, L. M.*, & Leung, Y. Y.*. Automatic detection and segmentation of morphological changes of the maxillary sinus mucosa on cone-beam computed tomography images using a three-dimensional convolutional neural network. Clin Oral Investig, 26.5 (2022): 3987-3998. (*Corresponding Author)\nLam, W. K. J., A. D. King, … Wong, L. M., …, et al. “Recommendations for Epstein-Barr Virus-Based Screening for Nasopharyngeal Cancer in High- and Intermediate-Risk Regions.” J Natl Cancer Inst 115, no. 4 (Apr 11 2023): 355-64.\nZhang, R., King, A. D., Wong, L. M., Bhatia, K. S., Qamar, S., Mo, F. K. F., Vlantis, A. C., & Ai, Q. Y. H. (2023). Discriminating between benign and malignant salivary gland tumors using diffusion-weighted imaging and intravoxel incoherent motion at 3 Tesla. Diagn Interv Imaging, 104(2), 67-75.\nZhang, R., Ai, Q. Y. H., Wong, L. M., Green, C., Qamar, S., So, T. Y., Vlantis, A. C., & King, A. D. (2022). Radiomics for discriminating benign and malignant salivary gland tumors; which radiomic feature categories and MRI sequences should be used? Cancers, 14(23), 5804."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nDiscrete Fourier Transform\n\n\n\nDFT\n\n\npython\n\n\ncode\n\n\nsignal\n\n\n\n\n\n\n\nMLun Wong\n\n\nJan 11, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSetting up a headless server\n\n\n\ncode\n\n\nserver\n\n\nIT\n\n\nopencl\n\n\nxrdp\n\n\n\n\n\n\n\nMLun Wong\n\n\nJan 9, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nImaging - Convolution between an image and a filtering kernel\n\n\n\nPython\n\n\nCode\n\n\nConvolution\n\n\n\n\n\n\n\nMLun Wong\n\n\nJan 6, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nStochastic - Particle Filtering and Markov Chain Monte Carlo\n\n\n\nstochastic\n\n\npython\n\n\nrandom\n\n\nnotes\n\n\nmonte carlo\n\n\nmcmc\n\n\n\n\n\n\n\nMLun Wong\n\n\nMay 11, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nStochastic - Python Example of a Random Walk Process\n\n\n\nstochastic\n\n\npython\n\n\nrandom\n\n\nnotes\n\n\n\n\n\n\n\nMLun Wong\n\n\nMar 12, 2017\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStochastic - Python Example of a Random Walk Process\n\n\n\nstochastic\n\n\npython\n\n\nrandom\n\n\nnotes\n\n\n\n\n\n\n\nMLun Wong\n\n\nMar 12, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nStochastic - Particle Filtering and Markov Chain Monte Carlo\n\n\n\nDICOM\n\n\nNIFTI\n\n\nimage orientation\n\n\nvtk\n\n\nitk\n\n\n\n\n\n\n\nMLun Wong\n\n\nMar 10, 2017\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/fourier-transform/index.html",
    "href": "posts/fourier-transform/index.html",
    "title": "Imaging - Convolution between an image and a filtering kernel",
    "section": "",
    "text": "The equivalence of convolution in the image domain and element-wise multiplication in the Fourier domain is a foundational concept in image processing, applicable even within discrete grid spaces. Textbooks often present this relationship as follows:\n\\[\n\\begin{align}\nI(\\vec{x}) = f(\\vec{x}) * g(\\vec{\\tau}) &= \\sum_{\\vec{\\tau}} f(\\vec{x} - \\vec{\\tau}) \\cdot g(\\vec{\\tau}) \\\\\n&= \\mathscr{F}^{-1}\\left\\{\\mathscr{F}[f(\\vec{x})] \\cdot \\mathscr{F}[g(\\vec{\\tau})]\\right\\} \\tag{1}\n\\end{align}\n\\]\nHere, \\(I(\\vec{x})\\) represents the resulting n-dimensional image after convolution, with \\(x_i \\in X\\) and \\(\\tau_i \\in T\\) such that the domain \\(T \\subseteq X\\). This formulation is straightforward when the convolution involves images of the same domain (i.e., identical size). However, the scenario of interest involves the special case where \\(T \\neq X\\), specifically \\(T \\subset X\\). The question then arises: how do we compute this\n\\[\n\\mathscr{F}[f(\\vec{x})] \\cdot \\mathscr{F}[g(\\vec{\\tau})]\n\\]\nPractically when the kernel \\(g(\\vec{\\tau})\\) and the image \\(f(\\vec{x})\\) differ in size?\nIn such cases, direct element-wise multiplication is not feasible. This dilemma is seldom addressed in textbooks, leaving a gap in practical understanding. So what can we do here?\nIn reality, you can thought of the scenario as convolving the image with a kernel that is centered and padded with zeros elsewhere. The most straightforward practical approach is to equalize the domains by padding the kernel \\(g(\\vec{\\tau})\\) with zeros, effectively expanding the domain \\(T\\) to match \\(X\\).\nThis method ensures that the Fourier transform of the padded kernel has the same dimensionality as the Fourier transform of the image, allowing for the required element-wise multiplication in the Fourier space.\n\n\n\n\n'''Import libraries'''\nimport numpy as np\nfrom IPython.display import *\nfrom skimage.color import rgb2gray\nfrom skimage.io import imread\nimport scipy.ndimage as ndimage\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n'''Load Images'''\n# Generate an example 512x512 image and 3x3 kernel\nimage = imread(\"./lena.png\").astype('float32')\nimage = rgb2gray(image[...,:3])\n\nplt.imshow(image,cmap='gray')\nplt.show()\n\n\n\n\nFig.1 Lena.png. We are using this image as our input.\n\n\n\n\ndef out_conv(im, kn):\n    \"\"\"Convolution\"\"\"\n    fftkern = np.fft.fftshift(np.fft.fft2(np.pad(kn,\n                            [[256-1, 256-2], [256-1,256-2]],\n                            constant_values=0,\n                            mode='constant')\n                    ))\n    fftimage = np.fft.fftshift(np.fft.fft2(im))\n\n    # Multiplication\n    fftoutput = fftkern*fftimage\n\n    # Inverse fourier transform\n    out = np.fft.ifftshift(np.fft.ifft2(np.fft.fftshift(fftoutput)))        \n    return np.real(out)\n\n# Define 3 by 3 Kernel\nkern = np.array([[1, 0, -1], [1, 0, -1], [1, 0, -1]], dtype=np.float32)\nioutput = out_conv(image, kern)\n\n# Scipy output as ground truth\nspoutput = ndimage.filters.convolve(image, kern)\n\nfig = plt.figure(figsize=(10, 5))\nax1, ax2 = [fig.add_subplot(i) for i in [121, 122]]\nax1.imshow(ioutput, cmap=\"Greys_r\")\nax2.imshow(spoutput, cmap=\"Greys_r\")\nax1.set_title(r\"$\\mathscr{F}[f(\\vec{x})]$ Result by us\")\nax2.set_title(r\"$\\mathscr{F}[f(\\vec{x})]$ Result by `scipy`\")\n\n\n# calculate and show the difference\ndiff = np.abs(spoutput - ioutput)\nfig = plt.figure(figsize=(5, 5))\nax1 = fig.add_subplot(111)\nax1.imshow(diff, cmap='jet', vmin=-10, vmax=10)\nax1.set_title(\"Real part differences between us and `scipy`\")\n\n# display(Markdown(\"## Results\"))\nplt.show()\n\n\n\n\n\n\n\n\n\n\nSeems like we have repeated the conv function scipy implemented! But did we really? Lets take closer to the edge of the image:\n\n\nCode\nfig, ax = plt.subplots(1, 2, figsize=(10, 5))\nax[0].imshow(spoutput[:25, :25], cmap='jet', vmin=-10, vmax=10)\nax[1].imshow(ioutput[:25, :25], cmap='jet', vmin=-10, vmax=10)\nax[0].set_title(\"`scipy` at the upper left corner\")\nax[1].set_title(\"Ours' at the upper left corner\")\nplt.show()\n\n\n\n\n\nFig.2 Difference between our implementation of convolution and scipy\n\n\n\n\nWow, what happened here? As you can see, our implementation differs with scipy at the edge of the output image. So what is actually happening? To keep things short, this has to do with how the kernel was convolved at the edge of the image. FFT was implemented based on the assumption that the image is periodic outside of the presented discrete domain \\(X\\). Therefore, when the convolve (in \\(x\\) space) reaches the edge, its wrapping back the other end of the image.\n\n\n\n\n\nThis is the most straightforward approach and the one used initially in this workbook, where zeros are added to the borders of the kernel image. Now remember that FFT requires the input to have a dimension as a power of 2, this means that for odd number sized kernels, the padding is always asymetric.\n\n\n\nWhen using FFT for convolution, some libraries and frameworks automatically handle padding. The input signals are implicitly padded with zeros to the next power of two or to a size that allows for the most efficient computation by the FFT algorithm.\n\n\n\nIn some cases, especially in neural networks, convolutions are performed without padding, known as ‘valid’ convolution. This approach results in an output that is smaller than the input because only the regions where the kernel and input fully overlap are computed.\n\n\n\nInstead of padding with zeros, the border values are reflected or mirrored across the edge. This can help reduce the artifacts introduced by the hard boundary of zero-padding and can be more appropriate for certain types of data, such as images.\n\n\n\nThis involves wrapping the signal around, effectively creating a toroidal topology. This is the inherent assumption in FFT-based convolution, but it can also be done explicitly in the spatial domain.\n\n\n\nThe edge values are replicated to the padded regions. This can sometimes produce more natural results than zero-padding, particularly for image processing tasks.\n\n\n\nUsed mainly in image inpainting, partial convolutions involve a mask that indicates which pixels are valid, allowing the convolution to be reweighted to ignore the contribution from missing or padded values.\n\n\n\nAlso known as atrous convolutions, these involve spacing out the kernel elements, which allows for a larger receptive field without increasing the computational complexity. This is not directly a padding method but can be used to reduce the need for padding by controlling the resolution at which feature maps are computed.\n\n\n\nBy skipping input values with a certain stride, these convolutions effectively downsample the input. This can also reduce the need for padding by adjusting the output size."
  },
  {
    "objectID": "posts/fourier-transform/index.html#zero-padding-explicit-padding",
    "href": "posts/fourier-transform/index.html#zero-padding-explicit-padding",
    "title": "Imaging - Convolution between an image and a filtering kernel",
    "section": "",
    "text": "This is the most straightforward approach and the one used initially in this workbook, where zeros are added to the borders of the kernel image. Now remember that FFT requires the input to have a dimension as a power of 2, this means that for odd number sized kernels, the padding is always asymetric."
  },
  {
    "objectID": "posts/fourier-transform/index.html#implicit-padding-in-fast-fourier-transform-fft",
    "href": "posts/fourier-transform/index.html#implicit-padding-in-fast-fourier-transform-fft",
    "title": "Imaging - Convolution between an image and a filtering kernel",
    "section": "",
    "text": "When using FFT for convolution, some libraries and frameworks automatically handle padding. The input signals are implicitly padded with zeros to the next power of two or to a size that allows for the most efficient computation by the FFT algorithm."
  },
  {
    "objectID": "posts/fourier-transform/index.html#valid-convolution",
    "href": "posts/fourier-transform/index.html#valid-convolution",
    "title": "Imaging - Convolution between an image and a filtering kernel",
    "section": "",
    "text": "In some cases, especially in neural networks, convolutions are performed without padding, known as ‘valid’ convolution. This approach results in an output that is smaller than the input because only the regions where the kernel and input fully overlap are computed."
  },
  {
    "objectID": "posts/fourier-transform/index.html#reflective-or-symmetric-padding",
    "href": "posts/fourier-transform/index.html#reflective-or-symmetric-padding",
    "title": "Imaging - Convolution between an image and a filtering kernel",
    "section": "",
    "text": "Instead of padding with zeros, the border values are reflected or mirrored across the edge. This can help reduce the artifacts introduced by the hard boundary of zero-padding and can be more appropriate for certain types of data, such as images."
  },
  {
    "objectID": "posts/fourier-transform/index.html#circular-padding",
    "href": "posts/fourier-transform/index.html#circular-padding",
    "title": "Imaging - Convolution between an image and a filtering kernel",
    "section": "",
    "text": "This involves wrapping the signal around, effectively creating a toroidal topology. This is the inherent assumption in FFT-based convolution, but it can also be done explicitly in the spatial domain."
  },
  {
    "objectID": "posts/fourier-transform/index.html#replicate-padding",
    "href": "posts/fourier-transform/index.html#replicate-padding",
    "title": "Imaging - Convolution between an image and a filtering kernel",
    "section": "",
    "text": "The edge values are replicated to the padded regions. This can sometimes produce more natural results than zero-padding, particularly for image processing tasks."
  },
  {
    "objectID": "posts/fourier-transform/index.html#partial-convolutions",
    "href": "posts/fourier-transform/index.html#partial-convolutions",
    "title": "Imaging - Convolution between an image and a filtering kernel",
    "section": "",
    "text": "Used mainly in image inpainting, partial convolutions involve a mask that indicates which pixels are valid, allowing the convolution to be reweighted to ignore the contribution from missing or padded values."
  },
  {
    "objectID": "posts/fourier-transform/index.html#dilated-convolutions",
    "href": "posts/fourier-transform/index.html#dilated-convolutions",
    "title": "Imaging - Convolution between an image and a filtering kernel",
    "section": "",
    "text": "Also known as atrous convolutions, these involve spacing out the kernel elements, which allows for a larger receptive field without increasing the computational complexity. This is not directly a padding method but can be used to reduce the need for padding by controlling the resolution at which feature maps are computed."
  },
  {
    "objectID": "posts/fourier-transform/index.html#strided-convolutions",
    "href": "posts/fourier-transform/index.html#strided-convolutions",
    "title": "Imaging - Convolution between an image and a filtering kernel",
    "section": "",
    "text": "By skipping input values with a certain stride, these convolutions effectively downsample the input. This can also reduce the need for padding by adjusting the output size."
  },
  {
    "objectID": "posts/medial-imaging/image-orientation.html",
    "href": "posts/medial-imaging/image-orientation.html",
    "title": "Stochastic - Particle Filtering and Markov Chain Monte Carlo",
    "section": "",
    "text": "Directions and orientation of medical images are indeed a very confusing problem, especially when you are trying to handle multiple images (then relative spacing becomes a concern). There are also multiple conventions to view these image (e.g. neurologist view and radiologist view, which defines the righthandness of the image), adding further confusions to the problem.\nFor most volume storage format,  a list of the most important components which defines a meaning image would be:\n\nVoxel data\nImage Origin\nImage Spacing\nDirection\n\nThese data are usually stored separately to maximize storage efficiency. Voxel data is always along Euclidean xyz-axises, which might not accurately represents the actual physical location each pixel. For instance, imagine you gave a metal bar two CT scans, one with it aligned perfectly with the machine’s rotational axis, the second with it purposely tilted 60° from the axis. The most effective way to store these two image is obviously by drawing the grid with the z-axis along the bar so that the voxel data stored is actually identically. However, we need to account for the 60° tilt of the second bar, which is achieved by specifying direction of the image. Similar though experiment goes with image origin, you get the idea?\nThis post tries to give a simplified explanation and solution to this problem for ITK and VTK users.\n\n\n\nWe consider only two of the most commonly seen medical image format, namely NIfTI and DICOM images. These two image formats use different hierarchy to store the direction information. While NIfTI images use QForm matrix and SForm matrix to store direction information, DICOM images use the DICOM tag stating cosines of angles. This topic will be discussed in detail in later sections.\n\n\nThree loading methods are considered in this post:\n\nLoad as ITK image.\nLoad as ITK image then use convert to vtkImageData.\nLoad as vtkImageData.\n\nTabulating their characters w.r.t. image directions:\n\n\n\n \nHas Direction\nHas Origin\nHas Orientation\n\n\n\n\nITK Image Loader\nYes\nYes\nYes\n\n\nITK to VTK\nNo\nYes\nNo\n\n\nVTK Image Loader\nNo\nNo\nNo\n\n\n\n\n\n\n\n\n\nImage origin is the position of the corner of the first stored voxel (i.e. index ijk = [0,0,0]). Which of the 8 corners depends on whether the image is righthanded or lefthanded, this give rise to the orientation definition.\n\n\n\n\n\nImage orientation refers to the specification of the three principle axis w.r.t. voxel index. First, we introduce a notion, +x means the x-coordinate increase with the first index i of the voxel., similar for +y and +z. A more common notion would be the permutation of six direction LR, SP and AP, e.g. RAI/LPS…etc. This, however, doesn’t defines the image  xyz-axis yet since they can be along any directions. Therefore, we requires the definition of Image Direction.\n\n\n\n\n\n\n\n\n\n\n\nImage origin of DICOM image is stored in the DICOM tag “Image Position (Patient) (0020,0032)”, which is a simple offset against all the voxel coordinates. Note that each slices has it’s own value of Image Position, but we only concern the first slice.\n\n\n\n\n\nImage direction of DICOM image is stored in DICOM tag “Image Orientation (Patient) (0020,0037)”, it is defined as the cosines of angle of three axis. Every slice of the same series (except for scouts) should have the same orientation.\n\n\n\n\n\n\n\n\nThe image origin of Nifti files are stored in the quantity qoffsets and the fourth element of srow_x/y/z in the header. Usually, they are the same so you can just use one of them.\n\n\n\n\n\nImage directions of Nifti files are defined by two matrices in the header, namely SForm Matrix and QForm Matrix, which, in most cases, are the essentially the same matrix except SForm Matrix includes the spacing . The usage of these two matrices are defined by two quantity called sform code and qform code, and the QForm Matrix is defined by a vector quaternion. According to the documentation, three methods are mentioned for using these matrices depending on whether the qform code and sform code are 0 or not.\n\n\n\n\n\n\n\n\nGenerally DICOM can be seen as a series of 2D images with extra information stored in headers. However, an image can be sliced along different normals, for example sagital, coronal or axial. These three direction are not the only direction that a DICOM series can take on, in fact, a DICOM series can be sliced along any directions. The slice direction is decided by the  “Image Orientation (Patient) (0020,0037)” DICOM tag, which specify the reference frame. The “Image Orientation(Patient)“ is a 6-element tuple consist of two vectors which describes the axis of the direction for row and column of that particular slice. For example, if your slices are Axial slice, then the two vectors defines sagital and coronal directions.\nIf the tag is “a”, then the cross product of the two vectors gives the third colume of the rotational matrix, i.e:\nIf the corresponding axis vectors will be\n\\[\n\\begin{align}\n\\vec{v}_x &= \\begin{bmatrix} a&b&c \\end{bmatrix}^T \\\\\n\\vec{v}_y &= \\begin{bmatrix} d&e&f \\end{bmatrix}^T \\\\\n\\vec{v}_z &= \\vec{v}_x \\times \\vec{v}_y \\end{align}\n\\]\n\n\n\nTo properly load the image data, we first examine the qform and sform code, that perform action according the the conditions below.\nIf qform = 0\nThis method is for backwards compatibility only. When qform code = 0, the matrices will not be used to define the orientation and direction, the index to coordinate matrix is simply the following:\n\\[\n\\vec{r} = \\vec{s}^{\\text{ }T} \\cdot \\vec{I}\\\n\\]\nwhere r is physical coordinate, s is spacing vector and I is the index vector (i.e. $ = [i, j, k] $). In this case, no further transformation is need after image loading (or the transform is Identity matrix).\n\n\nWhen qform code is greater than zero, this method applies. This method involves constructing a rotation matrix using a so called quaternion representation.\nDefinition of quaternion:\n\\[ \\vec{q} =\\begin{bmatrix} a\\\\b\\\\c\\\\d \\end{bmatrix} \\]\nwhere we require that:\n\\[  a^2 + b^2 + c^2 + d^2 = 1\\]\nSo having three of the four values gives the remaining one. In nifti format, only b, c, d are given in the header, and we calculate a by the formula:\n$ a = $\nUsing a, b, c, d the rotational matrix R is defined as:\n\\[ \\displaystyle\n\\text{R}_q = \\begin{bmatrix}\na^2 + b^2 - c^2 - d^2&2bc - 2ad&2bd+2ac\\\\\n2bc + 2ad& a^2 + c^2 - b^2 - d^2& 2cd - 2ab\\\\\n2bd - 2ac&2cd + 2ab&a^2 + d^2 - c^2 - b^2\\\\\n\\end{bmatrix}\n\\]\nThe index to physical position formula would therefore be:\n\\[ \\displaystyle\n\\vec{r} = \\text{R}_q [\\vec{s}^{\\text{ }T}\\cdot \\vec{I}\\] + \\vec{q}_0\\\n\\]\nwhere q_0 stands for qoffsets.\nHence the affine matrix vtkMatrix4x4 used for transform in VTK would be\n\\[ \\displaystyle\n\\text{A} = \\begin{bmatrix}\\\na^2 + b^2 - c^2 - d^2&2bc - 2ad&2bd+2ac&q_x\\\\\n2bc + 2ad& a^2 + c^2 - b^2 - d^2& 2cd - 2ab&q_y\\\\\n2bd - 2ac&2cd + 2ab&a^2 + d^2 - c^2 - b^2&q_z\\\\\n0&0&0&1\\\n\\end{bmatrix}\\\n\\]\n\n\n\nThis can coexist with qform &gt; 0 (i.e. both qform &gt; 0 and sform &gt; 0 can coexist, qform describe the transformation from data to scanning grid, sform describe the transformation from data to standard grid). The sform matrix is stored separately in three vectors: SRowX, SRowY and SRowZ, which we will denote as: $ _x, _y, _z $.\n\\[\n\\displaystyle \\text{R}_s =\\begin{bmatrix}g_{x_1}&g_{x_2}&g_{x_3}\\\\g_{y_1}&g_{y_2}&g_{y_3}\\\\g_{z_1}&g_{z_2}&g_{z_3} \\end{bmatrix}\n\\]\nSince the SForm matrix already include spacing, there are no needs to multiply spacing for each index. The index to physical position formula would therefore be:\n\\[\n\\displaystyle\n\\vec{r} = \\text{R}_s \\vec{I} + \\begin{bmatrix} g_{x_4}\\\\g_{y_4}\\\\g_{z_4} \\end{bmatrix}\\\n\\]\nThe affine matrix used for transform in vtkMatrix4x4 would then be the rotational matrix divided by spacing spacing vector s:\n\\[\n\\displaystyle\\\n\\text{A} =\\\n\\begin{bmatrix}\\\n\\frac{g_{x_1}}{s_1}&\\frac{g_{x_2}}{s_2}&\\frac{g_{x_3}}{s_3}&g_{x_4}\\\\\n\\frac{g_{y_1}}{s_1}&\\frac{g_{y_2}}{s_2}&\\frac{g_{y_3}}{s_3}&g_{y_4}\\\\\n\\frac{g_{z_1}}{s_1}&\\frac{g_{z_2}}{s_2}&\\frac{g_{z_3}}{s_3}&g_{z_4}\\\\\n0&0&0&1\\\n\\end{bmatrix}\\\n\\]\n\n\n\n\nimport vtk\nimport numpy as np\nimport tempfile\nimport SimpleITK as sitk\nfrom pprint import pprint\n\n# Create a fake image with simpleitk\nimage = sitk.Image(28, 28, 28, sitk.sitkUInt8)\nimage.SetSpacing([0.5, 0.1, 0.2])\n\n# Set directio to:\n#  [1, 0, 0]\n#  [0, 0, 1]\n#  [0, 1, 0]\nimage.SetDirection((1., 0., 0., 0., 0., 1., 0., 1., 0.))\n\n# Write this dummy image for VTK to read\nwith tempfile.TemporaryDirectory() as f:\n    sitk.WriteImage(image, f + \"/temp.nii.gz\")\n\n    # read it back with vtk\n    reader = vtk.vtkNIFTIImageReader()\n    reader.SetFileName(f + \"/temp.nii.gz\")\n    reader.Update()\n \n    header = reader.GetNIFTIHeader()\n    vtkimage = reader.GetOutput()\n\nprint(\"ITK: \")\npprint(np.asarray(image.GetDirection()).reshape(3, 3))\nprint(\"VTK: \")\nprint(vtkimage.GetDirectionMatrix())\n\n\n\n\n\n\nITK: \narray([[1., 0., 0.],\n       [0., 0., 1.],\n       [0., 1., 0.]])\nVTK: \nvtkMatrix3x3 (0000025FBE641A50)\n  Debug: Off\n  Modified Time: 1319\n  Reference Count: 2\n  Registered Events: (none)\n  Elements:\n    1   0   0\n    0   1   0\n    0   0   1\n\nSee how the two matrix are different? In fact VTK did not load the the direction into the matrix and therefore it remained as the default identity matrix.\n\n\n\n\n\n#Spacing\ns = image.GetSpacing()\ns = np.array([s[0], s[1], s[2], 1])\n \n#Origin (Set this to [0, 0, 0, 1] if you are using itkvtkConnector)\nori = np.array( [header.GetQOffsetX(), header.GetQOffsetY(), header.GetQOffsetZ(), 1])\n \n'''Directions'''\n# Use QForm matrix\nif (header.GetQFormCode() &gt; 0):\n    b = header.GetQuaternB()\n    c = header.GetQuaternC()\n    d = header.GetQuaternD()\n    a = np.sqrt(1 - b*b - c*c - d*d)\n    A = np.array([\n        [a*a + b*b - c*c - d*d, 2*b*c - 2*a*d, 2*b*d + 2*a*c, ori[0]],\n        [2*b*c + 2*a*d, a*a+c*c-b*b-d*d, 2*c*d - 2*a*b, ori[1]],\n        [2*b*d - 2*a*c, 2*c*d + 2*a*b, a*a + d*d - c*c - b*b, ori[2]],\n        [0, 0, 0, 1]\n    ])\n\n    # Obtain user transform for vtk algorithms\n    mat = vtk.vtkMatrix4x4()\n    [[mat.SetElement(i, j, A[i, j]) for i in range(4)] for j in range(4)]\n    print(\"From qform: \\n\", mat)\n \n# Use SForm Matrix\nif (header.GetSFormCode() &gt; 0):\n    gx = header.GetSRowX()\n    gy = header.GetSRowY()\n    gz = header.GetSRowZ()\n\n    # divide SForm matrix by spacing\n    gx /= s\n    gy /= s\n    gz /= s\n    A = np.zeros([4,4])\n    A[3, 3] = 1\n    A[0,:] = gx\n    A[1,:] = gy\n    A[2,:] = gz\n    # Obtain user transform for vtk algorithms\n    mat = vtk.vtkMatrix4x4()\n    [[mat.SetElement(i, j, A[i, j]) for i in range(4)] for j in range(4)]\n    print(\"From SForm: \\n\", mat)\n\n\n\n\nFrom qform: \n vtkMatrix4x4 (0000025FBEC509E0)\n  Debug: Off\n  Modified Time: 1478\n  Reference Count: 1\n  Registered Events: (none)\n  Elements:\n    -1 -0.000261643 0.000261643 0 \n    0.000261643 3.42285e-08 1 0 \n    -0.000261643 1 3.42285e-08 0 \n    0 0 0 1 \n\n\nFrom SForm: \n vtkMatrix4x4 (0000025FBEC4FF40)\n  Debug: Off\n  Modified Time: 1484\n  Reference Count: 1\n  Registered Events: (none)\n  Elements:\n    -1 0 0 0 \n    0 -0 -1 0 \n    0 1 0 0 \n    0 0 0 1 \n\nNow if you round them up, you can see how this is corrected. However, you might also notice the qFrom matrix and sForm matrix gives you different results."
  },
  {
    "objectID": "posts/medial-imaging/image-orientation.html#image-loading-methods",
    "href": "posts/medial-imaging/image-orientation.html#image-loading-methods",
    "title": "Stochastic - Particle Filtering and Markov Chain Monte Carlo",
    "section": "",
    "text": "Three loading methods are considered in this post:\n\nLoad as ITK image.\nLoad as ITK image then use convert to vtkImageData.\nLoad as vtkImageData.\n\nTabulating their characters w.r.t. image directions:\n\n\n\n \nHas Direction\nHas Origin\nHas Orientation\n\n\n\n\nITK Image Loader\nYes\nYes\nYes\n\n\nITK to VTK\nNo\nYes\nNo\n\n\nVTK Image Loader\nNo\nNo\nNo"
  },
  {
    "objectID": "posts/medial-imaging/image-orientation.html#defining-terms",
    "href": "posts/medial-imaging/image-orientation.html#defining-terms",
    "title": "Stochastic - Particle Filtering and Markov Chain Monte Carlo",
    "section": "",
    "text": "Image origin is the position of the corner of the first stored voxel (i.e. index ijk = [0,0,0]). Which of the 8 corners depends on whether the image is righthanded or lefthanded, this give rise to the orientation definition.\n\n\n\n\n\nImage orientation refers to the specification of the three principle axis w.r.t. voxel index. First, we introduce a notion, +x means the x-coordinate increase with the first index i of the voxel., similar for +y and +z. A more common notion would be the permutation of six direction LR, SP and AP, e.g. RAI/LPS…etc. This, however, doesn’t defines the image  xyz-axis yet since they can be along any directions. Therefore, we requires the definition of Image Direction."
  },
  {
    "objectID": "posts/medial-imaging/image-orientation.html#dicom-images",
    "href": "posts/medial-imaging/image-orientation.html#dicom-images",
    "title": "Stochastic - Particle Filtering and Markov Chain Monte Carlo",
    "section": "",
    "text": "Image origin of DICOM image is stored in the DICOM tag “Image Position (Patient) (0020,0032)”, which is a simple offset against all the voxel coordinates. Note that each slices has it’s own value of Image Position, but we only concern the first slice.\n\n\n\n\n\nImage direction of DICOM image is stored in DICOM tag “Image Orientation (Patient) (0020,0037)”, it is defined as the cosines of angle of three axis. Every slice of the same series (except for scouts) should have the same orientation."
  },
  {
    "objectID": "posts/medial-imaging/image-orientation.html#nifti",
    "href": "posts/medial-imaging/image-orientation.html#nifti",
    "title": "Stochastic - Particle Filtering and Markov Chain Monte Carlo",
    "section": "",
    "text": "The image origin of Nifti files are stored in the quantity qoffsets and the fourth element of srow_x/y/z in the header. Usually, they are the same so you can just use one of them.\n\n\n\n\n\nImage directions of Nifti files are defined by two matrices in the header, namely SForm Matrix and QForm Matrix, which, in most cases, are the essentially the same matrix except SForm Matrix includes the spacing . The usage of these two matrices are defined by two quantity called sform code and qform code, and the QForm Matrix is defined by a vector quaternion. According to the documentation, three methods are mentioned for using these matrices depending on whether the qform code and sform code are 0 or not."
  },
  {
    "objectID": "posts/medial-imaging/image-orientation.html#dicom",
    "href": "posts/medial-imaging/image-orientation.html#dicom",
    "title": "Stochastic - Particle Filtering and Markov Chain Monte Carlo",
    "section": "",
    "text": "Generally DICOM can be seen as a series of 2D images with extra information stored in headers. However, an image can be sliced along different normals, for example sagital, coronal or axial. These three direction are not the only direction that a DICOM series can take on, in fact, a DICOM series can be sliced along any directions. The slice direction is decided by the  “Image Orientation (Patient) (0020,0037)” DICOM tag, which specify the reference frame. The “Image Orientation(Patient)“ is a 6-element tuple consist of two vectors which describes the axis of the direction for row and column of that particular slice. For example, if your slices are Axial slice, then the two vectors defines sagital and coronal directions.\nIf the tag is “a”, then the cross product of the two vectors gives the third colume of the rotational matrix, i.e:\nIf the corresponding axis vectors will be\n\\[\n\\begin{align}\n\\vec{v}_x &= \\begin{bmatrix} a&b&c \\end{bmatrix}^T \\\\\n\\vec{v}_y &= \\begin{bmatrix} d&e&f \\end{bmatrix}^T \\\\\n\\vec{v}_z &= \\vec{v}_x \\times \\vec{v}_y \\end{align}\n\\]"
  },
  {
    "objectID": "posts/medial-imaging/image-orientation.html#nifti-1",
    "href": "posts/medial-imaging/image-orientation.html#nifti-1",
    "title": "Stochastic - Particle Filtering and Markov Chain Monte Carlo",
    "section": "",
    "text": "To properly load the image data, we first examine the qform and sform code, that perform action according the the conditions below.\nIf qform = 0\nThis method is for backwards compatibility only. When qform code = 0, the matrices will not be used to define the orientation and direction, the index to coordinate matrix is simply the following:\n\\[\n\\vec{r} = \\vec{s}^{\\text{ }T} \\cdot \\vec{I}\\\n\\]\nwhere r is physical coordinate, s is spacing vector and I is the index vector (i.e. $ = [i, j, k] $). In this case, no further transformation is need after image loading (or the transform is Identity matrix).\n\n\nWhen qform code is greater than zero, this method applies. This method involves constructing a rotation matrix using a so called quaternion representation.\nDefinition of quaternion:\n\\[ \\vec{q} =\\begin{bmatrix} a\\\\b\\\\c\\\\d \\end{bmatrix} \\]\nwhere we require that:\n\\[  a^2 + b^2 + c^2 + d^2 = 1\\]\nSo having three of the four values gives the remaining one. In nifti format, only b, c, d are given in the header, and we calculate a by the formula:\n$ a = $\nUsing a, b, c, d the rotational matrix R is defined as:\n\\[ \\displaystyle\n\\text{R}_q = \\begin{bmatrix}\na^2 + b^2 - c^2 - d^2&2bc - 2ad&2bd+2ac\\\\\n2bc + 2ad& a^2 + c^2 - b^2 - d^2& 2cd - 2ab\\\\\n2bd - 2ac&2cd + 2ab&a^2 + d^2 - c^2 - b^2\\\\\n\\end{bmatrix}\n\\]\nThe index to physical position formula would therefore be:\n\\[ \\displaystyle\n\\vec{r} = \\text{R}_q [\\vec{s}^{\\text{ }T}\\cdot \\vec{I}\\] + \\vec{q}_0\\\n\\]\nwhere q_0 stands for qoffsets.\nHence the affine matrix vtkMatrix4x4 used for transform in VTK would be\n\\[ \\displaystyle\n\\text{A} = \\begin{bmatrix}\\\na^2 + b^2 - c^2 - d^2&2bc - 2ad&2bd+2ac&q_x\\\\\n2bc + 2ad& a^2 + c^2 - b^2 - d^2& 2cd - 2ab&q_y\\\\\n2bd - 2ac&2cd + 2ab&a^2 + d^2 - c^2 - b^2&q_z\\\\\n0&0&0&1\\\n\\end{bmatrix}\\\n\\]\n\n\n\nThis can coexist with qform &gt; 0 (i.e. both qform &gt; 0 and sform &gt; 0 can coexist, qform describe the transformation from data to scanning grid, sform describe the transformation from data to standard grid). The sform matrix is stored separately in three vectors: SRowX, SRowY and SRowZ, which we will denote as: $ _x, _y, _z $.\n\\[\n\\displaystyle \\text{R}_s =\\begin{bmatrix}g_{x_1}&g_{x_2}&g_{x_3}\\\\g_{y_1}&g_{y_2}&g_{y_3}\\\\g_{z_1}&g_{z_2}&g_{z_3} \\end{bmatrix}\n\\]\nSince the SForm matrix already include spacing, there are no needs to multiply spacing for each index. The index to physical position formula would therefore be:\n\\[\n\\displaystyle\n\\vec{r} = \\text{R}_s \\vec{I} + \\begin{bmatrix} g_{x_4}\\\\g_{y_4}\\\\g_{z_4} \\end{bmatrix}\\\n\\]\nThe affine matrix used for transform in vtkMatrix4x4 would then be the rotational matrix divided by spacing spacing vector s:\n\\[\n\\displaystyle\\\n\\text{A} =\\\n\\begin{bmatrix}\\\n\\frac{g_{x_1}}{s_1}&\\frac{g_{x_2}}{s_2}&\\frac{g_{x_3}}{s_3}&g_{x_4}\\\\\n\\frac{g_{y_1}}{s_1}&\\frac{g_{y_2}}{s_2}&\\frac{g_{y_3}}{s_3}&g_{y_4}\\\\\n\\frac{g_{z_1}}{s_1}&\\frac{g_{z_2}}{s_2}&\\frac{g_{z_3}}{s_3}&g_{z_4}\\\\\n0&0&0&1\\\n\\end{bmatrix}\\\n\\]\n\n\n\n\nimport vtk\nimport numpy as np\nimport tempfile\nimport SimpleITK as sitk\nfrom pprint import pprint\n\n# Create a fake image with simpleitk\nimage = sitk.Image(28, 28, 28, sitk.sitkUInt8)\nimage.SetSpacing([0.5, 0.1, 0.2])\n\n# Set directio to:\n#  [1, 0, 0]\n#  [0, 0, 1]\n#  [0, 1, 0]\nimage.SetDirection((1., 0., 0., 0., 0., 1., 0., 1., 0.))\n\n# Write this dummy image for VTK to read\nwith tempfile.TemporaryDirectory() as f:\n    sitk.WriteImage(image, f + \"/temp.nii.gz\")\n\n    # read it back with vtk\n    reader = vtk.vtkNIFTIImageReader()\n    reader.SetFileName(f + \"/temp.nii.gz\")\n    reader.Update()\n \n    header = reader.GetNIFTIHeader()\n    vtkimage = reader.GetOutput()\n\nprint(\"ITK: \")\npprint(np.asarray(image.GetDirection()).reshape(3, 3))\nprint(\"VTK: \")\nprint(vtkimage.GetDirectionMatrix())"
  },
  {
    "objectID": "posts/medial-imaging/image-orientation.html#output",
    "href": "posts/medial-imaging/image-orientation.html#output",
    "title": "Stochastic - Particle Filtering and Markov Chain Monte Carlo",
    "section": "",
    "text": "ITK: \narray([[1., 0., 0.],\n       [0., 0., 1.],\n       [0., 1., 0.]])\nVTK: \nvtkMatrix3x3 (0000025FBE641A50)\n  Debug: Off\n  Modified Time: 1319\n  Reference Count: 2\n  Registered Events: (none)\n  Elements:\n    1   0   0\n    0   1   0\n    0   0   1\n\nSee how the two matrix are different? In fact VTK did not load the the direction into the matrix and therefore it remained as the default identity matrix."
  },
  {
    "objectID": "posts/medial-imaging/image-orientation.html#output-1",
    "href": "posts/medial-imaging/image-orientation.html#output-1",
    "title": "Stochastic - Particle Filtering and Markov Chain Monte Carlo",
    "section": "",
    "text": "From qform: \n vtkMatrix4x4 (0000025FBEC509E0)\n  Debug: Off\n  Modified Time: 1478\n  Reference Count: 1\n  Registered Events: (none)\n  Elements:\n    -1 -0.000261643 0.000261643 0 \n    0.000261643 3.42285e-08 1 0 \n    -0.000261643 1 3.42285e-08 0 \n    0 0 0 1 \n\n\nFrom SForm: \n vtkMatrix4x4 (0000025FBEC4FF40)\n  Debug: Off\n  Modified Time: 1484\n  Reference Count: 1\n  Registered Events: (none)\n  Elements:\n    -1 0 0 0 \n    0 -0 -1 0 \n    0 1 0 0 \n    0 0 0 1 \n\nNow if you round them up, you can see how this is corrected. However, you might also notice the qFrom matrix and sForm matrix gives you different results."
  },
  {
    "objectID": "posts/stochastic/monte-carlo.html",
    "href": "posts/stochastic/monte-carlo.html",
    "title": "Stochastic - Particle Filtering and Markov Chain Monte Carlo",
    "section": "",
    "text": "Caution\n\n\n\nUnder construction"
  },
  {
    "objectID": "posts/stochastic/monte-carlo.html#particle",
    "href": "posts/stochastic/monte-carlo.html#particle",
    "title": "Stochastic - Particle Filtering and Markov Chain Monte Carlo",
    "section": "Particle",
    "text": "Particle\n\nA particle can be seen as an evaluation of all random variables in a joint distribution.\n\nExamples:\n\\[\\displaystyle  \\text{Particle A: } [X=1, Y=2] \\\\ \\\\ \\text{Particle B: } [X=3, Y=1] \\\\ \\\\ \\text{where } X, Y \\in  {1, 2, 3}\\]"
  },
  {
    "objectID": "posts/stochastic/monte-carlo.html#markov-chain-monte-carlo-mcmc",
    "href": "posts/stochastic/monte-carlo.html#markov-chain-monte-carlo-mcmc",
    "title": "Stochastic - Particle Filtering and Markov Chain Monte Carlo",
    "section": "Markov Chain Monte Carlo (MCMC)",
    "text": "Markov Chain Monte Carlo (MCMC)\n\nMCMC refers to methods for randomly sample particles from a joint distribution with a Markov Chain."
  },
  {
    "objectID": "posts/stochastic/monte-carlo.html#particle-filtering",
    "href": "posts/stochastic/monte-carlo.html#particle-filtering",
    "title": "Stochastic - Particle Filtering and Markov Chain Monte Carlo",
    "section": "Particle Filtering",
    "text": "Particle Filtering\n\nParticle Filtering is also termed Sequential Monte Carlo. It refers to the process of repeatedly sampling, cast votes after each iteration based on sampled particles and modify the next sampling based on the votes in order to obtain the probability distribution of some un-observable states.\n\nFormally, let \\(x\\) be the unobservable states and \\(y\\) be the observable states related to \\(x\\). Suppose we receive observations of \\(y\\) at each time step \\(k\\), we can write the probability based on a Markov Chain:\n\\[\\displaystyle X_k|(X_{k-1} =x_{k-1}) \\propto p(x_k|x_{k-1})\\]\n\\[\\displaystyle Y_k|(X_{k} =x_{k}) \\propto p(y_k|x_{k})\\]\nBased on Chapman-Kolmogorov Equation and Bayes Theorem, the conditional probability distribution of latent states \\(x\\) based on priori knowledge \\(y\\) is:\n\\[\\displaystyle p(x_k|y_{1:k}) \\propto p(y_k|x_k)\\int_k p(x_k|x_{k-1})p(x_{k-1}|Y_{1:K-1})\\]"
  },
  {
    "objectID": "posts/stochastic/monte-carlo.html#gibbs-sampling",
    "href": "posts/stochastic/monte-carlo.html#gibbs-sampling",
    "title": "Stochastic - Particle Filtering and Markov Chain Monte Carlo",
    "section": "Gibbs Sampling",
    "text": "Gibbs Sampling\n% This quicksort algorithm is extracted from Chapter 7, Introduction to Algorithms (3rd edition)\n\\begin{algorithm}\n\\caption{Gibbs Sampling}\n\\begin{algorithmic}\n\\STATE \\textbf{Unknown}: Joint distribution $\\boldsymbol{X} = P(X_1, X_2, \\dots, X_n)$\n\\STATE \\textbf{Known}: Conditional Probability $P(X_i|X_{j\\neq i})$\n\\STATE \\textbf{Purpose}: Obtain an estimation of joint distribution $\\boldsymbol{X}$\n\n\\PROCEDURE{GibbsSampling}{$A, p, r$}    \n\\ENDPROCEDURE\n\\end{algorithmic}\n\\end{algorithm}\nUnknown: Joint distribution $P(X_1, X_2, , X_n) $\nKnown: Conditional Probability $P(X_i|_{others}) $\nGoal: Obtain an estimation of the joint distribution\nSteps:\n\nChoose an initial value  \\(X\\^0_i\\) for the variable of interest.\nCompute distribution by randomly fixing  “others” variable $P(X_j|X_i, _{others}) $ for some \\(j \\neq i\\)\nSample from distribution to get a realization of $X_j $, then update the conditional probability $P(X_i|X_j, _{others}) $ correspondingly,\nSample the target\nDo step 2 to step 3 repeatedly for all $j [1, n] \\neq i $ for k iterations.\n\nAn implementation is given below:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.integrate as integrate \nimport seaborn as sns \nimport tqdm.auto as auto  \n\n\"\"\" \nThis program demonstrates a two-variable Gibbs sampling iteration.  \nSuppose we are now interested in knowing P(X, Y), and both P(X|Y)  \nand P(Y|X) is known.  \n\nVariables: \n   PX, PY: \n        Pre-defined probability distribution of the two random variable. \n   properties: \n        Property of the pdf PX and PY, including the domain, resolution and a norm constant which is for plotting p.m.f  \n\"\"\" \n\ndef GenerateSamplers(): \n    \"\"\" \n    Creates a pair of random variables, one probability distribution is a \n    gaussian mixture, another is a simple gaussian with mean 0 and sd 10.  \n    Domain of the sample is set to -10 to 10  \n    :return [lambda: sample1, lambda: sample2: \n    \"\"\" \n    # Properties settings \n    resolution = 500 # 2000 partitions between whole domain \n    domain = [-10, 10] \n    gm = {'means': [-1, 2, -4], 'sds': [0.4, 8, 3], 'weight': [0.1, 0.6, 0.3]} \n    gy = {'means': 0, 'sds': 2}  \n    # define a normed gaussian \n    def Gaussian(mean, var, x): \n        return 1 / (var * np.sqrt(2 * np.pi)) * np.exp(-0.5 * (x - mean) ** 2 / var ** 2)  \n    \n    w = np.linspace(domain[0], domain[1], resolution)  \n    \n    # Generate pdf w/o normalization \n    _PX = lambda x: np.sum([gm['weight'][i]*Gaussian(gm['means'][i], gm['sds'][i], x) for i in range(len(gm['means']))], axis=0)  \n        _PY = lambda x: Gaussian(gy['means'], gy['sds'], x)  \n    \n    # Normalization \n    PX = lambda x: _PX(x) / integrate.quad(_PX, domain[0], domain[1])[0] # quad return mean, sd \n    PY = lambda x: _PY(x) / integrate.quad(_PY, domain[0], domain[1])[0]   \n    \n    # Create sampler functions \n    properties = {\n        'resolution': resolution, \n        'domain': domain, \n        'normConstant': (domain[1] - domain[0])/float(resolution - 1)\n    } \n    return PX, PY, properties\n\n'''main'''\nPX, PY, properties = GenerateSamplers() \nw = np.linspace(\n    properties['domain'][0], \n    properties['domain'][1], \n    properties['resolution']\n)  \n P_joint = lambda x: PX(x[0]) * PY(x[1]) \n PYcX = lambda x, y: P_joint((x, y)) / PX(x) # We somehow know this, here it is the arithmetic form, it could be established \n PXcY = lambda x, y: P_joint((x, y)) / PY(y) # with other methods (e.g., empirically estimated)      \n samples = [] \n x_k = float(np.random.choice(w)) # Initial sample \n for k in auto.trange(25000): \n\n  # Sample y_k based on X_k of the last iteration \n\n  _nPYcX = PYcX(x_k, w).sum() # normalize factor, for entertaining choice \n\n  y_k = np.random.choice(w, p=PYcX(x_k, w)/_nPYcX, size=1) # sample from new probability distribution \n\n   \n\n  # Now do it for x_k \n\n  _nPXcY = PXcY(w, y_k).sum() \n\n  x_k = np.random.choice(w, p=PXcY(w, y_k)/_nPXcY, size=1) \n\n  samples.append((float(x_k), float(y_k))) # Record the sample       \n # Plotting \n samples = np.stack(samples) \n joint_pdf = lambda x: PX(x[0]) * PY(x[1]) # This is what we are trying to estimate  \n joint_mesh = np.meshgrid(w, w) \n fig, ax = plt.subplots(1, 1, figsize=(6, 6)) \n CS = ax.contour(joint_mesh[0], joint_mesh[1], joint_pdf(joint_mesh), alpha=0.6, label=\"Estimated $P(X,Y)$\")      \n ax.scatter(samples[:, 0], samples[:, 1], s=2, alpha=0.2) \n ax.legend() \n plt.show() \n\nThe result is the following figure, where the sampled points are in blue and the contour of the joint distribution \\(P(X, Y)\\) is drawn:"
  }
]