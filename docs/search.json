[
  {
    "objectID": "posts/stochastic/simple-random-walk.html",
    "href": "posts/stochastic/simple-random-walk.html",
    "title": "Stochastic - Python Example of a Random Walk Process",
    "section": "",
    "text": "Simple random walk is the first step towards learning stochastic process. Essentially, we model a particle (of desired dimension) walking randomly as time (steps) goes by. Both the stride and the direction is random, but it follows a certain distribution.\n\n\nFirst, let us define the problem formally.\n\nTo implement a 1-D simulation of random walk \\(S(t)\\) within period \\(t \\in T = \\{0, 1, 2, \\dots, N\\}\\) in sample space \\(\\omega \\in \\mathbb{W}\\), with discrete stochastic process \\(X_T = \\{X_1, X_2, \\dots, X_n \\}\\) called steps of the random&gt; walk with the constrain \\(\\text{min} \\leq X(t) \\leq \\text{max}\\).\n\n\n\n\nThe random walk can be formally defined as follow:\n\\[S(t) = S_0 + \\sum_{t=1}^{n} X_t\\]\n\\(S_0\\) represents the initial value or start point of the random walk. Also, select that each elements of \\(X_T\\) can take on integer values between -5 and 5. Implementation\nThis simulation is equivalent to plotting \\(S(t)\\) against \\(t\\).\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate random numbers within the range -5 to 5\n# Note that randint(-5, high=6) generate range -5 to 5\nN =5100 ; MIN_STEP = -5; MAX_STEP = 6; S_0 = 0; # Define parameters of the simulation\nX_T = np.random.randint(MIN_STEP , high=MAX_STEP , size=N+1) # Generate the discrete stochastic process\nt = np.linspace(0, N, N+1) # Time domain\nS = [S_0 + np.sum(X_T[0:i]) for i in range(N+1)] # Calculate each S(t) of the random walk\nplt.plot(t, S, '-') # Plot\n\n\n\n\n\n\n\n\n\n\n\n\nHere’s an example of a random flight of 1000 steps sampled from a process similar to above, but in 2-dimension.\n\n\n\n\n\n\nIn this example, we select that each elements \\(X(t) \\in X_T, t \\in T = \\{0, 1, 2, \\dots, n\\}\\) to follows \\(-5 \\leq X(t) \\leq 5 \\text{ } \\forall \\text{ } t \\in T\\). It is, however, possible to introduce various other constrain to the process w.r.t. the application of your application. By modeling this distribution based on a known particle’s path, one might be able to estimate its future movement. An even more sophisticated method to sample the path is to decide the distribution again based on the location of the particle and its surrounding environment. In real life, physical phenomenon, such as Brownian motion, can also be described by random walk.\n\n\n\nIt is also worthwhile to note that both \\(S\\) and \\(X_T=\\{T_1, T_2, \\dots, T_n\\}\\) fulfills the definition of stochastic process with the state space being different. In otherwords, stochastic process is commutative in nature.",
    "crumbs": [
      "Home",
      "Posts",
      "Stochastic",
      "Stochastic - Python Example of a Random Walk Process"
    ]
  },
  {
    "objectID": "posts/stochastic/simple-random-walk.html#defining-the-problem",
    "href": "posts/stochastic/simple-random-walk.html#defining-the-problem",
    "title": "Stochastic - Python Example of a Random Walk Process",
    "section": "",
    "text": "First, let us define the problem formally.\n\nTo implement a 1-D simulation of random walk \\(S(t)\\) within period \\(t \\in T = \\{0, 1, 2, \\dots, N\\}\\) in sample space \\(\\omega \\in \\mathbb{W}\\), with discrete stochastic process \\(X_T = \\{X_1, X_2, \\dots, X_n \\}\\) called steps of the random&gt; walk with the constrain \\(\\text{min} \\leq X(t) \\leq \\text{max}\\).",
    "crumbs": [
      "Home",
      "Posts",
      "Stochastic",
      "Stochastic - Python Example of a Random Walk Process"
    ]
  },
  {
    "objectID": "posts/stochastic/simple-random-walk.html#formulation",
    "href": "posts/stochastic/simple-random-walk.html#formulation",
    "title": "Stochastic - Python Example of a Random Walk Process",
    "section": "",
    "text": "The random walk can be formally defined as follow:\n\\[S(t) = S_0 + \\sum_{t=1}^{n} X_t\\]\n\\(S_0\\) represents the initial value or start point of the random walk. Also, select that each elements of \\(X_T\\) can take on integer values between -5 and 5. Implementation\nThis simulation is equivalent to plotting \\(S(t)\\) against \\(t\\).\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate random numbers within the range -5 to 5\n# Note that randint(-5, high=6) generate range -5 to 5\nN =5100 ; MIN_STEP = -5; MAX_STEP = 6; S_0 = 0; # Define parameters of the simulation\nX_T = np.random.randint(MIN_STEP , high=MAX_STEP , size=N+1) # Generate the discrete stochastic process\nt = np.linspace(0, N, N+1) # Time domain\nS = [S_0 + np.sum(X_T[0:i]) for i in range(N+1)] # Calculate each S(t) of the random walk\nplt.plot(t, S, '-') # Plot",
    "crumbs": [
      "Home",
      "Posts",
      "Stochastic",
      "Stochastic - Python Example of a Random Walk Process"
    ]
  },
  {
    "objectID": "posts/stochastic/simple-random-walk.html#applications",
    "href": "posts/stochastic/simple-random-walk.html#applications",
    "title": "Stochastic - Python Example of a Random Walk Process",
    "section": "",
    "text": "In this example, we select that each elements \\(X(t) \\in X_T, t \\in T = \\{0, 1, 2, \\dots, n\\}\\) to follows \\(-5 \\leq X(t) \\leq 5 \\text{ } \\forall \\text{ } t \\in T\\). It is, however, possible to introduce various other constrain to the process w.r.t. the application of your application. By modeling this distribution based on a known particle’s path, one might be able to estimate its future movement. An even more sophisticated method to sample the path is to decide the distribution again based on the location of the particle and its surrounding environment. In real life, physical phenomenon, such as Brownian motion, can also be described by random walk.",
    "crumbs": [
      "Home",
      "Posts",
      "Stochastic",
      "Stochastic - Python Example of a Random Walk Process"
    ]
  },
  {
    "objectID": "posts/stochastic/simple-random-walk.html#nature-of-s-and-x_t",
    "href": "posts/stochastic/simple-random-walk.html#nature-of-s-and-x_t",
    "title": "Stochastic - Python Example of a Random Walk Process",
    "section": "",
    "text": "It is also worthwhile to note that both \\(S\\) and \\(X_T=\\{T_1, T_2, \\dots, T_n\\}\\) fulfills the definition of stochastic process with the state space being different. In otherwords, stochastic process is commutative in nature.",
    "crumbs": [
      "Home",
      "Posts",
      "Stochastic",
      "Stochastic - Python Example of a Random Walk Process"
    ]
  },
  {
    "objectID": "posts/stochastic/monte-carlo.html",
    "href": "posts/stochastic/monte-carlo.html",
    "title": "Stochastic - Particle Filtering and Markov Chain Monte Carlo",
    "section": "",
    "text": "Caution\n\n\n\nUnder construction",
    "crumbs": [
      "Home",
      "Posts",
      "Stochastic",
      "Stochastic - Particle Filtering and Markov Chain Monte Carlo"
    ]
  },
  {
    "objectID": "posts/stochastic/monte-carlo.html#particle",
    "href": "posts/stochastic/monte-carlo.html#particle",
    "title": "Stochastic - Particle Filtering and Markov Chain Monte Carlo",
    "section": "Particle",
    "text": "Particle\n\nA particle can be seen as an evaluation of all random variables in a joint distribution.\n\nExamples:\n\\[\\displaystyle  \\text{Particle A: } [X=1, Y=2] \\\\ \\\\ \\text{Particle B: } [X=3, Y=1] \\\\ \\\\ \\text{where } X, Y \\in  {1, 2, 3}\\]",
    "crumbs": [
      "Home",
      "Posts",
      "Stochastic",
      "Stochastic - Particle Filtering and Markov Chain Monte Carlo"
    ]
  },
  {
    "objectID": "posts/stochastic/monte-carlo.html#markov-chain-monte-carlo-mcmc",
    "href": "posts/stochastic/monte-carlo.html#markov-chain-monte-carlo-mcmc",
    "title": "Stochastic - Particle Filtering and Markov Chain Monte Carlo",
    "section": "Markov Chain Monte Carlo (MCMC)",
    "text": "Markov Chain Monte Carlo (MCMC)\n\nMCMC refers to methods for randomly sample particles from a joint distribution with a Markov Chain.",
    "crumbs": [
      "Home",
      "Posts",
      "Stochastic",
      "Stochastic - Particle Filtering and Markov Chain Monte Carlo"
    ]
  },
  {
    "objectID": "posts/stochastic/monte-carlo.html#particle-filtering",
    "href": "posts/stochastic/monte-carlo.html#particle-filtering",
    "title": "Stochastic - Particle Filtering and Markov Chain Monte Carlo",
    "section": "Particle Filtering",
    "text": "Particle Filtering\n\nParticle Filtering is also termed Sequential Monte Carlo. It refers to the process of repeatedly sampling, cast votes after each iteration based on sampled particles and modify the next sampling based on the votes in order to obtain the probability distribution of some un-observable states.\n\nFormally, let \\(x\\) be the unobservable states and \\(y\\) be the observable states related to \\(x\\). Suppose we receive observations of \\(y\\) at each time step \\(k\\), we can write the probability based on a Markov Chain:\n\\[\\displaystyle X_k|(X_{k-1} =x_{k-1}) \\propto p(x_k|x_{k-1})\\]\n\\[\\displaystyle Y_k|(X_{k} =x_{k}) \\propto p(y_k|x_{k})\\]\nBased on Chapman-Kolmogorov Equation and Bayes Theorem, the conditional probability distribution of latent states \\(x\\) based on priori knowledge \\(y\\) is:\n\\[\\displaystyle p(x_k|y_{1:k}) \\propto p(y_k|x_k)\\int_k p(x_k|x_{k-1})p(x_{k-1}|Y_{1:K-1})\\]",
    "crumbs": [
      "Home",
      "Posts",
      "Stochastic",
      "Stochastic - Particle Filtering and Markov Chain Monte Carlo"
    ]
  },
  {
    "objectID": "posts/stochastic/monte-carlo.html#gibbs-sampling",
    "href": "posts/stochastic/monte-carlo.html#gibbs-sampling",
    "title": "Stochastic - Particle Filtering and Markov Chain Monte Carlo",
    "section": "Gibbs Sampling",
    "text": "Gibbs Sampling\n% This quicksort algorithm is extracted from Chapter 7, Introduction to Algorithms (3rd edition)\n\\begin{algorithm}\n\\caption{Gibbs Sampling}\n\\begin{algorithmic}\n\\STATE \\textbf{Unknown}: Joint distribution $\\boldsymbol{X} = P(X_1, X_2, \\dots, X_n)$\n\\STATE \\textbf{Known}: Conditional Probability $P(X_i|X_{j\\neq i})$\n\\STATE \\textbf{Purpose}: Obtain an estimation of joint distribution $\\boldsymbol{X}$\n\n\\PROCEDURE{GibbsSampling}{$A, p, r$}    \n\\ENDPROCEDURE\n\\end{algorithmic}\n\\end{algorithm}\nUnknown: Joint distribution $P(X_1, X_2, , X_n) $\nKnown: Conditional Probability $P(X_i|_{others}) $\nGoal: Obtain an estimation of the joint distribution\nSteps:\n\nChoose an initial value  \\(X\\^0_i\\) for the variable of interest.\nCompute distribution by randomly fixing  “others” variable $P(X_j|X_i, _{others}) $ for some \\(j \\neq i\\)\nSample from distribution to get a realization of $X_j $, then update the conditional probability $P(X_i|X_j, _{others}) $ correspondingly,\nSample the target\nDo step 2 to step 3 repeatedly for all $j [1, n] \\neq i $ for k iterations.\n\nAn implementation is given below:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.integrate as integrate \nimport seaborn as sns \nimport tqdm.auto as auto  \n\n\"\"\" \nThis program demonstrates a two-variable Gibbs sampling iteration.  \nSuppose we are now interested in knowing P(X, Y), and both P(X|Y)  \nand P(Y|X) is known.  \n\nVariables: \n   PX, PY: \n        Pre-defined probability distribution of the two random variable. \n   properties: \n        Property of the pdf PX and PY, including the domain, resolution and a norm constant which is for plotting p.m.f  \n\"\"\" \n\ndef GenerateSamplers(): \n    \"\"\" \n    Creates a pair of random variables, one probability distribution is a \n    gaussian mixture, another is a simple gaussian with mean 0 and sd 10.  \n    Domain of the sample is set to -10 to 10  \n    :return [lambda: sample1, lambda: sample2: \n    \"\"\" \n    # Properties settings \n    resolution = 500 # 2000 partitions between whole domain \n    domain = [-10, 10] \n    gm = {'means': [-1, 2, -4], 'sds': [0.4, 8, 3], 'weight': [0.1, 0.6, 0.3]} \n    gy = {'means': 0, 'sds': 2}  \n    # define a normed gaussian \n    def Gaussian(mean, var, x): \n        return 1 / (var * np.sqrt(2 * np.pi)) * np.exp(-0.5 * (x - mean) ** 2 / var ** 2)  \n    \n    w = np.linspace(domain[0], domain[1], resolution)  \n    \n    # Generate pdf w/o normalization \n    _PX = lambda x: np.sum([gm['weight'][i]*Gaussian(gm['means'][i], gm['sds'][i], x) for i in range(len(gm['means']))], axis=0)  \n        _PY = lambda x: Gaussian(gy['means'], gy['sds'], x)  \n    \n    # Normalization \n    PX = lambda x: _PX(x) / integrate.quad(_PX, domain[0], domain[1])[0] # quad return mean, sd \n    PY = lambda x: _PY(x) / integrate.quad(_PY, domain[0], domain[1])[0]   \n    \n    # Create sampler functions \n    properties = {\n        'resolution': resolution, \n        'domain': domain, \n        'normConstant': (domain[1] - domain[0])/float(resolution - 1)\n    } \n    return PX, PY, properties\n\n'''main'''\nPX, PY, properties = GenerateSamplers() \nw = np.linspace(\n    properties['domain'][0], \n    properties['domain'][1], \n    properties['resolution']\n)  \n P_joint = lambda x: PX(x[0]) * PY(x[1]) \n PYcX = lambda x, y: P_joint((x, y)) / PX(x) # We somehow know this, here it is the arithmetic form, it could be established \n PXcY = lambda x, y: P_joint((x, y)) / PY(y) # with other methods (e.g., empirically estimated)      \n samples = [] \n x_k = float(np.random.choice(w)) # Initial sample \n for k in auto.trange(25000): \n\n  # Sample y_k based on X_k of the last iteration \n\n  _nPYcX = PYcX(x_k, w).sum() # normalize factor, for entertaining choice \n\n  y_k = np.random.choice(w, p=PYcX(x_k, w)/_nPYcX, size=1) # sample from new probability distribution \n\n   \n\n  # Now do it for x_k \n\n  _nPXcY = PXcY(w, y_k).sum() \n\n  x_k = np.random.choice(w, p=PXcY(w, y_k)/_nPXcY, size=1) \n\n  samples.append((float(x_k), float(y_k))) # Record the sample       \n # Plotting \n samples = np.stack(samples) \n joint_pdf = lambda x: PX(x[0]) * PY(x[1]) # This is what we are trying to estimate  \n joint_mesh = np.meshgrid(w, w) \n fig, ax = plt.subplots(1, 1, figsize=(6, 6)) \n CS = ax.contour(joint_mesh[0], joint_mesh[1], joint_pdf(joint_mesh), alpha=0.6, label=\"Estimated $P(X,Y)$\")      \n ax.scatter(samples[:, 0], samples[:, 1], s=2, alpha=0.2) \n ax.legend() \n plt.show() \n\nThe result is the following figure, where the sampled points are in blue and the contour of the joint distribution \\(P(X, Y)\\) is drawn:",
    "crumbs": [
      "Home",
      "Posts",
      "Stochastic",
      "Stochastic - Particle Filtering and Markov Chain Monte Carlo"
    ]
  },
  {
    "objectID": "posts/MISC/headless-server.html",
    "href": "posts/MISC/headless-server.html",
    "title": "Setting up a headless server",
    "section": "",
    "text": "This post is compiled from a list of steps which makes it possible to set up a headless server with nvidia GPU driver and glx support. The system is tested on Ubuntu 16.04 with nvidia-375.2 driver installed manually. Setting up XRDP",
    "crumbs": [
      "Home",
      "Posts",
      "MISC",
      "Setting up a headless server"
    ]
  },
  {
    "objectID": "posts/MISC/headless-server.html#prerequisite",
    "href": "posts/MISC/headless-server.html#prerequisite",
    "title": "Setting up a headless server",
    "section": "Prerequisite",
    "text": "Prerequisite\n\nlibtool\nautoconf\nautomake\ncmake-curses-gui",
    "crumbs": [
      "Home",
      "Posts",
      "MISC",
      "Setting up a headless server"
    ]
  },
  {
    "objectID": "posts/MISC/headless-server.html#installing-libjpeg-turbo8",
    "href": "posts/MISC/headless-server.html#installing-libjpeg-turbo8",
    "title": "Setting up a headless server",
    "section": "Installing libjpeg-turbo8",
    "text": "Installing libjpeg-turbo8\nClone source from Github then cd into directory.\nautoreconf\n./configure -prefix=/install/path\nmake -j 4\nmake install\nsudo ldconfig /install/path/lib\n\n\n\n\n\n\nNote\n\n\n\nNote that libtool is installed in anaconda, so you should use $ which libtool command to check which binary you are using if you get a version mismatch error here.",
    "crumbs": [
      "Home",
      "Posts",
      "MISC",
      "Setting up a headless server"
    ]
  },
  {
    "objectID": "posts/MISC/headless-server.html#installing-virtual-gl",
    "href": "posts/MISC/headless-server.html#installing-virtual-gl",
    "title": "Setting up a headless server",
    "section": "Installing Virtual GL",
    "text": "Installing Virtual GL\nClone source from Github then checkout 2.5.2 version with command $ git checkout -tag 2.5.2. It is recommended to use ccmake to config the project.\ncd /virutalgl/source\nmkdir build\ncd build;ccmake ..\nSet the following flags:\nCMAKE_CXX_FLAG=-L/Path/To/libjpeg-turbo8/lib\nCMAKE_C_FLAG=-L/Path/To/libjpeg-turbo8/lib\nCMAKE_CXX_FLAGS_RELEASE=-O3 -DNDEBUG -fPIC\nCMAKE_BUILD_TYPE=Release\nThen generate project and build, you should finish without any errors. Setup VirtualGL\nThe display of X server is configured by the file /etc/X11/xorg.conf, an example is given below:\n# nvidia-xconfig: X configuration file generated by nvidia-xconfig\n# nvidia-xconfig:  version 375.66  (buildmeister@swio-display-x86-rhel47-06)  Mon May  1 15:45:32 PDT 2017\nSection \"DRI\"\n        Mode 0666\nEndSection\n\n\nSection \"ServerLayout\"\n    Identifier     \"Layout0\"\n    Screen      0  \"Screen0\" 0 0\n    InputDevice    \"Keyboard0\" \"CoreKeyboard\"\n    InputDevice    \"Mouse0\" \"CorePointer\"\nEndSection\n\nSection \"Files\"\nEndSection\n\nSection \"InputDevice\"\n\n    # generated from default\n    Identifier     \"Mouse0\"\n    Driver         \"mouse\"\n    Option         \"Protocol\" \"auto\"\n    Option         \"Device\" \"/dev/psaux\"\n    Option         \"Emulate3Buttons\" \"no\"\n    Option         \"ZAxisMapping\" \"4 5\"\nEndSection\n\nSection \"InputDevice\"\n\n    # generated from default\n    Identifier     \"Keyboard0\"\n    Driver         \"kbd\"\nEndSection\n\nSection \"Monitor\"\n    Identifier     \"Monitor0\"\n    VendorName     \"Unknown\"\n    ModelName      \"Unknown\"\n    HorizSync       28.0 - 33.0\n    VertRefresh     43.0 - 72.0\n    Option         \"DPMS\"\nEndSection\n\nSection \"Device\"\n    Identifier     \"Device0\"\n    Driver         \"nvidia\"\n    VendorName     \"NVIDIA Corporation\"\n    BoardName      \"TITAN Xp\"\n    Option \"Coolbits\" \"5\"\nEndSection\n\nSection \"Screen\"\n    Identifier     \"Screen0\"\n    Device         \"Device0\"\n    Monitor        \"Monitor0\"\n    DefaultDepth    24\n    Option         \"UseDisplayDevice\" \"None\" #This is required for headless computer\n    SubSection     \"Display\"\n        Virtual     1920 1080\n        Depth       24\n    EndSubSection\nEndSection",
    "crumbs": [
      "Home",
      "Posts",
      "MISC",
      "Setting up a headless server"
    ]
  },
  {
    "objectID": "posts/MISC/headless-server.html#ipython-input-doesnt-have-any-keyboard-input",
    "href": "posts/MISC/headless-server.html#ipython-input-doesnt-have-any-keyboard-input",
    "title": "Setting up a headless server",
    "section": "Ipython input doesn’t have any keyboard input",
    "text": "Ipython input doesn’t have any keyboard input\nThis happens because keyboard was not exported to X11. Export QT_XKB_CONFIG_ROOT=/usr/share/X11/xkb",
    "crumbs": [
      "Home",
      "Posts",
      "MISC",
      "Setting up a headless server"
    ]
  },
  {
    "objectID": "posts/MISC/headless-server.html#kernel-version-mis-match",
    "href": "posts/MISC/headless-server.html#kernel-version-mis-match",
    "title": "Setting up a headless server",
    "section": "Kernel version mis-match",
    "text": "Kernel version mis-match\nTry rebooting, this happens when you update the driver",
    "crumbs": [
      "Home",
      "Posts",
      "MISC",
      "Setting up a headless server"
    ]
  },
  {
    "objectID": "posts/MISC/fourier-transform/discrete-fourier-transform.html",
    "href": "posts/MISC/fourier-transform/discrete-fourier-transform.html",
    "title": "Discrete Fourier Transform",
    "section": "",
    "text": "For every continuous function \\(f(x)\\), the sampling process can be expressed as:\n\\[ \\bar{f}(x) = \\sum_{k=-\\infty}^{\\infty} f(x) \\cdot \\delta(x - kT) = f(nT) \\cdot \\text{III}_T(x) \\tag{1} \\]\nwhere \\(\\bar{f}(x)\\) is the sampled discrete function. The upper equation can be seen as a convolution.",
    "crumbs": [
      "Home",
      "Posts",
      "MISC",
      "Imaging - Convolution between an image and a filtering kernel",
      "Discrete Fourier Transform"
    ]
  },
  {
    "objectID": "posts/MISC/fourier-transform/discrete-fourier-transform.html#sampling-from-continuous-function",
    "href": "posts/MISC/fourier-transform/discrete-fourier-transform.html#sampling-from-continuous-function",
    "title": "Discrete Fourier Transform",
    "section": "",
    "text": "For every continuous function \\(f(x)\\), the sampling process can be expressed as:\n\\[ \\bar{f}(x) = \\sum_{k=-\\infty}^{\\infty} f(x) \\cdot \\delta(x - kT) = f(nT) \\cdot \\text{III}_T(x) \\tag{1} \\]\nwhere \\(\\bar{f}(x)\\) is the sampled discrete function. The upper equation can be seen as a convolution.",
    "crumbs": [
      "Home",
      "Posts",
      "MISC",
      "Imaging - Convolution between an image and a filtering kernel",
      "Discrete Fourier Transform"
    ]
  },
  {
    "objectID": "posts/MISC/fourier-transform/discrete-fourier-transform.html#discrete-fourier-transform",
    "href": "posts/MISC/fourier-transform/discrete-fourier-transform.html#discrete-fourier-transform",
    "title": "Discrete Fourier Transform",
    "section": "Discrete Fourier Transform",
    "text": "Discrete Fourier Transform\nThen the Fourier transform of this equation would be\n\\[ \\mathscr{F}[\\bar{f}(x)] = \\hat{f}(k) * \\left[ \\frac{1}{T} \\text{III}\\_{1/T}(k) \\right]=\\frac{1}{T} \\sum\\_{n=-\\infty}^{\\infty} \\hat{f}(k - \\frac{n}{T}) \\tag{2.1}\\]\nwhere \\(\\hat{f}(k) = \\mathscr{F} [f(x)]\\).\nEq.[2.1] actually looks like copies of \\(\\hat{f}(k)\\) at \\(1/T\\) intervals. So you can say that sampling in the x-domain is equivalent to shift and paste in the k-domain. One problem is that if your $\\hat{f}(k) $ has components with higher then a certain frequency, it will overlap with the shifted \\(\\hat{f}(k - 1/T)\\). This overlap is un-resolvable and would create what we called Aliasing artifacts which we will talk about this later.",
    "crumbs": [
      "Home",
      "Posts",
      "MISC",
      "Imaging - Convolution between an image and a filtering kernel",
      "Discrete Fourier Transform"
    ]
  },
  {
    "objectID": "posts/MISC/fourier-transform/discrete-fourier-transform.html#x-vs-k-space-resolution-conversion",
    "href": "posts/MISC/fourier-transform/discrete-fourier-transform.html#x-vs-k-space-resolution-conversion",
    "title": "Discrete Fourier Transform",
    "section": "X vs k-space Resolution Conversion",
    "text": "X vs k-space Resolution Conversion\nNow moving back a little bit to Eq.[2.1]. I real life you won’t have infinite points. Suppose you sample $ N $ points from $ f(x)$ at a frequency \\(v\\) within the domain \\(x\\in [0, N-1]\\). The normalized DFT is defined by:\n\\[ X_k = \\sum\\_{n=0}^{N-1}x_n e^{-i 2 \\pi  k n / N } \\]\n\\[ x_n = \\frac{1}{N}\\sum\\_{n=0}^{N-1}X_k e^{i 2 \\pi  k n/N } \\]\nwhere \\(x_n, X_k\\) is the n-th sample and the k-th coefficient respectively. So which frequencies does the k-th bin represents?\nFor a simple sinusoidal function $f(x) = \\cos (2\\pi \\cdot 3x) $ we can easily identify the frequency of this function is 3. Similar for \\(f(x) = \\exp (i 2 \\pi 3 x)\\). Now consider the following expansion of the inverse DFT:\n\\[ \\displaystyle x_n = \\frac{1}{N} \\left[ X_0 +  X_1 \\cdot e^{i 2 \\pi n \\frac{1}{N} } + X_2 \\cdot e^{i 2 \\pi  n \\frac{2}{N} }  + \\cdots \\right] \\tag{2.2} \\]\nOne can immediately identify \\(X_k\\) is the coefficient of a \\[ k/N\\] frequency function. The resolution of the frequency domain is therefore \\(1/N\\), for example if you sample 512 points from 0mm to 511mm, each bin in the k-space resolves to \\(\\frac{1}{512} mm^{-1}\\) with the bound \\([0, 1]mm^{-1}\\),\nNow things gets trickier if you are not using 1mm as your sampling interval, but this problem is essentially an axis re-scale problem. Say you define \\(2y = x\\) then \\(f(x) = f(2y)\\), very straight forward. Now imagine you sample 512 points from 0 to 255mm. Just re-scale our previous result by substituting \\(n \\rightarrow 2n'\\):\n\\[ x\\_{n} = x\\_{2n'} = \\frac{1}{N} \\sum\\_{n'=0}^{N-1} X_k e^{i 2 \\pi  k \\cdot 2n'/N } \\]\nBy the same logic, the k-th bin corresponds to \\(\\frac{2k}{N}\\). Notice how the increase in x-space sampling rate reduce the k-space resolution while increase the k-space range. This is actually very logical, increasing your sampling rate allows one to discovers higher frequencies component. Imaging you sample at 1Hz, you will never know there are a 5Hz component in the sampling target.\n\n\n\n\n\n\nFigure 1\n\n\n\nWe therefore draw the conclusion about x-space vs k-space resolution conversion:\n\\[ \\Delta k = \\frac{\\Delta x}{N} \\tag{2.3}\\]\nwhere $ N $ is the number of sampled points, \\(\\Delta k, \\Delta x\\) are the resolution of k and x-space respectively. Note that in k-space, the range always starts from 0 regardless of the range of $ x$ sampled.",
    "crumbs": [
      "Home",
      "Posts",
      "MISC",
      "Imaging - Convolution between an image and a filtering kernel",
      "Discrete Fourier Transform"
    ]
  },
  {
    "objectID": "posts/medical-imaging/intensity-normalization/01_intensity-normalization-toc.html",
    "href": "posts/medical-imaging/intensity-normalization/01_intensity-normalization-toc.html",
    "title": "Intensity normalization (Part 1) - General Info",
    "section": "",
    "text": "The quantitative analysis of medical images is predicated on a fundamental assumption: the properties being analyzed are consistent across different patients, thereby allowing patterns identified within one patient group to be applicable to another without the need for transformation. In practice, however, this assumption is frequently violated due to a multitude of environmental variables that are challenging to control, including temperature, pressure, exposure to light, level of magnetism, air circulation …etc. In the field of genomics, the influence of such variations is acknowledged as the “batch effect,” referring to the inherent systematic biases in each batch of DNA sequencing data that are unrelated to the histological or pathological features of interest. Although imaging lacks a specific term for this phenomenon, it is subject to a comparable effect, compounded by additional physical constraints inherent to the imaging techniques themselves.\nTake, for instance, the MRI weighted sequences. The intensities of MRI T1-weighted and T2-weighted images are described as “weighted” precisely because they do not maintain a consistent correlation with a fixed intensity scale. As a result, identical tissue types may exhibit variable intensity values across different scans. Contrastingly, CT imaging does not typically reflect this issue, as the intensities in CT images are anchored to the physical densities of the scanned objects and are denoted in Hounsfield Units (HU). Despite this, CT imaging is not entirely immune to environmental influences.\nGiven these challenges, normalizing the intensity of medical images becomes an integral step prior to any quantitative analysis. Such normalization ensures that the patterns discerned from one cohort of patients can be validly extended to another, enhancing the reliability and translatability of the findings.",
    "crumbs": [
      "Home",
      "Posts",
      "Medical Imaging",
      "Intensity Normalization",
      "Intensity normalization (Part 1) - General Info"
    ]
  },
  {
    "objectID": "posts/medical-imaging/intensity-normalization/01_intensity-normalization-toc.html#assumption-of-a-reference-standard",
    "href": "posts/medical-imaging/intensity-normalization/01_intensity-normalization-toc.html#assumption-of-a-reference-standard",
    "title": "Intensity normalization (Part 1) - General Info",
    "section": "Assumption of a reference standard",
    "text": "Assumption of a reference standard\n\nVoxels sharing identical intensity values within a single image should maintain their equivalence in intensity following the normalization process.\nIdentical tissues should exhibit consistent intensity values both within the same scanner and across different scanners when the same or equivalent sequences are used.\n\n Advance discussion\n\n\n\n\n\n\nTip\n\n\n\nWe mentioned we think that the same type of tissue should show up with the same value in MRI scans, but people are different, and so is their tissue. Some tissues, like brain tissue, don’t vary much between people, so brain scan values are pretty consistent and well-adjusted in neuroscience studies. But for other tissues, like tumors, each one is unique. We can’t assume they’re all the same. We don’t have a solid reference to confirm if our assumptions about tissue values are right or wrong. What matters most is if adjusting these values helps us better understand and measure the features we see in scans. So, it’s important to be flexible with these assumptions.",
    "crumbs": [
      "Home",
      "Posts",
      "Medical Imaging",
      "Intensity Normalization",
      "Intensity normalization (Part 1) - General Info"
    ]
  },
  {
    "objectID": "posts/medical-imaging/intensity-normalization/01_intensity-normalization-toc.html#histogram-based-intensity-mapping",
    "href": "posts/medical-imaging/intensity-normalization/01_intensity-normalization-toc.html#histogram-based-intensity-mapping",
    "title": "Intensity normalization (Part 1) - General Info",
    "section": "Histogram-based intensity mapping",
    "text": "Histogram-based intensity mapping\nMost intensity normalization algorithms complies with the first assumption would aim to create an intensity mapping. Formally, normalization is the process of finding a map between the instance distribution (histogram) \\(d[I(x), v]\\) to the reference distribution \\(\\mathscr{D}(v)\\):\n\\[\n\\text{Normalizaiton}[I(x)]: d[I(x), v]\\mapsto D(v)\n\\]\nwhere \\(d[I(x), v]\\) is the distribution of image \\(I(x)\\) and \\(x\\in X\\) with \\(X\\) being the domain where the Image is defined.\n\n\n\n\n\n\n\n\nFigure 1: Example of histogram of gray-scale Lena transformed",
    "crumbs": [
      "Home",
      "Posts",
      "Medical Imaging",
      "Intensity Normalization",
      "Intensity normalization (Part 1) - General Info"
    ]
  },
  {
    "objectID": "posts/medical-imaging/intensity-normalization/01_intensity-normalization-toc.html#methodologies",
    "href": "posts/medical-imaging/intensity-normalization/01_intensity-normalization-toc.html#methodologies",
    "title": "Intensity normalization (Part 1) - General Info",
    "section": "Methodologies",
    "text": "Methodologies\nThere are really many ways to construct the histogram mapping, most commonly involve linear mapping and piecewise-linear mapping.\n\nLinear mapping\nLinear mapping is easy to understand, it means the the same transform is applied across the entire intensity domain. This normalize the tissue value of the foreground tissues to have a standardized mean of \\(\\mu\\) and a variance of \\(\\sigma\\):\n\\[\nI'_{ij}=\\frac{I_{ij} - \\mu}{\\sigma}\n\\]\nwhere \\(I_{ij}\\) is the intensity at index \\((i,j)\\).\n\nDeciding \\(\\mu\\) and \\(\\sigma\\)\nThe main idea of this linear mapping is to shift, expand or compress the entire histogram. You would have do decide how based on the property of the scanned tissues. Here are some common choices.\n\nZ-score normalization\nThe mean and variance can be arbitrarily defined. When we set \\(\\mu=\\text{E}|\\textbf{I}(X)|\\) and \\(\\sigma = \\text{Var}[\\textbf{I}(X)]\\); \\(X\\subset U\\) is the domain of foreground tissues in the image, this become Z-score normalization, which sets all images’ mean and variance to 0 and 1, respectively. This corresponds to the StandardScaler in the package sklearn. Note that sklearn does not deal with foreground tissue masks.\nAlternatively, you can also chose to reference the averaged foreground intensity across the entire batch, i.e., calculating \\(\\mu\\) and \\(\\sigma\\) from the entire dataset. This averages the mean and variance across all data.\n\n\nMin-max scaling\nThis aims to scale the minimal and maximum intensity value in each image to be a fixed range, say \\([a, b],b&gt;a\\). Then we can set \\(\\mu=\\min[\\textbf{I}(X)]\\) and \\(\\sigma=\\{\\max[\\textbf{I}(X)] - \\min[\\textbf{I}(X)]\\}/ (b -a)\\). The MinMaxScaler in sklearn is similar, but not exactly the same. The main difference being that MinMaxScaler scales the range to between 0 to 1, the case presented here is more general and scales the range to between \\(a\\) and \\(b\\).\n\n\n\n\n\n\nThis method is very sensitive to outliers. You should make sure the outliers are cleaned if you want to use this method directly. MinMaxScaler does not perform this clean automatically.\n\n\n\n\n\nIQR scaling\nScales the histogram based on location of the quartiles to specific values \\([a, b], b&gt;a\\). Simply set \\(\\mu=Q[\\textbf{I}(X), 0.25]\\) and\n\\[\n\\sigma=\\frac{Q[\\textbf{I}(X),0.75] - Q[\\textbf{I}(X),0.25]}{b-a}\n\\]\nThis method moves median to 0 and the IQR to \\([a, b]\\). This is suitable for images with very skewed histogram and with distinct outliers. Again, this does not remove the outliers for you.\nNote that this does guarantee the median to be scaled to 0. If you wish to do so, you can set \\(\\mu=\\text{Median}[\\text{I}(X)]\\), and scaling to a value based on IQR.\n\n\n\n\n\nPiece-wise linear mapping\nNow the above linear mappings mainly cater for histogram with roughly a normal distribution. This is often not the case. As you can see in the histogram of lena Figure 1, there are plenty of peaks at different locations. Now if we assume these peaks in different people represents the same set of tissues, because of the contrast between tissues is consistent, we can further align the peaks to normalize the image. This requires shifts of the histogram at different peak locations, hence, piece-wise linear mapping means the intensity domain is partitioned into multiple segments, and different linear transform is applied to these segments.\n\\[\n\\begin{matrix}\nI'(X_i)=N_i[I(X_i)] & \\forall i\\in \\mathbb{Z}^+, x_i\\in X, & \\text{where } x_i \\leq x &lt;x_{i+1}\n\\end{matrix}\n\\]\n\\(I'(X_i)\\) is the normalized image corresponding to normalization of domain \\(X_i\\), i.e., the \\(i\\)-th segment in the intensity profile, and \\(N_i(.)\\) is the normalization operation for the \\(i\\)-th segment.\nPiece-wise mapping allows for a better fit of histogram, but also risk distorting the contrast. Therefore, the linear mapping used for each segment needs to be designed carefully. This usually involve referencing the specific imaging modality of interest.",
    "crumbs": [
      "Home",
      "Posts",
      "Medical Imaging",
      "Intensity Normalization",
      "Intensity normalization (Part 1) - General Info"
    ]
  },
  {
    "objectID": "posts/medical-imaging/intensity-normalization/01_intensity-normalization-toc.html#mri",
    "href": "posts/medical-imaging/intensity-normalization/01_intensity-normalization-toc.html#mri",
    "title": "Intensity normalization (Part 1) - General Info",
    "section": "MRI",
    "text": "MRI\nSee Intensity normalization - MRI",
    "crumbs": [
      "Home",
      "Posts",
      "Medical Imaging",
      "Intensity Normalization",
      "Intensity normalization (Part 1) - General Info"
    ]
  },
  {
    "objectID": "posts/medical-imaging/image-orientation.html",
    "href": "posts/medical-imaging/image-orientation.html",
    "title": "Medical imaging orientation in VTK and ITK",
    "section": "",
    "text": "Directions and orientation of medical images are indeed a very confusing problem, especially when you are trying to handle multiple images (then relative spacing becomes a concern). There are also multiple conventions to view these image (e.g. neurologist view and radiologist view, which defines the right-handness of the image), adding further confusions to the problem.\nFor most volume storage format, there are four elements that define the scanned object and its physical location:\n\nVoxel data\nImage Origin\nImage Spacing\nDirection\n\nThese data are usually stored in the header of the scan files. Voxel intensity array is always stored in ijk space, which might not accurately represents the actual physical location each wpixel. Additionally, direct mapping of ijk space to Cartesian xyz space can be ineffective in terms of storage. For instance, imagine you gave a metal bar two CT scans, one with it aligned perfectly with the machine’s rotational axis, the second with it purposely tilted 60° from the axis. The most effective way for the scanner to store these two images digitally is obviously by drawing the grid with the z-axis along the bar so that the voxel data stored is actually identically. However, we need to account for the 60° tilt of the second scan. This is achieved by specifying the direction of the image. Similar rationale for the other attributes mentioned.\nThis post aims to analyze the data structure of two commonly used data format, DICOM and Nifty, in storing direction and physical information that maps ijk indices to xyz coordinates. Additionally, I will also look into two renowned toolkits, ITK and VTK, which are frequently used for handling medical images.\n\n\n\nWe consider only two of the most commonly seen medical image format, namely NIfTI and DICOM images. These two image formats use different hierarchy to store the direction information. While NIfTI images use QForm matrix and SForm matrix to store direction information, DICOM images use the DICOM tag stating cosines of angles. This topic will be discussed in detail in later sections.\n\n\nThree loading methods you will likely use at some point are considered in this post:\n\nLoad as ITK image.\nLoad as ITK image using ImageToVTKImageFilter then convert it to vtkImageData.\nLoad as vtkImageData.\n\nTabulating their characters w.r.t. image directions:\n\n\n\n \nHas Direction\nHas Origin\nHas Orientation\n\n\n\n\nITK Image Loader\nYes\nYes\nYes\n\n\nITK to VTK\nNo\nYes\nNo\n\n\nVTK Image Loader\nNo\nNo\nNo\n\n\n\n\n\n\n\n\n\nImage origin is the position of the corner of the first stored voxel (i.e. index ijk = [0,0,0]). Which of the 8 corners depends on whether the image is righthanded or lefthanded, this give rise to the orientation definition.\n\n\n\n\n\nImage orientation refers to the specification of the three principle axis w.r.t. voxel ijk indices. A common notion used in radiology would be the permutation of six direction Left/Right (LR), Superior/Inferior (SI) and Anterior/Posterior (AP), e.g. RAI/LPS…etc., marking whether positive xyz move towards which of the above 6 directions. This, however, doesn’t fully defines the image’s xyz-axis yet and this only gives people a rough idea what is the “closest” orientation the xyz axis are pointing at. For example, an RAI image means that increase in \\(i, j, k\\) indices moves towards right, anterior and inferior direction, but they does not necessarily points directly to it. An image with a direction \\(i\\) mapping to Cartesian vector \\(\\vec{x} = [0.9, 0.1, 0]\\) will merit it to take “R” as this vector points dominantly towards \\(x\\).\n\nIn summary, image orientation can be seen as the definition of a matrix that maps ijk to xyz:\n\\[\n\\vec{v}_{xyz}=M\\vec{v}_{ijk}\n\\]\n\n\n\n\n\n\n\n\n\n\nImage origin of DICOM image is stored in the DICOM tag “Image Position (Patient) (0020,0032)”, which is a simple offset against all the voxel coordinates. Note that each slices has it’s own value of Image Position, but we only concern the first slice.\n\n\n\n\n\nImage direction of DICOM image is stored in DICOM tag “Image Orientation (Patient) (0020,0037)”, it is defined as the cosines of angle of three axis. Every slice of the same series (except for scouts) should have the same orientation.\n\n\n\n\n\n\n\n\nThe image origin of Nifti files are stored in the quantity qoffsets and the fourth element of srow_x/y/z in the header. Usually, they are the same so you can just use one of them.\n\n\n\n\n\nImage directions of Nifti files are defined by two matrices in the header, namely SForm Matrix and QForm Matrix, which, in most cases, are the essentially the same matrix except SForm Matrix includes the spacing . The usage of these two matrices are defined by two quantity called sform code and qform code, and the QForm Matrix is defined by a vector quaternion. According to the documentation, three methods are mentioned for using these matrices depending on whether the qform code and sform code are 0 or not.\n\n\n\n\n\n\n\n\nGenerally DICOM can be seen as a series of 2D images with extra information stored in headers. However, an image can be sliced along different normals, for example sagital, coronal or axial. These three direction are not the only direction that a DICOM series can take on, in fact, a DICOM series can be sliced along any directions. The slice direction is decided by the  “Image Orientation (Patient) (0020,0037)” DICOM tag, which specify the reference frame. The “Image Orientation(Patient)“ is a 6-element tuple consist of two vectors which describes the axis of the direction for row and column of that particular slice. For example, if your slices are Axial slice, then the two vectors defines sagital and coronal directions.\nIf the tag is in the format [a\\b\\c\\d\\e\\f], then its most likely two flattened 3-vectors. The cross product of the two vectors gives the third column of the rotational matrix, i.e:\n\\[\n\\begin{align}\n\\vec{v}_x &= \\begin{bmatrix} a&b&c \\end{bmatrix}^T \\\\\n\\vec{v}_y &= \\begin{bmatrix} d&e&f \\end{bmatrix}^T \\\\\n\\vec{v}_z &= \\vec{v}_x \\times \\vec{v}_y \\end{align}\n\\]\n\n\n\nNifti stands for Neuroimaging Informatics Technology Initiative. It was initially designed to store brain MRI data. It is very suitable for dealing with 3D data because, unlike DICOM, the images from the same scan series are stored in a single file instead of multiple files.\nThe metadata of Nifti contains necessary information to establish the physical location of the patient during his/her scan. To properly load the image data, we first examine the qform and sform code, that perform action according the the conditions below.\nIf qform = 0\nThis method is for backwards compatibility only. When qform code = 0, the matrices will not be used to define the orientation and direction, the index to coordinate matrix is simply the following:\n\\[\n\\vec{r} = \\vec{s}^{\\text{ }T} \\cdot \\vec{I}\\\n\\]\nwhere r is physical coordinate, s is spacing vector and I is the index vector (i.e. $ = [i, j, k] $). In this case, no further transformation is need after image loading (or the transform is Identity matrix).\n\n\nWhen qform code is greater than zero, this method applies. This method involves constructing a rotation matrix using a so called quaternion representation.\nDefinition of quaternion:\n\\[ \\vec{q} =\\begin{bmatrix} a\\\\b\\\\c\\\\d \\end{bmatrix} \\]\nwhere we require that:\n\\[  a^2 + b^2 + c^2 + d^2 = 1\\]\nSo having three of the four values gives the remaining one. In nifti format, only b, c, d are given in the header, and we calculate a by the formula:\n\\[\na = \\sqrt{1 - b^2 - c^2 - d^2}\n\\]\nUsing a, b, c, d the rotational matrix R is defined as:\n\\[ \\displaystyle\n\\text{R}_q = \\begin{bmatrix}\na^2 + b^2 - c^2 - d^2&2bc - 2ad&2bd+2ac\\\\\n2bc + 2ad& a^2 + c^2 - b^2 - d^2& 2cd - 2ab\\\\\n2bd - 2ac&2cd + 2ab&a^2 + d^2 - c^2 - b^2\\\\\n\\end{bmatrix}\n\\]\nThe index to physical position formula would therefore be:\n\\[ \\displaystyle\n\\vec{r} = \\text{R}_q [\\vec{s}^{\\text{ }T}\\cdot \\vec{I}] + \\vec{q}_0\\\n\\]\nwhere q_0 stands for qoffsets.\nHence the affine matrix vtkMatrix4x4 used for transform in VTK would be\n\\[ \\displaystyle\n\\text{A} = \\begin{bmatrix}\\\na^2 + b^2 - c^2 - d^2&2bc - 2ad&2bd+2ac&q_x\\\\\n2bc + 2ad& a^2 + c^2 - b^2 - d^2& 2cd - 2ab&q_y\\\\\n2bd - 2ac&2cd + 2ab&a^2 + d^2 - c^2 - b^2&q_z\\\\\n0&0&0&1\\\n\\end{bmatrix}\\\n\\]\n\n\n\nThis can coexist with qform &gt; 0 (i.e. both qform &gt; 0 and sform &gt; 0 can coexist, qform describe the transformation from data to scanning grid, sform describe the transformation from data to standard grid). The sform matrix is stored separately in three vectors: SRowX, SRowY and SRowZ, which we will denote as: $ _x, _y, _z $.\n\\[\n\\displaystyle \\text{R}_s =\\begin{bmatrix}g_{x_1}&g_{x_2}&g_{x_3}\\\\g_{y_1}&g_{y_2}&g_{y_3}\\\\g_{z_1}&g_{z_2}&g_{z_3} \\end{bmatrix}\n\\]\nSince the SForm matrix already include spacing, there are no needs to multiply spacing for each index. The index to physical position formula would therefore be:\n\\[\n\\displaystyle\n\\vec{r} = \\text{R}_s \\vec{I} + \\begin{bmatrix} g_{x_4}\\\\g_{y_4}\\\\g_{z_4} \\end{bmatrix}\\\n\\]\nThe affine matrix used for transform in vtkMatrix4x4 would then be the rotational matrix divided by spacing spacing vector s:\n\\[\n\\displaystyle\\\n\\text{A} =\\\n\\begin{bmatrix}\\\n\\frac{g_{x_1}}{s_1}&\\frac{g_{x_2}}{s_2}&\\frac{g_{x_3}}{s_3}&g_{x_4}\\\\\n\\frac{g_{y_1}}{s_1}&\\frac{g_{y_2}}{s_2}&\\frac{g_{y_3}}{s_3}&g_{y_4}\\\\\n\\frac{g_{z_1}}{s_1}&\\frac{g_{z_2}}{s_2}&\\frac{g_{z_3}}{s_3}&g_{z_4}\\\\\n0&0&0&1\\\n\\end{bmatrix}\\\n\\]\n\n\n\n\nimport vtk\nimport numpy as np\nimport tempfile\nimport SimpleITK as sitk\nfrom pprint import pprint\n\n# Create a fake image with simpleitk\nimage = sitk.Image(28, 28, 28, sitk.sitkUInt8)\nimage.SetSpacing([0.5, 0.1, 0.2])\n\n# Set directio to:\n#  [1, 0, 0]\n#  [0, 0, 1]\n#  [0, 1, 0]\nimage.SetDirection((1., 0., 0., 0., 0., 1., 0., 1., 0.))\n\n# Write this dummy image for VTK to read\nwith tempfile.TemporaryDirectory() as f:\n    sitk.WriteImage(image, f + \"/temp.nii.gz\")\n\n    # read it back with vtk\n    reader = vtk.vtkNIFTIImageReader()\n    reader.SetFileName(f + \"/temp.nii.gz\")\n    reader.Update()\n \n    header = reader.GetNIFTIHeader()\n    vtkimage = reader.GetOutput()\n\nprint(\"ITK: \")\npprint(np.asarray(image.GetDirection()).reshape(3, 3))\nprint(\"VTK: \")\nprint(vtkimage.GetDirectionMatrix())\n\n\n\n\n\n\nITK: \narray([[1., 0., 0.],\n       [0., 0., 1.],\n       [0., 1., 0.]])\nVTK: \nvtkMatrix3x3 (0000025FBE641A50)\n  Debug: Off\n  Modified Time: 1319\n  Reference Count: 2\n  Registered Events: (none)\n  Elements:\n    1   0   0\n    0   1   0\n    0   0   1\n\nSee how the two matrix are different? In fact VTK did not load the the direction into the matrix and therefore it remained as the default identity matrix.\n\n\n\n\n\n#Spacing\ns = image.GetSpacing()\ns = np.array([s[0], s[1], s[2], 1])\n \n#Origin (Set this to [0, 0, 0, 1] if you are using itkvtkConnector)\nori = np.array( [header.GetQOffsetX(), header.GetQOffsetY(), header.GetQOffsetZ(), 1])\n \n'''Directions'''\n# Use QForm matrix\nif (header.GetQFormCode() &gt; 0):\n    b = header.GetQuaternB()\n    c = header.GetQuaternC()\n    d = header.GetQuaternD()\n    a = np.sqrt(1 - b*b - c*c - d*d)\n    A = np.array([\n        [a*a + b*b - c*c - d*d, 2*b*c - 2*a*d, 2*b*d + 2*a*c, ori[0]],\n        [2*b*c + 2*a*d, a*a+c*c-b*b-d*d, 2*c*d - 2*a*b, ori[1]],\n        [2*b*d - 2*a*c, 2*c*d + 2*a*b, a*a + d*d - c*c - b*b, ori[2]],\n        [0, 0, 0, 1]\n    ])\n\n    # Obtain user transform for vtk algorithms\n    mat = vtk.vtkMatrix4x4()\n    [[mat.SetElement(i, j, A[i, j]) for i in range(4)] for j in range(4)]\n    print(\"From qform: \\n\", mat)\n \n# Use SForm Matrix\nif (header.GetSFormCode() &gt; 0):\n    gx = header.GetSRowX()\n    gy = header.GetSRowY()\n    gz = header.GetSRowZ()\n\n    # divide SForm matrix by spacing\n    gx /= s\n    gy /= s\n    gz /= s\n    A = np.zeros([4,4])\n    A[3, 3] = 1\n    A[0,:] = gx\n    A[1,:] = gy\n    A[2,:] = gz\n    # Obtain user transform for vtk algorithms\n    mat = vtk.vtkMatrix4x4()\n    [[mat.SetElement(i, j, A[i, j]) for i in range(4)] for j in range(4)]\n    print(\"From SForm: \\n\", mat)\n\n\n\n\nFrom qform: \n vtkMatrix4x4 (0000025FBEC509E0)\n  Debug: Off\n  Modified Time: 1478\n  Reference Count: 1\n  Registered Events: (none)\n  Elements:\n    -1 -0.000261643 0.000261643 0 \n    0.000261643 3.42285e-08 1 0 \n    -0.000261643 1 3.42285e-08 0 \n    0 0 0 1 \n\n\nFrom SForm: \n vtkMatrix4x4 (0000025FBEC4FF40)\n  Debug: Off\n  Modified Time: 1484\n  Reference Count: 1\n  Registered Events: (none)\n  Elements:\n    -1 0 0 0 \n    0 -0 -1 0 \n    0 1 0 0 \n    0 0 0 1 \n\nNow if you round them up, you can see how this is corrected, but the x-axis and y-axis are now inverted. THis is because vtk and itk has inherently different orientation. While ITK defaults image to RAI orientation (Medical image convention), VTK defaults image to LPI (Computer graphics convention). Specifically, if you look at MRI images, the structures one the right actually belongs to the left side of the patient, as if the patient is standing facing the viewer. Knowing these subtle different would help to figure why the orientation is always messed up if you use VTK and ITK interchangibly.\n\n\n\nFig. 1 ITK-snap display of the CT of a patient. Data collected and releasted to the public domain by Aerts et al [1].\n\n\n\n\n\n\n\nAerts, H. J., Velazquez, E. R., Leijenaar, R. T., Parmar, C., Grossmann, P., Carvalho, S., Bussink, J., Monshouwer, R., Haibe-Kains, B., Rietveld, D., Hoebers, F., Rietbergen, M. M., Leemans, C. R., Dekker, A., Quackenbush, J., Gillies, R. J., & Lambin, P. (2014). Decoding tumour phenotype by noninvasive imaging using a quantitative radiomics approach [Article]. Nat Commun, 5, 4006, Article 4006. https://doi.org/10.1038/ncomms5006",
    "crumbs": [
      "Home",
      "Posts",
      "Medical Imaging",
      "Medical imaging orientation in VTK and ITK"
    ]
  },
  {
    "objectID": "posts/medical-imaging/image-orientation.html#image-loading-methods",
    "href": "posts/medical-imaging/image-orientation.html#image-loading-methods",
    "title": "Medical imaging orientation in VTK and ITK",
    "section": "",
    "text": "Three loading methods you will likely use at some point are considered in this post:\n\nLoad as ITK image.\nLoad as ITK image using ImageToVTKImageFilter then convert it to vtkImageData.\nLoad as vtkImageData.\n\nTabulating their characters w.r.t. image directions:\n\n\n\n \nHas Direction\nHas Origin\nHas Orientation\n\n\n\n\nITK Image Loader\nYes\nYes\nYes\n\n\nITK to VTK\nNo\nYes\nNo\n\n\nVTK Image Loader\nNo\nNo\nNo",
    "crumbs": [
      "Home",
      "Posts",
      "Medical Imaging",
      "Medical imaging orientation in VTK and ITK"
    ]
  },
  {
    "objectID": "posts/medical-imaging/image-orientation.html#defining-terms",
    "href": "posts/medical-imaging/image-orientation.html#defining-terms",
    "title": "Medical imaging orientation in VTK and ITK",
    "section": "",
    "text": "Image origin is the position of the corner of the first stored voxel (i.e. index ijk = [0,0,0]). Which of the 8 corners depends on whether the image is righthanded or lefthanded, this give rise to the orientation definition.\n\n\n\n\n\nImage orientation refers to the specification of the three principle axis w.r.t. voxel ijk indices. A common notion used in radiology would be the permutation of six direction Left/Right (LR), Superior/Inferior (SI) and Anterior/Posterior (AP), e.g. RAI/LPS…etc., marking whether positive xyz move towards which of the above 6 directions. This, however, doesn’t fully defines the image’s xyz-axis yet and this only gives people a rough idea what is the “closest” orientation the xyz axis are pointing at. For example, an RAI image means that increase in \\(i, j, k\\) indices moves towards right, anterior and inferior direction, but they does not necessarily points directly to it. An image with a direction \\(i\\) mapping to Cartesian vector \\(\\vec{x} = [0.9, 0.1, 0]\\) will merit it to take “R” as this vector points dominantly towards \\(x\\).\n\nIn summary, image orientation can be seen as the definition of a matrix that maps ijk to xyz:\n\\[\n\\vec{v}_{xyz}=M\\vec{v}_{ijk}\n\\]",
    "crumbs": [
      "Home",
      "Posts",
      "Medical Imaging",
      "Medical imaging orientation in VTK and ITK"
    ]
  },
  {
    "objectID": "posts/medical-imaging/image-orientation.html#dicom-images",
    "href": "posts/medical-imaging/image-orientation.html#dicom-images",
    "title": "Medical imaging orientation in VTK and ITK",
    "section": "",
    "text": "Image origin of DICOM image is stored in the DICOM tag “Image Position (Patient) (0020,0032)”, which is a simple offset against all the voxel coordinates. Note that each slices has it’s own value of Image Position, but we only concern the first slice.\n\n\n\n\n\nImage direction of DICOM image is stored in DICOM tag “Image Orientation (Patient) (0020,0037)”, it is defined as the cosines of angle of three axis. Every slice of the same series (except for scouts) should have the same orientation.",
    "crumbs": [
      "Home",
      "Posts",
      "Medical Imaging",
      "Medical imaging orientation in VTK and ITK"
    ]
  },
  {
    "objectID": "posts/medical-imaging/image-orientation.html#nifti",
    "href": "posts/medical-imaging/image-orientation.html#nifti",
    "title": "Medical imaging orientation in VTK and ITK",
    "section": "",
    "text": "The image origin of Nifti files are stored in the quantity qoffsets and the fourth element of srow_x/y/z in the header. Usually, they are the same so you can just use one of them.\n\n\n\n\n\nImage directions of Nifti files are defined by two matrices in the header, namely SForm Matrix and QForm Matrix, which, in most cases, are the essentially the same matrix except SForm Matrix includes the spacing . The usage of these two matrices are defined by two quantity called sform code and qform code, and the QForm Matrix is defined by a vector quaternion. According to the documentation, three methods are mentioned for using these matrices depending on whether the qform code and sform code are 0 or not.",
    "crumbs": [
      "Home",
      "Posts",
      "Medical Imaging",
      "Medical imaging orientation in VTK and ITK"
    ]
  },
  {
    "objectID": "posts/medical-imaging/image-orientation.html#dicom",
    "href": "posts/medical-imaging/image-orientation.html#dicom",
    "title": "Medical imaging orientation in VTK and ITK",
    "section": "",
    "text": "Generally DICOM can be seen as a series of 2D images with extra information stored in headers. However, an image can be sliced along different normals, for example sagital, coronal or axial. These three direction are not the only direction that a DICOM series can take on, in fact, a DICOM series can be sliced along any directions. The slice direction is decided by the  “Image Orientation (Patient) (0020,0037)” DICOM tag, which specify the reference frame. The “Image Orientation(Patient)“ is a 6-element tuple consist of two vectors which describes the axis of the direction for row and column of that particular slice. For example, if your slices are Axial slice, then the two vectors defines sagital and coronal directions.\nIf the tag is in the format [a\\b\\c\\d\\e\\f], then its most likely two flattened 3-vectors. The cross product of the two vectors gives the third column of the rotational matrix, i.e:\n\\[\n\\begin{align}\n\\vec{v}_x &= \\begin{bmatrix} a&b&c \\end{bmatrix}^T \\\\\n\\vec{v}_y &= \\begin{bmatrix} d&e&f \\end{bmatrix}^T \\\\\n\\vec{v}_z &= \\vec{v}_x \\times \\vec{v}_y \\end{align}\n\\]",
    "crumbs": [
      "Home",
      "Posts",
      "Medical Imaging",
      "Medical imaging orientation in VTK and ITK"
    ]
  },
  {
    "objectID": "posts/medical-imaging/image-orientation.html#nifti-1",
    "href": "posts/medical-imaging/image-orientation.html#nifti-1",
    "title": "Medical imaging orientation in VTK and ITK",
    "section": "",
    "text": "Nifti stands for Neuroimaging Informatics Technology Initiative. It was initially designed to store brain MRI data. It is very suitable for dealing with 3D data because, unlike DICOM, the images from the same scan series are stored in a single file instead of multiple files.\nThe metadata of Nifti contains necessary information to establish the physical location of the patient during his/her scan. To properly load the image data, we first examine the qform and sform code, that perform action according the the conditions below.\nIf qform = 0\nThis method is for backwards compatibility only. When qform code = 0, the matrices will not be used to define the orientation and direction, the index to coordinate matrix is simply the following:\n\\[\n\\vec{r} = \\vec{s}^{\\text{ }T} \\cdot \\vec{I}\\\n\\]\nwhere r is physical coordinate, s is spacing vector and I is the index vector (i.e. $ = [i, j, k] $). In this case, no further transformation is need after image loading (or the transform is Identity matrix).\n\n\nWhen qform code is greater than zero, this method applies. This method involves constructing a rotation matrix using a so called quaternion representation.\nDefinition of quaternion:\n\\[ \\vec{q} =\\begin{bmatrix} a\\\\b\\\\c\\\\d \\end{bmatrix} \\]\nwhere we require that:\n\\[  a^2 + b^2 + c^2 + d^2 = 1\\]\nSo having three of the four values gives the remaining one. In nifti format, only b, c, d are given in the header, and we calculate a by the formula:\n\\[\na = \\sqrt{1 - b^2 - c^2 - d^2}\n\\]\nUsing a, b, c, d the rotational matrix R is defined as:\n\\[ \\displaystyle\n\\text{R}_q = \\begin{bmatrix}\na^2 + b^2 - c^2 - d^2&2bc - 2ad&2bd+2ac\\\\\n2bc + 2ad& a^2 + c^2 - b^2 - d^2& 2cd - 2ab\\\\\n2bd - 2ac&2cd + 2ab&a^2 + d^2 - c^2 - b^2\\\\\n\\end{bmatrix}\n\\]\nThe index to physical position formula would therefore be:\n\\[ \\displaystyle\n\\vec{r} = \\text{R}_q [\\vec{s}^{\\text{ }T}\\cdot \\vec{I}] + \\vec{q}_0\\\n\\]\nwhere q_0 stands for qoffsets.\nHence the affine matrix vtkMatrix4x4 used for transform in VTK would be\n\\[ \\displaystyle\n\\text{A} = \\begin{bmatrix}\\\na^2 + b^2 - c^2 - d^2&2bc - 2ad&2bd+2ac&q_x\\\\\n2bc + 2ad& a^2 + c^2 - b^2 - d^2& 2cd - 2ab&q_y\\\\\n2bd - 2ac&2cd + 2ab&a^2 + d^2 - c^2 - b^2&q_z\\\\\n0&0&0&1\\\n\\end{bmatrix}\\\n\\]\n\n\n\nThis can coexist with qform &gt; 0 (i.e. both qform &gt; 0 and sform &gt; 0 can coexist, qform describe the transformation from data to scanning grid, sform describe the transformation from data to standard grid). The sform matrix is stored separately in three vectors: SRowX, SRowY and SRowZ, which we will denote as: $ _x, _y, _z $.\n\\[\n\\displaystyle \\text{R}_s =\\begin{bmatrix}g_{x_1}&g_{x_2}&g_{x_3}\\\\g_{y_1}&g_{y_2}&g_{y_3}\\\\g_{z_1}&g_{z_2}&g_{z_3} \\end{bmatrix}\n\\]\nSince the SForm matrix already include spacing, there are no needs to multiply spacing for each index. The index to physical position formula would therefore be:\n\\[\n\\displaystyle\n\\vec{r} = \\text{R}_s \\vec{I} + \\begin{bmatrix} g_{x_4}\\\\g_{y_4}\\\\g_{z_4} \\end{bmatrix}\\\n\\]\nThe affine matrix used for transform in vtkMatrix4x4 would then be the rotational matrix divided by spacing spacing vector s:\n\\[\n\\displaystyle\\\n\\text{A} =\\\n\\begin{bmatrix}\\\n\\frac{g_{x_1}}{s_1}&\\frac{g_{x_2}}{s_2}&\\frac{g_{x_3}}{s_3}&g_{x_4}\\\\\n\\frac{g_{y_1}}{s_1}&\\frac{g_{y_2}}{s_2}&\\frac{g_{y_3}}{s_3}&g_{y_4}\\\\\n\\frac{g_{z_1}}{s_1}&\\frac{g_{z_2}}{s_2}&\\frac{g_{z_3}}{s_3}&g_{z_4}\\\\\n0&0&0&1\\\n\\end{bmatrix}\\\n\\]\n\n\n\n\nimport vtk\nimport numpy as np\nimport tempfile\nimport SimpleITK as sitk\nfrom pprint import pprint\n\n# Create a fake image with simpleitk\nimage = sitk.Image(28, 28, 28, sitk.sitkUInt8)\nimage.SetSpacing([0.5, 0.1, 0.2])\n\n# Set directio to:\n#  [1, 0, 0]\n#  [0, 0, 1]\n#  [0, 1, 0]\nimage.SetDirection((1., 0., 0., 0., 0., 1., 0., 1., 0.))\n\n# Write this dummy image for VTK to read\nwith tempfile.TemporaryDirectory() as f:\n    sitk.WriteImage(image, f + \"/temp.nii.gz\")\n\n    # read it back with vtk\n    reader = vtk.vtkNIFTIImageReader()\n    reader.SetFileName(f + \"/temp.nii.gz\")\n    reader.Update()\n \n    header = reader.GetNIFTIHeader()\n    vtkimage = reader.GetOutput()\n\nprint(\"ITK: \")\npprint(np.asarray(image.GetDirection()).reshape(3, 3))\nprint(\"VTK: \")\nprint(vtkimage.GetDirectionMatrix())",
    "crumbs": [
      "Home",
      "Posts",
      "Medical Imaging",
      "Medical imaging orientation in VTK and ITK"
    ]
  },
  {
    "objectID": "posts/medical-imaging/image-orientation.html#output",
    "href": "posts/medical-imaging/image-orientation.html#output",
    "title": "Medical imaging orientation in VTK and ITK",
    "section": "",
    "text": "ITK: \narray([[1., 0., 0.],\n       [0., 0., 1.],\n       [0., 1., 0.]])\nVTK: \nvtkMatrix3x3 (0000025FBE641A50)\n  Debug: Off\n  Modified Time: 1319\n  Reference Count: 2\n  Registered Events: (none)\n  Elements:\n    1   0   0\n    0   1   0\n    0   0   1\n\nSee how the two matrix are different? In fact VTK did not load the the direction into the matrix and therefore it remained as the default identity matrix.",
    "crumbs": [
      "Home",
      "Posts",
      "Medical Imaging",
      "Medical imaging orientation in VTK and ITK"
    ]
  },
  {
    "objectID": "posts/medical-imaging/image-orientation.html#output-1",
    "href": "posts/medical-imaging/image-orientation.html#output-1",
    "title": "Medical imaging orientation in VTK and ITK",
    "section": "",
    "text": "From qform: \n vtkMatrix4x4 (0000025FBEC509E0)\n  Debug: Off\n  Modified Time: 1478\n  Reference Count: 1\n  Registered Events: (none)\n  Elements:\n    -1 -0.000261643 0.000261643 0 \n    0.000261643 3.42285e-08 1 0 \n    -0.000261643 1 3.42285e-08 0 \n    0 0 0 1 \n\n\nFrom SForm: \n vtkMatrix4x4 (0000025FBEC4FF40)\n  Debug: Off\n  Modified Time: 1484\n  Reference Count: 1\n  Registered Events: (none)\n  Elements:\n    -1 0 0 0 \n    0 -0 -1 0 \n    0 1 0 0 \n    0 0 0 1 \n\nNow if you round them up, you can see how this is corrected, but the x-axis and y-axis are now inverted. THis is because vtk and itk has inherently different orientation. While ITK defaults image to RAI orientation (Medical image convention), VTK defaults image to LPI (Computer graphics convention). Specifically, if you look at MRI images, the structures one the right actually belongs to the left side of the patient, as if the patient is standing facing the viewer. Knowing these subtle different would help to figure why the orientation is always messed up if you use VTK and ITK interchangibly.\n\n\n\nFig. 1 ITK-snap display of the CT of a patient. Data collected and releasted to the public domain by Aerts et al [1].",
    "crumbs": [
      "Home",
      "Posts",
      "Medical Imaging",
      "Medical imaging orientation in VTK and ITK"
    ]
  },
  {
    "objectID": "posts/deep-learning/torch/torchio-ddp.html",
    "href": "posts/deep-learning/torch/torchio-ddp.html",
    "title": "PyTorch DDP with torchio",
    "section": "",
    "text": "Distributed Data Parallel (DDP) is an enhancement to PyTorch’s DataParallel, allowing for efficient batch distribution across multiple GPUs and machines. DDP introduces model parallelism, enabling parts of a model to be housed on separate GPUs within the same process while distributing data across all processes. This is particularly beneficial for models too large for a single GPU, though a batch size of one is generally not recommended for best performance.\nDDP stands out by enabling the sharing of model parameters and data across different machines. Additionally, it supports synchronized batch normalization, a significant improvement over DataParallel. While DataParallel calculates batch normalization for each GPU’s data in isolation, DDP updates batch normalization parameters considering all samples in the batch, regardless of their GPU distribution.\n\n\n\n\n\n\nNote\n\n\n\nNote that for model parallel, the advantages is not apparent because the reason someone would like to distribute his/her model across GPU is going to be because of GRAM issue. If you can’t fit your model into one GPU for batch-size 1, you are not going to do any better fitting it into multiple-GPUs because the bottle-neck is small batch-size.\n\n\n\n\n\nTorchIO is a package for image I/O with torch, in particular 3D medical image volumes. It can load images with a pre-defined configuration with augmentations that is common to MRI such as adding bias-field, ghosting, noise…etc. Currently, its loading mechanism is simple like this:\n\n\n\n\n\nflowchart LR\n    Sub[(Subjects)] --&gt; sam[Sampler] --&gt; empty{Queue\\nEmpty?}\n    empty --&gt; |yes|sam\n  empty --&gt; |no|pop[\"pop()\"]\n\n\n Fig 1. torchio sampling logic \n\n\n\nThis does not work with DDP well. The vanilla DsitributedSampler in torch basically has a key __iter__() function that returns indices. The indices are the same for all instances created in the same way. This means that the distribution requires the whole dataset to be defined before initiating the DDP threads.\n\n\n\n\n\nflowchart TD\n    Sub[(Subjects)] --&gt; sp[Samples] --&gt; Samplers\n    sp --&gt; ddp(DDP) --&gt; im[Indices Mapping] --&gt; Samplers\n    Samplers --&gt; th1[Thread 1 Samples] & th2[Thread 2 Samples] & th3[Thread 3 Samples]\n   \n\n\n Fig 2. DDP sampling logic \n\n\n\nThis goes against torchio logic, which samples from the subjects on the run to fill up a queue, and pop the element when torch.DataLoader requests them. The tio.Sampler will work in the background to fill up the tio.Queue when its empty. This means there are no fix indices such that naive indices mapping and it will not work properly with DistributedSampler because one is call next() and the other is calling get_item().\n\n\n\n\nThe tio.Queue should have a multi-processing wrapper class that has a shared patch pool. Only the queue with rank = 0 should call _generate_patches() or _fill() when the patch pool is empty. All the MP queues then refences the same patch pool based on a fixed index mapping. The patch pool discards a patch only when all the MP queues have accessed the patch.\n\n\n\n\n\nflowchart TD\n    DDPController --&gt; |spawn|DDPThread --&gt; |create|q[\"tio.Queue\"]\n    DDPThread --&gt; |create if rank == 0|Samplers[tio.Samplers] & im[/DDP indices map/]\n    Samplers --&gt; |push patches|q\n    q --&gt; emp{Empty} ---&gt; |\"&lt;b&gt;Yes&lt;/b&gt;&lt;br&gt;signal rank 0 sampler\"|Samplers\n    Sub[(Subjects)] --&gt; |provide data|Samplers\n    im --&gt; |request data|q\n    emp ---&gt; |&lt;b&gt;No|return[\"Pop() Return Patch\"] --&gt; End\n\n\n Fig 3. tio.Queue for DDP \n\n\n\nThe tio.Queue should be a shuffled list of patches. The DDP rank i thread should get the i-th element from the shared tio.Queue pool. When all the queues got their patches, the queue should pop the elements that were released.\nAfter each epoch, the shared pool is shuffled to ensure each GPU optimizer sees the entire dataset.\n\n\n\nThe pop() was called at random sequence multiple times (number of times equal to batch size)\nIf I write a function wrapping solver and multiprocess the fit function, it is needed to write it such that only the first process does validation loop.\nCurrent CFG system does not allow dynamically changing attributes\nThere is a problem where if the data changes (i.e., new subjects are added or some are removed), the Sampler in torchio needs to be reconfigured with new indices. This can be a cumbersome and time-consuming process, especially if the data changes frequently.\nThese operations should not be repeated and should only run once:\n\nValidation loop\nSaving best checkpoint\nEvaluating early stopping criteria\nTriggers tio.Queue sampling\n\n\n\n\n\n\nI implemented wrappers for the class instance SolverBase, DataLoaderBase, and DataSetBase classes to replace some of the original functions with DDP versions. This allows for the parallelization of model training across multiple GPUs or nodes, improving both the speed and efficiency of the training process.\n\n\nSee this fork for details of the changes made.\nIn summary, the tio.Queue class is inherited to implement the tio.QueueDDP class, which reload the subjects based on the rank of the the current process.\nimport torch\nimport torch.distributed as dist\nfrom torch.utils.data import DistributedSampler\nfrom .queue import Queue\nfrom .. import NUM_SAMPLES\nfrom .dataset import SubjectsDataset\nfrom .sampler import PatchSampler\nfrom .subject import Subject\n\nfrom typing import Optional\n\nclass QueueDDP(Queue):\n    r\"\"\"Wrapper for distributed training.\n\n    This class wraps :class:`~torchio.data.Queue` and add two key arguements for DDP.\n    Namely the `num_of_replicas`, which often is the world size and also `rank`, which\n    dictates the behavior of the loader. Note that the argument `start_background` is disabled.\n\n    Args:\n        num_of_replicas:\n            The number of queue that will be running parallelly. Default to the value returned\n            by :func:`torch.distributed.get_world_size`.\n        rank:\n            The rank of the queue instance. Default to the value returned by :func:`torch.distributed.\n            get_rank`.\n\n    See Also:\n        For other initialization arguments, please see: :class:`~torch.data.Queue`\n\n    \"\"\" # noqa: E501\n    def __init__(self,\n                 subjects_dataset  : SubjectsDataset,\n                 max_length        : int,\n                 samples_per_volume: int,\n                 sampler           : PatchSampler,\n                 num_workers       : int             = 0,\n                 shuffle_subjects  : bool            = True,\n                 shuffle_patches   : bool            = True,\n                 start_background  : bool            = True,\n                 verbose           : bool            = False,\n                 num_of_replicas   : Optional[int]   = None,\n                 rank              : Optional[int]   = None,\n                 ):\n        super(QueueDDP, self).__init__(subjects_dataset,\n                                       max_length,\n                                       samples_per_volume,\n                                       sampler,\n                                       num_workers,\n                                       shuffle_subjects,\n                                       shuffle_patches,\n                                       False, # start_background doesn't work here\n                                       verbose,\n                                       )\n        if not dist.is_initialized():\n            msg = \"This class should always be used for distributed training. However, \" \\\n                  \"DDP doesn't seem to have been activated.\"\n            raise RuntimeError(msg)\n\n        self.num_of_replicas = num_of_replicas or dist.get_world_size()\n        self.rank = rank or dist.get_rank()\n\n        # separate original\n        self.subjects_dataset = SubjectsDataset(\n            subjects_dataset._subjects[self.rank:len(subjects_dataset):self.num_of_replicas],\n            transform = subjects_dataset._transform\n        )\n\n\n\nimport torch\nimport torchio as tio\nfrom torch.utils.data import DataLoader\nfrom torchio.data import UniformSampler\nfrom torchio.utils import create_dummy_dataset\nimport os\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\n\n\"\"\"Configurations\"\"\"\nhost = 'localhost'\nport = '23455'\nbackbone = `nccl`\nworld_size = 4 # make sure this is less than your GPU count\n\n\ndef ddp_helper(rank, subjects, world_size, num_workers=0) -&gt; tio.QueueDDP:\n    # kick start ddp\n    os.environ['MASTER_ADDR'] = host\n    os.environ['MASTER_PORT'] = port\n    dist.init_process_group(backbone, world_size=world_size, rank=rank)\n\n    # Setup torch sampler\n    subject_dataset = tio.SubjectsDataset(subjects)\n    patch_size = 10\n    sampler = UniformSampler(patch_size)\n\n    # Set up DDP Queue\n    queue_dataset = tio.QueueDDP(\n        subjects_dataset,\n        max_length=6,\n        samples_per_volume=2,\n        sampler=sampler,\n        num_workers=num_workers,\n        num_of_replicas=world_size,\n        rank=rank\n    )\n\n    # Demonstrate loader \n    _ = str(queue_dataset)\n    batch_loader = DataLoader(queue_dataset, batch_size=4, drop_last=True)\n    for batch in batch_loader:\n        # &lt;replace with your training code&gt;\n        _ = batch['one_modality'][tio.DATA]\n        _ = batch['segmentation'][tio.DATA]\n    dist.destroy_process_group()\n    return queue_dataset\n\ndef main():\n    subjects_list = create_dummy_dataset(\n        num_images = 10, \n        size_range = (10, 20), \n        directory = \".\", \n        suffix = '.nii', \n        force=False\n    )\n    \n    #! define environment\n    for i in range(world_size):\n        mp.spawn(ddp_helper, args=(subjects_list, backbone, world_size, 4,), nprocs=world_size)\n\n\n\n\n\nThis implementation is not quite what we want yet because each GPU now has its own set of subjects. partition. If this order is not changed for each epoch then each GPU is going to be dealing with the same group of patients. Although theoretically it makes no differences if you are using synchronized batchnorm, the underlying side effects could limit the convergence. Therefore, one should shuffle the subjects and rebuild the queue after each epoch as a work around for this problem.",
    "crumbs": [
      "Home",
      "Posts",
      "Deep Learning",
      "Torch",
      "PyTorch DDP with torchio"
    ]
  },
  {
    "objectID": "posts/deep-learning/torch/torchio-ddp.html#distributed-data-parallel-ddp",
    "href": "posts/deep-learning/torch/torchio-ddp.html#distributed-data-parallel-ddp",
    "title": "PyTorch DDP with torchio",
    "section": "",
    "text": "Distributed Data Parallel (DDP) is an enhancement to PyTorch’s DataParallel, allowing for efficient batch distribution across multiple GPUs and machines. DDP introduces model parallelism, enabling parts of a model to be housed on separate GPUs within the same process while distributing data across all processes. This is particularly beneficial for models too large for a single GPU, though a batch size of one is generally not recommended for best performance.\nDDP stands out by enabling the sharing of model parameters and data across different machines. Additionally, it supports synchronized batch normalization, a significant improvement over DataParallel. While DataParallel calculates batch normalization for each GPU’s data in isolation, DDP updates batch normalization parameters considering all samples in the batch, regardless of their GPU distribution.\n\n\n\n\n\n\nNote\n\n\n\nNote that for model parallel, the advantages is not apparent because the reason someone would like to distribute his/her model across GPU is going to be because of GRAM issue. If you can’t fit your model into one GPU for batch-size 1, you are not going to do any better fitting it into multiple-GPUs because the bottle-neck is small batch-size.",
    "crumbs": [
      "Home",
      "Posts",
      "Deep Learning",
      "Torch",
      "PyTorch DDP with torchio"
    ]
  },
  {
    "objectID": "posts/deep-learning/torch/torchio-ddp.html#torchio-and-its-issue-with-ddp",
    "href": "posts/deep-learning/torch/torchio-ddp.html#torchio-and-its-issue-with-ddp",
    "title": "PyTorch DDP with torchio",
    "section": "",
    "text": "TorchIO is a package for image I/O with torch, in particular 3D medical image volumes. It can load images with a pre-defined configuration with augmentations that is common to MRI such as adding bias-field, ghosting, noise…etc. Currently, its loading mechanism is simple like this:\n\n\n\n\n\nflowchart LR\n    Sub[(Subjects)] --&gt; sam[Sampler] --&gt; empty{Queue\\nEmpty?}\n    empty --&gt; |yes|sam\n  empty --&gt; |no|pop[\"pop()\"]\n\n\n Fig 1. torchio sampling logic \n\n\n\nThis does not work with DDP well. The vanilla DsitributedSampler in torch basically has a key __iter__() function that returns indices. The indices are the same for all instances created in the same way. This means that the distribution requires the whole dataset to be defined before initiating the DDP threads.\n\n\n\n\n\nflowchart TD\n    Sub[(Subjects)] --&gt; sp[Samples] --&gt; Samplers\n    sp --&gt; ddp(DDP) --&gt; im[Indices Mapping] --&gt; Samplers\n    Samplers --&gt; th1[Thread 1 Samples] & th2[Thread 2 Samples] & th3[Thread 3 Samples]\n   \n\n\n Fig 2. DDP sampling logic \n\n\n\nThis goes against torchio logic, which samples from the subjects on the run to fill up a queue, and pop the element when torch.DataLoader requests them. The tio.Sampler will work in the background to fill up the tio.Queue when its empty. This means there are no fix indices such that naive indices mapping and it will not work properly with DistributedSampler because one is call next() and the other is calling get_item().",
    "crumbs": [
      "Home",
      "Posts",
      "Deep Learning",
      "Torch",
      "PyTorch DDP with torchio"
    ]
  },
  {
    "objectID": "posts/deep-learning/torch/torchio-ddp.html#challenges",
    "href": "posts/deep-learning/torch/torchio-ddp.html#challenges",
    "title": "PyTorch DDP with torchio",
    "section": "",
    "text": "The pop() was called at random sequence multiple times (number of times equal to batch size)\nIf I write a function wrapping solver and multiprocess the fit function, it is needed to write it such that only the first process does validation loop.\nCurrent CFG system does not allow dynamically changing attributes\nThere is a problem where if the data changes (i.e., new subjects are added or some are removed), the Sampler in torchio needs to be reconfigured with new indices. This can be a cumbersome and time-consuming process, especially if the data changes frequently.\nThese operations should not be repeated and should only run once:\n\nValidation loop\nSaving best checkpoint\nEvaluating early stopping criteria\nTriggers tio.Queue sampling",
    "crumbs": [
      "Home",
      "Posts",
      "Deep Learning",
      "Torch",
      "PyTorch DDP with torchio"
    ]
  },
  {
    "objectID": "posts/deep-learning/torch/torchio-ddp.html#modify-torchio",
    "href": "posts/deep-learning/torch/torchio-ddp.html#modify-torchio",
    "title": "PyTorch DDP with torchio",
    "section": "",
    "text": "See this fork for details of the changes made.\nIn summary, the tio.Queue class is inherited to implement the tio.QueueDDP class, which reload the subjects based on the rank of the the current process.\nimport torch\nimport torch.distributed as dist\nfrom torch.utils.data import DistributedSampler\nfrom .queue import Queue\nfrom .. import NUM_SAMPLES\nfrom .dataset import SubjectsDataset\nfrom .sampler import PatchSampler\nfrom .subject import Subject\n\nfrom typing import Optional\n\nclass QueueDDP(Queue):\n    r\"\"\"Wrapper for distributed training.\n\n    This class wraps :class:`~torchio.data.Queue` and add two key arguements for DDP.\n    Namely the `num_of_replicas`, which often is the world size and also `rank`, which\n    dictates the behavior of the loader. Note that the argument `start_background` is disabled.\n\n    Args:\n        num_of_replicas:\n            The number of queue that will be running parallelly. Default to the value returned\n            by :func:`torch.distributed.get_world_size`.\n        rank:\n            The rank of the queue instance. Default to the value returned by :func:`torch.distributed.\n            get_rank`.\n\n    See Also:\n        For other initialization arguments, please see: :class:`~torch.data.Queue`\n\n    \"\"\" # noqa: E501\n    def __init__(self,\n                 subjects_dataset  : SubjectsDataset,\n                 max_length        : int,\n                 samples_per_volume: int,\n                 sampler           : PatchSampler,\n                 num_workers       : int             = 0,\n                 shuffle_subjects  : bool            = True,\n                 shuffle_patches   : bool            = True,\n                 start_background  : bool            = True,\n                 verbose           : bool            = False,\n                 num_of_replicas   : Optional[int]   = None,\n                 rank              : Optional[int]   = None,\n                 ):\n        super(QueueDDP, self).__init__(subjects_dataset,\n                                       max_length,\n                                       samples_per_volume,\n                                       sampler,\n                                       num_workers,\n                                       shuffle_subjects,\n                                       shuffle_patches,\n                                       False, # start_background doesn't work here\n                                       verbose,\n                                       )\n        if not dist.is_initialized():\n            msg = \"This class should always be used for distributed training. However, \" \\\n                  \"DDP doesn't seem to have been activated.\"\n            raise RuntimeError(msg)\n\n        self.num_of_replicas = num_of_replicas or dist.get_world_size()\n        self.rank = rank or dist.get_rank()\n\n        # separate original\n        self.subjects_dataset = SubjectsDataset(\n            subjects_dataset._subjects[self.rank:len(subjects_dataset):self.num_of_replicas],\n            transform = subjects_dataset._transform\n        )",
    "crumbs": [
      "Home",
      "Posts",
      "Deep Learning",
      "Torch",
      "PyTorch DDP with torchio"
    ]
  },
  {
    "objectID": "posts/deep-learning/torch/torchio-ddp.html#usage-of-tio.queueddp",
    "href": "posts/deep-learning/torch/torchio-ddp.html#usage-of-tio.queueddp",
    "title": "PyTorch DDP with torchio",
    "section": "",
    "text": "import torch\nimport torchio as tio\nfrom torch.utils.data import DataLoader\nfrom torchio.data import UniformSampler\nfrom torchio.utils import create_dummy_dataset\nimport os\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\n\n\"\"\"Configurations\"\"\"\nhost = 'localhost'\nport = '23455'\nbackbone = `nccl`\nworld_size = 4 # make sure this is less than your GPU count\n\n\ndef ddp_helper(rank, subjects, world_size, num_workers=0) -&gt; tio.QueueDDP:\n    # kick start ddp\n    os.environ['MASTER_ADDR'] = host\n    os.environ['MASTER_PORT'] = port\n    dist.init_process_group(backbone, world_size=world_size, rank=rank)\n\n    # Setup torch sampler\n    subject_dataset = tio.SubjectsDataset(subjects)\n    patch_size = 10\n    sampler = UniformSampler(patch_size)\n\n    # Set up DDP Queue\n    queue_dataset = tio.QueueDDP(\n        subjects_dataset,\n        max_length=6,\n        samples_per_volume=2,\n        sampler=sampler,\n        num_workers=num_workers,\n        num_of_replicas=world_size,\n        rank=rank\n    )\n\n    # Demonstrate loader \n    _ = str(queue_dataset)\n    batch_loader = DataLoader(queue_dataset, batch_size=4, drop_last=True)\n    for batch in batch_loader:\n        # &lt;replace with your training code&gt;\n        _ = batch['one_modality'][tio.DATA]\n        _ = batch['segmentation'][tio.DATA]\n    dist.destroy_process_group()\n    return queue_dataset\n\ndef main():\n    subjects_list = create_dummy_dataset(\n        num_images = 10, \n        size_range = (10, 20), \n        directory = \".\", \n        suffix = '.nii', \n        force=False\n    )\n    \n    #! define environment\n    for i in range(world_size):\n        mp.spawn(ddp_helper, args=(subjects_list, backbone, world_size, 4,), nprocs=world_size)",
    "crumbs": [
      "Home",
      "Posts",
      "Deep Learning",
      "Torch",
      "PyTorch DDP with torchio"
    ]
  },
  {
    "objectID": "posts/deep-learning/torch/torchio-ddp.html#remaining-challenges",
    "href": "posts/deep-learning/torch/torchio-ddp.html#remaining-challenges",
    "title": "PyTorch DDP with torchio",
    "section": "",
    "text": "This implementation is not quite what we want yet because each GPU now has its own set of subjects. partition. If this order is not changed for each epoch then each GPU is going to be dealing with the same group of patients. Although theoretically it makes no differences if you are using synchronized batchnorm, the underlying side effects could limit the convergence. Therefore, one should shuffle the subjects and rebuild the queue after each epoch as a work around for this problem.",
    "crumbs": [
      "Home",
      "Posts",
      "Deep Learning",
      "Torch",
      "PyTorch DDP with torchio"
    ]
  },
  {
    "objectID": "posts/deep-learning/multi-modal/hallucinations/01_hallucination.html",
    "href": "posts/deep-learning/multi-modal/hallucinations/01_hallucination.html",
    "title": "Narrative review - Reducing hallucination in MMLLM",
    "section": "",
    "text": "Hallucination refers to when a multi-modal generative models hallucinates about the input and generate text with fake description that is not associated with the input image (Figure 1).\n\n\n\n\n\n\nFigure 1: Example of hallucination by a multi-modal model. Extracted from (Yin et al. 2023).\n\n\n\nThese hallucination is more common in smaller models with less than 100B parameters, and is currently a huge challenge because training these model requires cooperate-level resources. As an individual researcher, these sort of resources is not that easy to obtain. Therefore, understanding hallucination is essential to decentralize the technology advancement of multi-modal language models (MMLM). This post is a narrative review looking at current studies that are working towards solving hallucination in MMLLM. I referenced the awesome-mmllmm list for some noted studies.\n\n\nWhile hallucination is a well-known phenomenon, our understanding of its root cause is limited. Most believe this is merely a limitation of the lack of trainable parameters and can be solved by increasing model size. This is true to some extent with evidence suggesting LLM with &gt;100B parameters showing a much better performance and accuracy in reasoning when compared to smaller version of itself. However, LLM as big as GPT-4V with &gt; 175B parameters still hallucinates, so there must be more to this issue.\n\n\nOne interesting study I came across on Arxiv suggests hallucination is a result of the gradient-based optimization process (yao2023?). Essentially Yao et al. constructed rogue prompts to induce the LLM to give a designated fabricated hallucination output by optimizing the token loss of the model output with designated hallucination (without touching model parameters, just swapping input prompt tokens). They found that tokens forming text prompt will still get the designated hallucination for a 7B model (Viculna-7B). E.g:\n# Input\n/] The name enumer Mission capital city of fol ([Yes [- nazw engl Londres britannique Peru Franvera Sorry}{—(?\n\n# Output\nThe name of the capital city of France is London.\nThey conclude that hallucination is a result embedded into gradient-optimization method for training LLM.\nAdvanced discussion\nI do found their conclusion to be a bit hasted, and I frankly does not fully agree to it, but I can see the logic behind their argument. They seems to believe hallucinations are attributed to the encoder mechanisms within LLMs, which are fine-tuned through an imperfect loss function. This imperfectness can be exploited to generate prompts that, despite appearing illogical to humans, are perceived as normal by the LLM and produces hallucination output. Although they did not mentioned it in their paper, I deduced they are theorizing that one of the causes of hallucination is the proximity of semantically disparate tokens in the embedding space.\nI have also tried their OoD prompts in larger models including Llama-70B, it seems to perceive many of their OoD prompts from their Table 6. to be harmful language and refuse to answer.\nAlso, I noticed a pattern where fragment of word in their OoD are actually related to the output hallucination in a somewhat flawed but not entirely illogical manner. For example, the prompt: !?? wonderful!! Add better!!altro!!emi . Parisian always result in response related to Paris. And it’s obviously attributed to “Parisian”, despite this is not a commonly used term.\n\n\n\n\n\n\nFigure 2: Screen capture of response by Llama-70B. Although I used a model with 10x the original parameters of the model tested by the authors, I believe the OoD prompt does has logical connection with the output of the model.\n\n\n\n\n\n\n\n(Guerreiro et al. 2023) reports that LLM hallucinates more under low-resources situations. In other words, they pretend to know and answer with confident even though they don’t. This is quite believable but is also quite obvious to people who played with a LLM chat bot for a while. I also think the few-shot, one-shot and zero-shot configuration is a testimony thatLLM performs better with more resources. Now, the lying behavior is notorious, the real question is “why” but this paper didn’t really give us an explanation.\n\nGuerreiro, Nuno M., Duarte Alves, Jonas Waldendorf, Barry Haddow, Alexandra Birch, Pierre Colombo, and André F. T. Martins. 2023. “Hallucinations in Large Multilingual Translation Models.” https://doi.org/10.48550/ARXIV.2303.16104.\nThe authors also showed that hallucination, in particular “Toxic” (e.g., hate speech) hallucination, occurs more frequently when the LLM is translating out of English from language with less training data.\n\n\n\nI find it shocking that the literature does not offer much information about the association between temperature and hallucination in LLM. It seems there again not really a solid conclusion, but I read on GDELT blog [link] that setting to a temperature to 0, meaning always select the highest probability token, will result in more hallucination by removing the model’s flexibility to escape high-probability low-relevance phrasal assemblies (i.e., frequently used term that are unrelated to the discussed subject matter). I cannot really say I fully believe this, but the author, who remained annonymus, mentioned that many AI firm suggest that setting temperature to 0 will eliminate hallucination, I stand firmly with him/her that this is not the case.",
    "crumbs": [
      "Home",
      "Posts",
      "Deep Learning",
      "Multi Modal",
      "Hallucinations",
      "Narrative review - Reducing hallucination in MMLLM"
    ]
  },
  {
    "objectID": "posts/deep-learning/multi-modal/hallucinations/01_hallucination.html#cause-of-hallucination",
    "href": "posts/deep-learning/multi-modal/hallucinations/01_hallucination.html#cause-of-hallucination",
    "title": "Narrative review - Reducing hallucination in MMLLM",
    "section": "",
    "text": "While hallucination is a well-known phenomenon, our understanding of its root cause is limited. Most believe this is merely a limitation of the lack of trainable parameters and can be solved by increasing model size. This is true to some extent with evidence suggesting LLM with &gt;100B parameters showing a much better performance and accuracy in reasoning when compared to smaller version of itself. However, LLM as big as GPT-4V with &gt; 175B parameters still hallucinates, so there must be more to this issue.\n\n\nOne interesting study I came across on Arxiv suggests hallucination is a result of the gradient-based optimization process (yao2023?). Essentially Yao et al. constructed rogue prompts to induce the LLM to give a designated fabricated hallucination output by optimizing the token loss of the model output with designated hallucination (without touching model parameters, just swapping input prompt tokens). They found that tokens forming text prompt will still get the designated hallucination for a 7B model (Viculna-7B). E.g:\n# Input\n/] The name enumer Mission capital city of fol ([Yes [- nazw engl Londres britannique Peru Franvera Sorry}{—(?\n\n# Output\nThe name of the capital city of France is London.\nThey conclude that hallucination is a result embedded into gradient-optimization method for training LLM.\nAdvanced discussion\nI do found their conclusion to be a bit hasted, and I frankly does not fully agree to it, but I can see the logic behind their argument. They seems to believe hallucinations are attributed to the encoder mechanisms within LLMs, which are fine-tuned through an imperfect loss function. This imperfectness can be exploited to generate prompts that, despite appearing illogical to humans, are perceived as normal by the LLM and produces hallucination output. Although they did not mentioned it in their paper, I deduced they are theorizing that one of the causes of hallucination is the proximity of semantically disparate tokens in the embedding space.\nI have also tried their OoD prompts in larger models including Llama-70B, it seems to perceive many of their OoD prompts from their Table 6. to be harmful language and refuse to answer.\nAlso, I noticed a pattern where fragment of word in their OoD are actually related to the output hallucination in a somewhat flawed but not entirely illogical manner. For example, the prompt: !?? wonderful!! Add better!!altro!!emi . Parisian always result in response related to Paris. And it’s obviously attributed to “Parisian”, despite this is not a commonly used term.\n\n\n\n\n\n\nFigure 2: Screen capture of response by Llama-70B. Although I used a model with 10x the original parameters of the model tested by the authors, I believe the OoD prompt does has logical connection with the output of the model.\n\n\n\n\n\n\n\n(Guerreiro et al. 2023) reports that LLM hallucinates more under low-resources situations. In other words, they pretend to know and answer with confident even though they don’t. This is quite believable but is also quite obvious to people who played with a LLM chat bot for a while. I also think the few-shot, one-shot and zero-shot configuration is a testimony thatLLM performs better with more resources. Now, the lying behavior is notorious, the real question is “why” but this paper didn’t really give us an explanation.\n\nGuerreiro, Nuno M., Duarte Alves, Jonas Waldendorf, Barry Haddow, Alexandra Birch, Pierre Colombo, and André F. T. Martins. 2023. “Hallucinations in Large Multilingual Translation Models.” https://doi.org/10.48550/ARXIV.2303.16104.\nThe authors also showed that hallucination, in particular “Toxic” (e.g., hate speech) hallucination, occurs more frequently when the LLM is translating out of English from language with less training data.\n\n\n\nI find it shocking that the literature does not offer much information about the association between temperature and hallucination in LLM. It seems there again not really a solid conclusion, but I read on GDELT blog [link] that setting to a temperature to 0, meaning always select the highest probability token, will result in more hallucination by removing the model’s flexibility to escape high-probability low-relevance phrasal assemblies (i.e., frequently used term that are unrelated to the discussed subject matter). I cannot really say I fully believe this, but the author, who remained annonymus, mentioned that many AI firm suggest that setting temperature to 0 will eliminate hallucination, I stand firmly with him/her that this is not the case.",
    "crumbs": [
      "Home",
      "Posts",
      "Deep Learning",
      "Multi Modal",
      "Hallucinations",
      "Narrative review - Reducing hallucination in MMLLM"
    ]
  },
  {
    "objectID": "posts/deep-learning/multi-modal/base-modules/02-CLIP.html",
    "href": "posts/deep-learning/multi-modal/base-modules/02-CLIP.html",
    "title": "LVM - CLIP",
    "section": "",
    "text": "CLIP stands for Contrastive Language-Image Pre-training, and it is brought to us again by Alec Radford’s group in OpenAI (Radford et al. 2021). It’s no overstatement to say this is a hallmark of modern language-vision model, but credits should also be given to the original work that outline contrastive learning ConVIRT by (Zhang et al. 2020). To highlight some of the key differences, ConVIRT in (Zhang et al. 2020) used CNN for encoding, whereas Radford used vision transformer (ViT) and ResNet as image encoders.\nNow despite many people, including OpenAI themselves, keep referring CLIP as a some sort of a trained model, it is important to recognize CLIP is not really a module or architecture, but a framework Figure 1 to pre-train both image and text encoder such that their latent spaces align with each other, these sort of pre-train methods are generally referred to as Vision-Language Pre-training (VLP). CLIP does not restrict the architecture of vision- and language-encoder, and it does introduce structual connection between these two entities.\nCLIP has since been used for many multi-modal tasks, such as stable diffusion, because of its remarkable capacity to capture the association vision information with text.\n\n\n\n\n\n\nFigure 1: Overview of CLIP method extracted from (Radford et al. 2021).\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nRegardless of what I said above, I have to admit, nowadays, the term “CLIP” has become a convenient label for referring to both the training process and the resulting model as a whole. Somewhat confusing IMO, but that’s how language works. Here in this post, let’s stick to the context of the training framework.",
    "crumbs": [
      "Home",
      "Posts",
      "Deep Learning",
      "Multi Modal",
      "Base Modules",
      "LVM - CLIP"
    ]
  },
  {
    "objectID": "posts/deep-learning/multi-modal/base-modules/02-CLIP.html#contrastive-learning",
    "href": "posts/deep-learning/multi-modal/base-modules/02-CLIP.html#contrastive-learning",
    "title": "LVM - CLIP",
    "section": "Contrastive learning",
    "text": "Contrastive learning\nThe core of contrastive learning is the notion of learning by comparison. By contrasting positive pairs (related or similar data points) against negative pairs (unrelated or dissimilar data points), models can develop an understanding of the underlying structure and semantics of the data without the need for explicit annotations. This is accomplished through a contrastive loss function, which encourages the model to minimize the distance between representations of positive pairs while maximizing the distance between negative pairs.\n\nNoise contrastive estimation (NCE) loss\nThis loss function advocated to be used with CLIP is the Noice contrastive estimation (InfoNCE):\n\\[ L({\\bf U\\rightarrow V}) = -\\log\\left\\{ \\frac{\\exp \\left[\\operatorname{cosim}(u, v) / τ \\right]} {\\sum_{k=1}^N \\exp[\\operatorname{cosim}(u, v_{neg}) / τ] }\\right\\} \\]\nwhere:\n\n\\(N\\) is the number of samples\n\\(u\\) is the embedding vector of the image,\n\\(v\\) is the embedding vector of the corresponding positive text,\n\\(v_{neg}\\) are the embedding vectors of the negative texts,\n\\(\\text{cosim}(u, v)\\) is the cosine similarity between the image and text embeddings, i.e., \\(v^Tu / \\Vert v \\Vert \\Vert u \\Vert\\)\n\\(\\tau\\) is a temperature parameter that controls the separation of the scores. In Radford et al. (2021), this can be a learnable parameter as well.\n\nSee how this encourages the factor to be large (i.e., large distance between negative pairs), and the denominator to be small (i.e., small distance between positive pairs).\nIn both CLIP and ConVIRT, NCE is commonly used in pair:\n\\[ \\mathscr{L}({\\bf U, V}) = [\\lambda L({\\bf U \\rightarrow V}) + (1-\\lambda)\\cdot L({\\bf V \\rightarrow U})]\\]\nwhere \\(\\lambda\\) is a scalar weight to balance the importance of \\(\\bf U\\) and \\(\\bf V\\) as ground-truth. This loss is similar to co-registering the two encoded space, with \\(\\lambda\\) dictating which space moves faster.\n\n\n\n\n\n\nTip\n\n\n\nIn Raford et al.’s paper, they mentioned they used 592 and 256 V100 GPUs, and spent &gt; 10 days to train the largest ResNet and largest ViT, respectively (Radford et al. 2021). Whereas in (Zhang et al. 2020), it only took them 3 days on a Titan RTX.",
    "crumbs": [
      "Home",
      "Posts",
      "Deep Learning",
      "Multi Modal",
      "Base Modules",
      "LVM - CLIP"
    ]
  },
  {
    "objectID": "posts/deep-learning/multi-modal/base-modules/02-CLIP.html#notable-results",
    "href": "posts/deep-learning/multi-modal/base-modules/02-CLIP.html#notable-results",
    "title": "LVM - CLIP",
    "section": "Notable results",
    "text": "Notable results\n\nclassifier test results\n\n\n\nTable 1: Zero-shot classifying result extracted from (Radford et al. 2021). Notably, its doing bad at CLEVR, which is a counting challenge dataset, which is already tackled years ago.\n\n\n\n\n\n\nIts notable CLEVR is the only dataset where the trained CLIP-ResNet showed a significant drop at RN50x4 and RN50x16. This dataset contains mainly counting challenges. This might suggest CLIP does not have very strong reasoning capability.\n\n\n\n\n\n\nFigure 2: Zero-shot classifying results extracted from (Radford et al. 2021).\n\n\n\nBased on how these results were presented, I would say Radford et al. might have tested with dataset specific classes only when evaluating the zero-shot performance. For example, in Table 1, the number of classes tested for each dataset is different, but far from the claimed 32,768 classes. This might overstate the zero-shot classification performance because the image encoder was not put to test against cross dataset image-text pairs, but is only selected within the possible classes in a a dataset.",
    "crumbs": [
      "Home",
      "Posts",
      "Deep Learning",
      "Multi Modal",
      "Base Modules",
      "LVM - CLIP"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nLVM - BLIP-1 & BLIP-2\n\n\n\nlvm\n\n\nblip\n\n\nreview\n\n\nNLP\n\n\n\n\n\n\n\nMLun Wong\n\n\nFeb 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nLVM - CLIP\n\n\n\nlvm\n\n\nclip\n\n\nreview\n\n\nNLP\n\n\n\n\n\n\n\nMLun Wong\n\n\nFeb 9, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nLVM - General Info\n\n\n\ntransformer\n\n\nbert\n\n\nNLP\n\n\nreview\n\n\nlvm\n\n\n\n\n\n\n\nMLun Wong\n\n\nFeb 4, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nNarrative review - Reducing hallucination in MMLLM\n\n\n\nmulti-modal\n\n\nMMLLM\n\n\nhallucination\n\n\n\n\n\n\n\nMLun Wong\n\n\nFeb 4, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n[Review] Reducing hallucination: Woodpecker\n\n\n\nmulti-modal\n\n\nMMLLM\n\n\nhallucination\n\n\nreview\n\n\n\n\n\n\n\nMLun Wong\n\n\nFeb 4, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nIntensity normalization (Part 1) - General Info\n\n\n\nDICOM\n\n\nnormalization\n\n\nimaging\n\n\n\n\n\n\n\nMLun Wong\n\n\nJan 26, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nIntensity normalization (Part 2) - MRI\n\n\n\nDICOM\n\n\nnormalization\n\n\nimaging\n\n\nMRI\n\n\npython\n\n\n\n\n\n\n\nMLun Wong\n\n\nJan 26, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nDiscrete Fourier Transform\n\n\n\nDFT\n\n\npython\n\n\ncode\n\n\nsignal\n\n\n\n\n\n\n\nMLun Wong\n\n\nJan 11, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPyTorch DDP with torchio\n\n\n\npython\n\n\nddp\n\n\npytorch\n\n\ntorchio\n\n\n\n\n\n\n\nMLun Wong\n\n\nJan 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSetting up a headless server\n\n\n\ncode\n\n\nserver\n\n\nIT\n\n\nopencl\n\n\nxrdp\n\n\n\n\n\n\n\nMLun Wong\n\n\nJan 9, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nImaging - Convolution between an image and a filtering kernel\n\n\n\nPython\n\n\nCode\n\n\nConvolution\n\n\n\n\n\n\n\nMLun Wong\n\n\nJan 6, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nExplainable artificial intelligence for automatic detection of early nasopharyngeal carcinoma on MRI\n\n\nIn this study, we propose a simple method to improve the explainability of artificial intelligence, specifically convolutional neural networks (CNNs), for the automatic…\n\n\n\nLun M Wong, Qi-Yong H Ai, Tiffany Y So, Jacky WK Lam, Ann D King\n\n\nJun 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nStochastic - Particle Filtering and Markov Chain Monte Carlo\n\n\n\nstochastic\n\n\npython\n\n\nrandom\n\n\nnotes\n\n\nmonte carlo\n\n\nmcmc\n\n\n\n\n\n\n\nMLun Wong\n\n\nMay 11, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nStochastic - Python Example of a Random Walk Process\n\n\n\nstochastic\n\n\npython\n\n\nrandom\n\n\nnotes\n\n\n\n\n\n\n\nMLun Wong\n\n\nMar 12, 2017\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStochastic - Python Example of a Random Walk Process\n\n\n\nstochastic\n\n\npython\n\n\nrandom\n\n\nnotes\n\n\n\n\n\n\n\nMLun Wong\n\n\nMar 12, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nMedical imaging orientation in VTK and ITK\n\n\n\nDICOM\n\n\nNIFTI\n\n\nimage orientation\n\n\nvtk\n\n\nitk\n\n\n\n\n\n\n\nMLun Wong\n\n\nMar 10, 2017\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About Me",
    "section": "Education",
    "text": "Education\n\n\n\n\n\n\n\nYear\nQualification\n\n\n\n\n2015\nBachelor of Science (BSc), \nma Physics, mi Computer Science\nThe Chinese University of Hong Kong, HKSAR\n\n\n2018\nMaster of Philosophy (MPhil)\nDepartment of Imaging and Interventional Radiology\nThe Chinese University of Hong Kong, HKSAR\n\n\n2021\nDoctor of Philosophy (PhD)\nDepartment of Imaging and Interventional Radiology\nThe Chinese University of Hong Kong, HKSAR\n\n\n\n\nSelected Publications\n\nWong, L. M., et al. “Radiomics for Discrimination between Early-Stage Nasopharyngeal Carcinoma and Benign Hyperplasia with Stable Feature Selection on MRI. Cancers (Basel). 2022;14(14).\nWong, L. M., et al. “A convolutional neural network combined with positional and textural attention for the fully automatic delineation of primary nasopharyngeal carcinoma on non-contrast-enhanced MRI.” Quantitative Imaging in Medicine and Surgery 11.9 (2021): 3932-3944.\nWong, L. M., et al. “Convolutional neural network in nasopharyngeal carcinoma: how good is automatic delineation for primary tumor on a non-contrast-enhanced fat-suppressed T2-weighted MRI?” Japanese Journal of Radiology (2021): 1-9.\nWong, L. M., et al. “Convolutional neural network for discriminating nasopharyngeal carcinoma and benign hyperplasia on MRI.” European Radiology (2020): 1-8.\nWong, L. M., Shi, L., Xiao, F., & Griffith, J. F. “Fully automated segmentation of wrist bones on T2-weighted fat-suppressed MR images in early rheumatoid arthritis.” Quantitative imaging in medicine and surgery 9.4 (2019): 57.\nHung, K. F., …, Wong, L. M.*, & Leung, Y. Y.*. Automatic detection and segmentation of morphological changes of the maxillary sinus mucosa on cone-beam computed tomography images using a three-dimensional convolutional neural network. Clin Oral Investig, 26.5 (2022): 3987-3998. (*Corresponding Author)\nLam, W. K. J., A. D. King, … Wong, L. M., …, et al. “Recommendations for Epstein-Barr Virus-Based Screening for Nasopharyngeal Cancer in High- and Intermediate-Risk Regions.” J Natl Cancer Inst 115, no. 4 (Apr 11 2023): 355-64.\nZhang, R., King, A. D., Wong, L. M., Bhatia, K. S., Qamar, S., Mo, F. K. F., Vlantis, A. C., & Ai, Q. Y. H. (2023). Discriminating between benign and malignant salivary gland tumors using diffusion-weighted imaging and intravoxel incoherent motion at 3 Tesla. Diagn Interv Imaging, 104(2), 67-75.\nZhang, R., Ai, Q. Y. H., Wong, L. M., Green, C., Qamar, S., So, T. Y., Vlantis, A. C., & King, A. D. (2022). Radiomics for discriminating benign and malignant salivary gland tumors; which radiomic feature categories and MRI sequences should be used? Cancers, 14(23), 5804.",
    "crumbs": [
      "Home",
      "About Me"
    ]
  },
  {
    "objectID": "posts/deep-learning/explainable-ai/01-xai-npc-bn.html",
    "href": "posts/deep-learning/explainable-ai/01-xai-npc-bn.html",
    "title": "Explainable artificial intelligence for automatic detection of early nasopharyngeal carcinoma on MRI",
    "section": "",
    "text": "In this study, we propose a simple method to improve the explainability of artificial intelligence, specifically convolutional neural networks (CNNs), for the automatic detection of early nasopharyngeal carcinoma (NPC) on magnetic resonance imaging (MRI). We show a long-short-term-memory (LSTM) unit can be introduced into a CNN to read 3-dimensional medical image series. A risk curve can be extracted from the LSTM to visualize the “thought process” of the network when it reads through the input MRI slice-by-slice. This modification improves the explainability of the network without reducing performance for the early NPC detections of the original CNN.\n\n\n\n\n\n\nTip\n\n\n\nThis work was presented during ISMRM 2023, in Toronto, CA.",
    "crumbs": [
      "Home",
      "Posts",
      "Deep Learning",
      "Explainable AI (XAI)",
      "Explainable artificial intelligence for automatic detection of early nasopharyngeal carcinoma on MRI"
    ]
  },
  {
    "objectID": "posts/deep-learning/explainable-ai/01-xai-npc-bn.html#introduction",
    "href": "posts/deep-learning/explainable-ai/01-xai-npc-bn.html#introduction",
    "title": "Explainable artificial intelligence for automatic detection of early nasopharyngeal carcinoma on MRI",
    "section": "Introduction",
    "text": "Introduction\nEarly detection of nasopharyngeal carcinoma (NPC) can markedly reduce the mortality and morbidity of treatment complications. Magnetic resonance imaging (MRI) can detect up to 38.1% more early NPCs when compared to the endoscopy (King et al. 2020; Liu et al. 2021), which is the current investigation of choice for individuals who are Epstein-Barr Virus DNA positive on an NPC screening blood test (K. C. A. Chan et al. 2017; D. C. T. Chan et al. 2022). To expand the role of MRI in early detection, our team has previously investigated artificial intelligence (AI) algorithms to automatically diagnose and reduce costs (Wong, Ai, Mo, et al. 2021; Wong, Ai, Poon, et al. 2021; Wong, King, et al. 2021). However, despite these algorithms as well as those proposed by other research teams (Ke et al. 2020; Deng et al. 2022) performed remarkably in these tasks, they inevitably lack explainability because of their complex model structure comprising millions of learnable parameters. The explainability issue is currently one of the major factors that has prevented the implementation of these deep learning methods in clinical workflows.\nIn this study, we propose a method to improve the explainability of convolutional neural network (CNN) algorithms for the automatic diagnosis of NPC on MRI. It requires simply an additional long short-term memory (LSTM), which enables visualization of the network’s “thought process” and localization of key slices that determined the AI’s prediction and could aid understanding of the results.",
    "crumbs": [
      "Home",
      "Posts",
      "Deep Learning",
      "Explainable AI (XAI)",
      "Explainable artificial intelligence for automatic detection of early nasopharyngeal carcinoma on MRI"
    ]
  },
  {
    "objectID": "posts/deep-learning/explainable-ai/01-xai-npc-bn.html#methods",
    "href": "posts/deep-learning/explainable-ai/01-xai-npc-bn.html#methods",
    "title": "Explainable artificial intelligence for automatic detection of early nasopharyngeal carcinoma on MRI",
    "section": "Methods",
    "text": "Methods\n\nModel architecture\nWe introduced an LSTM unit to our previously proposed slice-wise residual attention network (SWRAN) (Wong, King, et al. 2021; Wang et al. 2017), inserted before the output linear layer and replacing the original max-pooling layer for improving explainability. On receiving MRI images, the original SWRAN encoded the input slice-by-slice which were then read sequentially by the additional LSTM to identify NPC. The prediction obtained from individual slices has the advantage over a single prediction from all slices in that it can indicate how the network shifted between an NPC positive or negative prediction while it scans through the slices. This shift can be visualized as a “risk curve” to improve explainability. Detailed architecture and a legend of the risk curve is given in Figure 1 and Figure 2, respectively. This network was trained in two steps. First, the original SWRAN without the LSTM was pre-trained. Then, the convolutional kernels were fixed to finetune only the LSTM and the output layer of the network (Figure 1).\n\n\n\n\n\n\nFigure 1: Network architecture of the proposed method for detecting nasopharyngeal carcinoma (NPC) on MRI. The original slice-wise residual attention network (SWRAN) encodes each slice of the input into deep feature vectors. During pre-train, the original output layer of SWRAN is invoked. During finetuning, the encoded vectors are sequentially fed into the long short-term memory (LSTM) unit. Its output after reading all slices is the final prediction, whereas the individual hidden states is the risk curve that visualize the “thought process” of the network.\n\n\n\n\n\nPatients for validation of performance\nWe retrospectively included 884 patients previously scanned for suspected NPC using the T2-weighted fat-suppressed sequence in our center on either a 1.5T or 3.0T MRI scanner, of which 316 had stage I/II NPC and 568 had benign hyperplasia/normal nasopharynx without NPC. These patients were divided with stratification to NPC status into training, testing and validation sets at a ratio of roughly 7:2:1 (620:176:88). Finally, the diagnostic performance of the pre-trained and finetuned network was evaluated over the testing set through receiver operator characteristics (ROC) analysis.\n\n\n\n\n\n\nFigure 2: Legend of the risk curve plotted using the hidden states of the long short-term memory (LSTM). There are three elements in this plot: (a) the risk curve, (b) the reference threshold, and (c) a vertical line marking the current slice. As the LSTM scans through the slices, it can respond to malignant features and predict higher risks of NPC, which stays high for subsequent slices that without tumor, showing the LSTM registers malignant features seen in its memory. The final prediction is taken at the end of the curve at the right, area above the threshold is the NPC +ve zone (red shade).\n\n\n\n\n\nAssessment of the risk curves\nAn expert in head and neck imaging inspected the risk curves of the testing set and summarized qualitatively the patterns and characteristics of the curves in NPC and benign hyperplasia/normal subjects.",
    "crumbs": [
      "Home",
      "Posts",
      "Deep Learning",
      "Explainable AI (XAI)",
      "Explainable artificial intelligence for automatic detection of early nasopharyngeal carcinoma on MRI"
    ]
  },
  {
    "objectID": "posts/deep-learning/explainable-ai/01-xai-npc-bn.html#results",
    "href": "posts/deep-learning/explainable-ai/01-xai-npc-bn.html#results",
    "title": "Explainable artificial intelligence for automatic detection of early nasopharyngeal carcinoma on MRI",
    "section": "Results",
    "text": "Results\n\nDiagnostic performance\nSensitivity, specificity, accuracy and area under the ROC curve (AUC) of the pre-trained network were 91.3%, 94.4%, 93.2% and 0.981, respectively; and those of the fine-tuned network with LSTM were 95.7%, 91.6%, 93.2% and 0.976, respectively.\n\n\nAssessment of the risk curve\nIn 63 out of 66 true-positive predictions of NPC cases, the risk curves were one of two forms: (i) started off flat but sharp increase when reaching the primary NPC (n=29); (ii) raised slowly from the beginning till the end (n=34). In 68 out of 98 true-negative predictions, the risk curves stayed flat under the reference line unremarkably, the rest fluctuates but eventually settled below the referenced threshold after all slices were read. In 2 out of 3 false-negative cases, the risk curves showed a local peak at the primary NP which might allow manual rectification. Representative examples are provided in Figure 3. The slope and value of the risk curves were exploited to locate and highlight key slices that lead to an NPC positive prediction shown in Figure 4. Examples of interpreting the risk curves are given in Figure 5.\n\n\n\n\n\n\nFigure 3: Representative examples of the risk curves plotted on the axial MRI of two patients with nasopharyngeal carcinoma (NPC) (left) and with benign hyperplasia (right). The x- and y-axis of the risk curves are the slice index and predict risk respectively. The green vertical line marks the position of the current slice on the risk plot. As shown on the left, the network responded and the risk value raised when it detected slices with NPC (white arrow), whereas the curve typically stays under the reference threshold for the benign cases like the one on the right.\n\n\n\n\n\n\n\n\n\nFigure 4: Auxiliary visual effects designed to highlight the slices that triggers the nasopharyngeal carcinoma (NPC) positive prediction by automatically interpreting the slope of the risk curve. Slices are order from left to right and top to bottom. The slope of the risk curve is used to locate critical slices that lead to the network’s prediction. Here we empirically set a threshold for slope to automatically highlight slices as amber, suggesting the network identify suspicious malignant features, and a higher level of slope to highlight slices as red, suggesting the network.\n\n\n\n\n\n\n\n\n\nFigure 5: More examples of interpretating the risk curves of two nasopharyngeal carcinoma (NPC) and a benign hyperplasia patient in the testing set. (a) true-positive prediction for NPC, the network responded to the enlarged left retropharyngeal node (blue arrow) that is near the tumor (green arrows) by rising risk values across slices; (b) false-negative prediction, but the risk curve reflected suspicious thickening (blue arrows) briefly; (c) a true-negative prediction, the benign hyperplasia was recognized and lighted up a slice, but the risk curve is otherwise unremarkable.",
    "crumbs": [
      "Home",
      "Posts",
      "Deep Learning",
      "Explainable AI (XAI)",
      "Explainable artificial intelligence for automatic detection of early nasopharyngeal carcinoma on MRI"
    ]
  },
  {
    "objectID": "posts/deep-learning/explainable-ai/01-xai-npc-bn.html#discussion",
    "href": "posts/deep-learning/explainable-ai/01-xai-npc-bn.html#discussion",
    "title": "Explainable artificial intelligence for automatic detection of early nasopharyngeal carcinoma on MRI",
    "section": "Discussion",
    "text": "Discussion\nWe have proposed a simple method using LSTM to improve the explainability of CNN for early NPC detection on MRI in this study that achieved an AUC of 0.975, which is similar to the CNN without the LSTM (AUC=0.981). Compared to other methods employed to explain classification predictions by CNN in the literature, such as the attention mechanism and guided back-propagation (Springenberg et al. 2014) and Grad-CAM (Selvaraju et al., n.d.), our proposed method has the advantage of removing the arbitrariness from the need to pick a layer for visualization. It can add also insight into cases that produce false-positive or negative results which may enable modifications to future networks to improve performance. This study has some limitations. Firstly, this study did not address quantitatively the improvements in explainability owing to the lack of well-established metrics or systematic methods to do so. Secondly, the addition of LSTM increases training time and difficulty because the network needs to be trained in two steps.",
    "crumbs": [
      "Home",
      "Posts",
      "Deep Learning",
      "Explainable AI (XAI)",
      "Explainable artificial intelligence for automatic detection of early nasopharyngeal carcinoma on MRI"
    ]
  },
  {
    "objectID": "posts/deep-learning/explainable-ai/01-xai-npc-bn.html#conclusion",
    "href": "posts/deep-learning/explainable-ai/01-xai-npc-bn.html#conclusion",
    "title": "Explainable artificial intelligence for automatic detection of early nasopharyngeal carcinoma on MRI",
    "section": "Conclusion",
    "text": "Conclusion\nLSTM introduced to CNN can improve the explainability without compromising performance in automatic early NPC detection on MRI. Acknowledgements We would like to acknowledge Mr. Yip Man Tsang for his effort to collect the imaging dataset that is used in this study.",
    "crumbs": [
      "Home",
      "Posts",
      "Deep Learning",
      "Explainable AI (XAI)",
      "Explainable artificial intelligence for automatic detection of early nasopharyngeal carcinoma on MRI"
    ]
  },
  {
    "objectID": "posts/deep-learning/multi-modal/base-modules/01-general-info.html",
    "href": "posts/deep-learning/multi-modal/base-modules/01-general-info.html",
    "title": "LVM - General Info",
    "section": "",
    "text": "Language-visual model (LVM) has quite a general scope covering any model that deal with both language and visual information, but let’s not complicate things and only look at the possible combination of outputs:\n\nText to image ➠ E.g., Stable diffusion\nImage to text ➠ E.g., Imaging captioning\nText to Image + text\nImage to Image + text\nText + Image to Text ➠ E.g., Radiology reporting.\nText + Image to Image ➠ E.g., Stable diffusion, Animation, Style changes.\n\n\n\n\n\n\n\nTwo combinations was crossed out just because no compelling examples came to my mind, but I am sure there’s some applications out there that fits the profile.\n\n\n\nTo understand how these applications were achieved, some milestone modules needs to be properly introduced, which is the purpose of this post.",
    "crumbs": [
      "Home",
      "Posts",
      "Deep Learning",
      "Multi Modal",
      "Base Modules",
      "LVM - General Info"
    ]
  },
  {
    "objectID": "posts/deep-learning/multi-modal/base-modules/01-general-info.html#self-attention-mechanism",
    "href": "posts/deep-learning/multi-modal/base-modules/01-general-info.html#self-attention-mechanism",
    "title": "LVM - General Info",
    "section": "Self attention mechanism",
    "text": "Self attention mechanism\nAt the heart of the Transformer is the self-attention mechanism, which computes attention scores to capture the relevance of all other tokens in the input sequence for each token. Considering the sentence “I am a boy,” the Transformer doesn’t simply assign weights to “I” related to “am”, “a”, and “boy”. Instead, it evaluates how each word should attend to every other word in the sentence, allowing the model to contextualize each word, such as understanding “boy” in relation to ““. This contextualization is essential for nuanced language understanding.",
    "crumbs": [
      "Home",
      "Posts",
      "Deep Learning",
      "Multi Modal",
      "Base Modules",
      "LVM - General Info"
    ]
  },
  {
    "objectID": "posts/deep-learning/multi-modal/base-modules/01-general-info.html#multi-head-attention",
    "href": "posts/deep-learning/multi-modal/base-modules/01-general-info.html#multi-head-attention",
    "title": "LVM - General Info",
    "section": "Multi-head attention",
    "text": "Multi-head attention\nAttention heads in transformer refers to the attention mapping mentioned about. Multi-head attention simply means there are multiple mappings being constructed to attend to different “relevance” between the tokens. In my previous example, “I” and “boy” would have different semantic connection as “I” and “am”, and therefore result in a different attention weight.\n\nSelf-attention (SA)\nSelf-attention is a mechanism that allows each position in the input sequence to attend to all positions within the same sequence. In transformer blocks, this is used to compute a representation of the sequence where the contribution of other tokens to the representation of each token is determined by the attention scores.\nThe process involves three sets of learned weights: query \\(Q\\), key \\(K\\), and value \\(V\\) matrices. For a given token, the model computes its query and then calculates attention scores by taking the dot product of this query with the keys of all tokens in the sequence, including itself. These scores are then normalized using a softmax function, and this distribution is used to create a weighted sum of the values, resulting in the final self-attended output for that token.\nSelf-attention allows the model to integrate information from the entire sequence, making it powerful for tasks such as language modeling, where the relevance of context can vary greatly based on the content.\n\n\n\n\n\n\nTechnical details\n\n\n\n\n\nHere are some additional details of \\(Q,V,K\\) vectors in SA module\n\\[ \\begin{array}{cll} Q\\in\\mathbb{R}^{d_k}: &\\text{query, to match others} & =\\mathscr{U}\\cdot W^q \\\\ K\\in\\mathbb{R}^{d_k}: &\\text{key, to be matched} &= \\mathscr{U}\\cdot W^k \\\\ V\\in\\mathbb{R}^{d_v}: & \\text{values, information to be extracted} & =\\mathscr{U}\\cdot W^v \\end{array} \\]\nAttention matrices are then formed by:\n\\[ \\alpha=\\frac{Q^TK}{\\sqrt{d}} \\]\nwhere d is the dim of \\(Q\\) and \\(K\\). Self-attention \\(\\hat{\\alpha}\\) is obtained from the soft-max of this attention matrices:\n\\[ \\hat{\\alpha}_{m,i}=\\frac{\\exp(\\alpha_{m,i})}{\\sum_j \\exp(\\alpha_{m,j})} \\]\nwhere \\(m\\) denotes the row index and \\(i\\) denotes the column index. This attention is a coefficient prepared for \\(V\\). The output sequence is then \\(B=\\hat{\\alpha}V^T\\), or\n\\[\nB=\\text{Attention}(Q, K, V)=V\\cdot \\text{softmax}\\left(\\frac{Q^TK}{\\sqrt{d_k}} \\right)\n\\]\n\nMulti-head SA\nFor multi-head, it just mean there’s more than one attention mapper such that the hidden layer is obtained by:\n\\[\n\\begin{aligned}\n\\text{Multi-head}(Q, K, V)=\\text{concat}(H_1, \\dots,H_h)W^O \\\\\n\\text{where }H_i =\\text{Attention}(QW^q_i,KW^k_i, VW^vi) \\\\\n\\end{aligned}\n\\]\nwhere there’s an additional dimension \\(d_k\\) for number of attention heads.\n\\[\n\\begin{align*}\nW^q_i &\\in \\mathbb{R}^{d_{model}\\times d_k} \\\\\nW^k_i &\\in \\mathbb{R}^{d_{model}\\times d_k} \\\\\nW^v_i &\\in \\mathbb{R}^{d_{model}\\times d_v} \\\\\nW^O &\\in \\mathbb{R}^{hd_{model}\\times d_v}\n\\end{align*}\n\\]\n\n\n\n\n\n\nCross-attention (CA)\nCross-attention is used when there are two different sequences, and the goal is to let one sequence attend to the other. This mechanism is central to the encoder-decoder structure in transformer models, where the decoder attends to the output of the encoder.\nSimilar to self-attention, cross-attention also uses \\((Q, K, V)\\) However, in this case, \\(Q\\) come from one sequence (typically the decoder), and \\((K, V)\\) come from another (typically from the encoder). The decoder’s queries are used to attend to the encoder’s keys and values, allowing the decoder to focus on relevant parts of the input sequence when generating each token of the output sequence.\nCross-attention is particularly useful in machine translation, where the model needs to consider information from an input sentence when constructing a sentence in the target language. It is also useful in mult-modal situation where you want different encoded sequences attend to each other int the low-dimensional latent space.\n\n\n\n\n\n\nFigure 2: Example of cross-attention layer extract from (Li et al. 2021) ([CC BY-NC-SA]). Notice how they use cross-attention to embed both textual and visual information.",
    "crumbs": [
      "Home",
      "Posts",
      "Deep Learning",
      "Multi Modal",
      "Base Modules",
      "LVM - General Info"
    ]
  },
  {
    "objectID": "posts/deep-learning/multi-modal/base-modules/01-general-info.html#postion-encoding",
    "href": "posts/deep-learning/multi-modal/base-modules/01-general-info.html#postion-encoding",
    "title": "LVM - General Info",
    "section": "Postion encoding",
    "text": "Postion encoding\nOne interesting details about transformer is it uses the sine and cosine function for position encoding. In particular, the position of tokens in the vector is encoded by adding a fix sequence obtained by:\n\\[\n\\begin{align}\n\\operatorname{PE}_{\\tt pos, 2i} &= \\sin(\\text{pos} / 10000^{2i/d_{\\tt model}}) \\\\\n\\operatorname{PE}_{\\tt pos, 2i+1} &=\\cos(\\text{pos} / 10000^{2i/d_{\\tt model}})  \n\\end{align}\n\\]\nwhere \\(\\tt pos\\) is the sequence position of the token, \\(i\\) is the channel, \\(d_{\\tt model}\\) is the total dimension of the embedding space. The position information is embedded just by adding the resultant vector from the equation above to the input vector (which is already embedded by the input embedding step).\nCiting the authors explaination:\n\nWe chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset \\(k, PE_{pos+k}\\) can be represented as a linear function of \\(PE_{pos}\\)\n\nThis is quite interesting as the more intuitive way would just be adding equally spaced embeddings, but the author mentioned this choice is an empirical decision that returned better results. They did not really went into detail why this is the case, but I think this post online summarized it pretty comprehensibly.",
    "crumbs": [
      "Home",
      "Posts",
      "Deep Learning",
      "Multi Modal",
      "Base Modules",
      "LVM - General Info"
    ]
  },
  {
    "objectID": "posts/deep-learning/multi-modal/base-modules/01-general-info.html#purpose-of-bert",
    "href": "posts/deep-learning/multi-modal/base-modules/01-general-info.html#purpose-of-bert",
    "title": "LVM - General Info",
    "section": "Purpose of BERT",
    "text": "Purpose of BERT\nBERT plays the role as the “dictionary” for downstream applications by providing an effective way to learn representations of corpus from a bunch of unlabeled text, i.e., allowing the computer to understand human language. The original author showed that by pre-training BERT on a corpus, and then fine-tunning it to do various downstream tasks (by adding an output layer), BERT showed state-of-the-art performance at the time of their publication, which is quite astonishing taking the number of trainable parameters into consideration.\n\n\n\n\n\n\nflowchart LR\n  data[(Big un-labelled &lt;br&gt;text dataset)]\n  subgraph B[BERT model]\n    direction LR\n    BERT --&gt; |Attached to|ol[Output Layer]\n  end\n    data --&gt; |Pre-train|BERT\n  data2[(Labelled targets)] --&gt; |Fine-tune|B\n  %% data2 --&gt; |fine-tune|BERT & ol\n\n\n\n\nFigure 4: Simplified workflow to train BERT for customized tasks",
    "crumbs": [
      "Home",
      "Posts",
      "Deep Learning",
      "Multi Modal",
      "Base Modules",
      "LVM - General Info"
    ]
  },
  {
    "objectID": "posts/deep-learning/multi-modal/base-modules/01-general-info.html#application-from-bert",
    "href": "posts/deep-learning/multi-modal/base-modules/01-general-info.html#application-from-bert",
    "title": "LVM - General Info",
    "section": "Application from BERT",
    "text": "Application from BERT\nAfter BERT is pre-trained, it can be quickly adapted to a certain task which uses the pre-trained corpus by attaching BERT to an output layer. For example, BERT can be configured to extract an answer of a question from a paragraph of text (Question answering model) by adding an output layer that convert the transformer outputs into 2-channel logits representing probability as starting index, and ending index.\nTaking Huggingface’s example implementation of BERT QA model transformers.BertForQuestionAnswering\nBertForQuestionAnswering(\n1  (bert): BertModel(\n    (embeddings): BertEmbeddings(...)\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(...)\n        )\n      )\n    )\n  )\n2  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n)\n\n1\n\nBERT model pretrained with any corpus\n\n2\n\nOutput layer customized for different applications",
    "crumbs": [
      "Home",
      "Posts",
      "Deep Learning",
      "Multi Modal",
      "Base Modules",
      "LVM - General Info"
    ]
  },
  {
    "objectID": "posts/deep-learning/multi-modal/base-modules/01-general-info.html#training-gpt",
    "href": "posts/deep-learning/multi-modal/base-modules/01-general-info.html#training-gpt",
    "title": "LVM - General Info",
    "section": "Training GPT",
    "text": "Training GPT\nThe first version of GPT is trained two steps: 1) unsupervised next-word prediction, 2) supervised fine-tuning. The first step is pretty standard so lets not go into details. The second step involving feeding the network with the text tokens that has been labeled. The text tokens are constructed based on the task, and you can see in Figure 5 how some of the tasks were formulated. The GPT, receiving the input, would not only have to output tokens, but also predict the task that is labeled.",
    "crumbs": [
      "Home",
      "Posts",
      "Deep Learning",
      "Multi Modal",
      "Base Modules",
      "LVM - General Info"
    ]
  },
  {
    "objectID": "posts/deep-learning/multi-modal/base-modules/01-general-info.html#modifications-in-gpt-2",
    "href": "posts/deep-learning/multi-modal/base-modules/01-general-info.html#modifications-in-gpt-2",
    "title": "LVM - General Info",
    "section": "Modifications in GPT-2",
    "text": "Modifications in GPT-2\nOpen-AI was not being incredibly transparent for the modifications they made in GPT-2. Based on the paper they released (Radford et al. 2019), the only structural changes are model size and they moved the layer norm layer to the inputs of each sub-block, instead of after masked multi-head attention, and finally they prune the pre-train data used. They also trained with different sizes to observe the performance changes.\n\n\n\nFigure 1: Architecture of transformer extracted from (Vaswani et al. 2017).\nFigure 2: Example of cross-attention layer extract from (Li et al. 2021) ([CC BY-NC-SA]). Notice how they use cross-attention to embed both textual and visual information.\nFigure 3: Summary of training BERT to understand language. Extracted from (Devlin et al. 2018)\nFigure 5: Outline of first generation of GPT structure and principles. See how the input text is being used not just for text generation, but also task classification. The authors trained the input encoder to Image is extracted from (Radford et al. 2018).",
    "crumbs": [
      "Home",
      "Posts",
      "Deep Learning",
      "Multi Modal",
      "Base Modules",
      "LVM - General Info"
    ]
  },
  {
    "objectID": "posts/deep-learning/multi-modal/base-modules/03-BLIP.html",
    "href": "posts/deep-learning/multi-modal/base-modules/03-BLIP.html",
    "title": "LVM - BLIP-1 & BLIP-2",
    "section": "",
    "text": "Under construction\n\n\n\nThis page is under construction.",
    "crumbs": [
      "Home",
      "Posts",
      "Deep Learning",
      "Multi Modal",
      "Base Modules",
      "LVM - BLIP-1 & BLIP-2"
    ]
  },
  {
    "objectID": "posts/deep-learning/multi-modal/base-modules/03-BLIP.html#components",
    "href": "posts/deep-learning/multi-modal/base-modules/03-BLIP.html#components",
    "title": "LVM - BLIP-1 & BLIP-2",
    "section": "Components",
    "text": "Components\n\nUnimodal encoders\nThis refers to the typical vision encoder and text encoder pair, which are trained together using the image-text contrastive loss like in CLIP. In BLIP’s paper, they use ViT as vision-encoder and BERT as text-encoder. For this part of the text they use [CLS] token to signify its begining.\n\n\nImage-ground text encoder\nCLIP-like training in unimodal encoders aims to alight the vision-encoded space to text-encoded space. However, the encoding is not always capturing the desirable information because this desired information could be versatile depending on the context of the task at hand.\nTherefore, the image-grounded text encoder is designed to create a representation for understanding-based tasks such as image-text retrieval. Here’s how it functions:\n\nA task-specific token ([Encode]) is added to text input, the output embedding of which serves as the image-text representation.\nIt is trained with an Image-Text Matching (ITM) loss to determine if an image-text pair matches (positive) or not (negative).\nThe purpose of the image-grounded text encoder is to enable the model to understand and reason about the content when given an image and corresponding text.\n\n\n\nImage-ground text decoder\nThe image-grounded text decoder focuses on the generation of text based on visual inputs:\n\nIt employs causal self-attention to ensure that the generated text follows a coherent sequence corresponding to the image content.\nThe decoder is trained with a Language Modeling (LM) loss to maximize the likelihood of generating a caption that accurately describes the given image.\nThe rationale behind the image-grounded text decoder is to allow the model to generate natural language descriptions for images, effectively learning to “speak” about what it “sees”.",
    "crumbs": [
      "Home",
      "Posts",
      "Deep Learning",
      "Multi Modal",
      "Base Modules",
      "LVM - BLIP-1 & BLIP-2"
    ]
  },
  {
    "objectID": "posts/deep-learning/multi-modal/base-modules/03-BLIP.html#cap-filt",
    "href": "posts/deep-learning/multi-modal/base-modules/03-BLIP.html#cap-filt",
    "title": "LVM - BLIP-1 & BLIP-2",
    "section": "Cap Filt",
    "text": "Cap Filt\nWeb crawling results in noisy image, therefore the authors proposed the Captioning and Filtering to filter away noisy (image, text) pairs obtained from the internet. The basic principle is to use a captioner module to caption web images, and a filter module to remove noisy image-text pairs.\n\n\n\n\n\n\nFigure 2: Concept of Caption and Filter proposed in (Li et al. 2022). (CC-BY)\n\n\n\nThe captioned and filters are initialized by the pre-trained model and then fine-tuned on small-scale human-annotated dataset to perform both captioning and filtering.",
    "crumbs": [
      "Home",
      "Posts",
      "Deep Learning",
      "Multi Modal",
      "Base Modules",
      "LVM - BLIP-1 & BLIP-2"
    ]
  },
  {
    "objectID": "posts/deep-learning/multi-modal/base-modules/03-BLIP.html#results",
    "href": "posts/deep-learning/multi-modal/base-modules/03-BLIP.html#results",
    "title": "LVM - BLIP-1 & BLIP-2",
    "section": "Results",
    "text": "Results\nBLIP-1 did present state-of-the-art results in most of the tasks, but it was not really out-performing other modules by too a lot in these regular tasks. However, a notable point is it’s performance in zero-shot text-to-video retrieval, which out-perform other methods including CLIP by a big margin. This retrieval was done by uniformly sample 8 and 16 images from each video for retrieval and captioning task, respectively. Although the author did not further analyze possible insights into this, I believe this has to do with the ability to tell noise from context through CapFilt, which would be especially important in grasping the context of a video as the chance to capture relevant context from uniformly sampled frames ain’t that high.\n\n\n\nTable 1: BLIP-1 showing significant improvements in text-to-video retreival (i.e., seaching for most relevant video with text key words)",
    "crumbs": [
      "Home",
      "Posts",
      "Deep Learning",
      "Multi Modal",
      "Base Modules",
      "LVM - BLIP-1 & BLIP-2"
    ]
  },
  {
    "objectID": "posts/deep-learning/multi-modal/base-modules/03-BLIP.html#q-former",
    "href": "posts/deep-learning/multi-modal/base-modules/03-BLIP.html#q-former",
    "title": "LVM - BLIP-1 & BLIP-2",
    "section": "Q-Former",
    "text": "Q-Former\n\n\n\n\n\n\nFigure 3: Structure of Q-Former extracted from (Li et al. 2023). The Q-former consist of two transformer submodules that shares the same self-attention layers (orange layer). The first transformer interacts with image encoder, whereas the second transformer can act as both a text encoder and decoder, depending on task context. The outputs of these two transformer are not connected during inference.\n\n\n\nThe structure of Q-Former is cropped from the original paper and placed above Figure 3.\n\nImage transformer module\nThis module is responsible for “querying” the encoded image for the extraction of visual information with consideration to the input text input into the Text transformer module.\nA key novelty here is there’s a set of learnable query embedding for EACH of the transformer blocks of this module. They interacts with the text input through transformer’s self-attention (SA) layer connection, and interact with the encoded image through cross-attention layer.\n\n\nText transformer module\nThis module takes in the input prompt, there’s no CA layer\n\n\n\nFigure 1: The outline of first version of BLIP extracted from (Li et al. 2022) (CC-BY)\nFigure 2: Concept of Caption and Filter proposed in (Li et al. 2022). (CC-BY)\nTable 1: BLIP-1 showing significant improvements in text-to-video retreival (i.e., seaching for most relevant video with text key words)\nFigure 3: Structure of Q-Former extracted from (Li et al. 2023). The Q-former consist of two transformer submodules that shares the same self-attention layers (orange layer). The first transformer interacts with image encoder, whereas the second transformer can act as both a text encoder and decoder, depending on task context. The outputs of these two transformer are not connected during inference.",
    "crumbs": [
      "Home",
      "Posts",
      "Deep Learning",
      "Multi Modal",
      "Base Modules",
      "LVM - BLIP-1 & BLIP-2"
    ]
  },
  {
    "objectID": "posts/deep-learning/multi-modal/hallucinations/02_woodpecker.html",
    "href": "posts/deep-learning/multi-modal/hallucinations/02_woodpecker.html",
    "title": "[Review] Reducing hallucination: Woodpecker",
    "section": "",
    "text": "A group from Tecent proposed to use 5-step prompt-based technique to reduce hallucination (Yin et al. 2023). This method does not require any fine-tunning or re-training as it’s only a prompt engineering technique. It involves using carefully designed few-shot prompts to augment the original output of by the MMLLM. Here’s an outline of their method Figure 1.\n\nYin, Shukang, Chaoyou Fu, Sirui Zhao, Tong Xu, Hao Wang, Dianbo Sui, Yunhang Shen, Ke Li, Xing Sun, and Enhong Chen. 2023. “Woodpecker: Hallucination Correction for Multimodal Large Language Models.” https://doi.org/10.48550/ARXIV.2310.16045.\n\n\n\n\n\n\nflowchart TD\n    input(Original MLLM output) --&gt; s1[Step 1: Key Concept Extraction] \n    s1 --&gt; s2[Step 2: Question formulation]\n    s2 --&gt; s3[Step 3: Visual knowledge validation]\n    s3 --&gt; s4[Step 4: Visual claim generation]\n    s3 --&gt; |Bounding Box Info|image(Annotate on Image)\n    s4 --&gt; s5[Step 5: Hallucination correction]\n    s5 --&gt; output(Output prompt)\n\n\n\n\nFigure 1: Flowchart outlinging the Woodpecker method.\n\n\n\n\n\nTheir default base model was mPLUG-owl, and their online demo was down as of the date I write this post. But I did have had a chance to play with it. It gives pretty intuitive results and the model make much more sense than the original mplug-owl-llama-7b . I am quite surprise to see the model is able to give pretty accurate bounding box without fine-tunning\n\n\n\n\n\n\n\n\nFigure 2: Summary workflow of Woodpecker method extracted from the original paper. Note that the bounding box is drawn from the prompt output by visual knowledge validation step.\n\n\n\n\n\n\nThe authors provides three prompt templates corresponding to their 5-step workflow. Here’s a recap\n\n\nThe target of this step is to extract the entities from the MMLLM output of concern for further verification.\n# System message\nYou are a language assistant that helps to extract information from given sentences\n\n# Prompt\nGiven a sentence, extract the existent entities within the sentence for me. Extract the common objects and summarize them as general categories without repetition, merge essentially similar objects.\nAvoid extracting abstract or non-specific entities. Only extract concrete, certainly existent objects that fall in general categories and are described in a certain tone in the sentence. Extract entity in the singular form. \nOutput all the extracted types of items in one line and separate each object type with a period. If there is nothing to output, then output a single “None”.\n\n# Examples:\n{In-context Examples}\n\n# Sentence: \n{Input Sentence}\n\nOutput:\n\n\n\n\n\n\nNote\n\n\n\nThe authors did not give the specific details of the {In-context Examples} they noted in all of their templates on their arxiv publication, but they can be found in their github repository [link].\n\n\n\n\n\nGiven the key concepts extracted from the last step, the authors designed a template to instruct the original model to expand the original questions. This step increase the reasoning by adding intermediate points before arriving to a conclusion.\n# System message\nYou are a language assistant that helps to extract information from given sentences\n\n# Prompt\nGiven a sentence, extract the existent entities within the sentence for me.\nGiven a sentence and some entities connected by periods, you are required to ask some relevant questions about the\nspecified entities involved in the sentence, so that the questions can help to verify the factuality of the sentence.\nQuestions may involve basic attributes such as colors and actions mentioned in the sentence. Do not ask questions involving object counts or the existence of objects.\nWhen asking questions about attributes, try to ask simple questions that only involve one entity.\nAsk questions that can be easily decided visually. Do not ask questions that require complex reasoning.\nDo not ask semantically similar questions. Do not ask questions only about scenes or places.\nUse “where” type questions to query the position information of the involved entities.\nDo not ask questions about uncertain or conjecture parts of the sentence, for example, the parts described with “maybe” or “likely”, etc.\nIt is no need to cover all the specified entities. If there is no question to ask, simply output a “None”.\nWhen asking questions, do not assume the claims in the description as true in advance. Only ask questions relevant to the information in the sentence.\nOnly ask questions about common, specific, and concrete entities. The entities involved in the questions are limited to the range within the given entities.\nOutput only one question in each line. For each line, first output the question, then a single “&”, and finally entities involved in the question, still connected by periods if multiple entities are invovled.\n\n# Examples:\n{In context examples}\n\n# Sentence:\n{Input sentence}\n\n# Entities:\n{Input entities}\n\nQuestions:\n\n\n\n\n\n\nNote\n\n\n\nAgain their {In-context examples} can be found on their github. [link]\n\n\n\n\n\nIn this step, the authors employed a pre-trained VQA (Visual question answering) model, which they found to produce fewer hallucination, to answer the question. This step also produces the bounding box that they used to label the items on the image.\n\n\n\n\n\n\nTip\n\n\n\nVQA here is already pre-trained, therefore you need to find a VQA model that is most relevant with the problem at hand. I.e., its not effective to train a medical imaging MMLVM model to have a VQA model trained on regular natural images. For Woodpecker to work, you have to also have a VQA model that is trained using medical imaging data.\n\n\n\n(To be continue)",
    "crumbs": [
      "Home",
      "Posts",
      "Deep Learning",
      "Multi Modal",
      "Hallucinations",
      "[Review] Reducing hallucination: Woodpecker"
    ]
  },
  {
    "objectID": "posts/deep-learning/multi-modal/hallucinations/02_woodpecker.html#summary",
    "href": "posts/deep-learning/multi-modal/hallucinations/02_woodpecker.html#summary",
    "title": "[Review] Reducing hallucination: Woodpecker",
    "section": "",
    "text": "Figure 2: Summary workflow of Woodpecker method extracted from the original paper. Note that the bounding box is drawn from the prompt output by visual knowledge validation step.",
    "crumbs": [
      "Home",
      "Posts",
      "Deep Learning",
      "Multi Modal",
      "Hallucinations",
      "[Review] Reducing hallucination: Woodpecker"
    ]
  },
  {
    "objectID": "posts/deep-learning/multi-modal/hallucinations/02_woodpecker.html#prompt-template",
    "href": "posts/deep-learning/multi-modal/hallucinations/02_woodpecker.html#prompt-template",
    "title": "[Review] Reducing hallucination: Woodpecker",
    "section": "",
    "text": "The authors provides three prompt templates corresponding to their 5-step workflow. Here’s a recap\n\n\nThe target of this step is to extract the entities from the MMLLM output of concern for further verification.\n# System message\nYou are a language assistant that helps to extract information from given sentences\n\n# Prompt\nGiven a sentence, extract the existent entities within the sentence for me. Extract the common objects and summarize them as general categories without repetition, merge essentially similar objects.\nAvoid extracting abstract or non-specific entities. Only extract concrete, certainly existent objects that fall in general categories and are described in a certain tone in the sentence. Extract entity in the singular form. \nOutput all the extracted types of items in one line and separate each object type with a period. If there is nothing to output, then output a single “None”.\n\n# Examples:\n{In-context Examples}\n\n# Sentence: \n{Input Sentence}\n\nOutput:\n\n\n\n\n\n\nNote\n\n\n\nThe authors did not give the specific details of the {In-context Examples} they noted in all of their templates on their arxiv publication, but they can be found in their github repository [link].\n\n\n\n\n\nGiven the key concepts extracted from the last step, the authors designed a template to instruct the original model to expand the original questions. This step increase the reasoning by adding intermediate points before arriving to a conclusion.\n# System message\nYou are a language assistant that helps to extract information from given sentences\n\n# Prompt\nGiven a sentence, extract the existent entities within the sentence for me.\nGiven a sentence and some entities connected by periods, you are required to ask some relevant questions about the\nspecified entities involved in the sentence, so that the questions can help to verify the factuality of the sentence.\nQuestions may involve basic attributes such as colors and actions mentioned in the sentence. Do not ask questions involving object counts or the existence of objects.\nWhen asking questions about attributes, try to ask simple questions that only involve one entity.\nAsk questions that can be easily decided visually. Do not ask questions that require complex reasoning.\nDo not ask semantically similar questions. Do not ask questions only about scenes or places.\nUse “where” type questions to query the position information of the involved entities.\nDo not ask questions about uncertain or conjecture parts of the sentence, for example, the parts described with “maybe” or “likely”, etc.\nIt is no need to cover all the specified entities. If there is no question to ask, simply output a “None”.\nWhen asking questions, do not assume the claims in the description as true in advance. Only ask questions relevant to the information in the sentence.\nOnly ask questions about common, specific, and concrete entities. The entities involved in the questions are limited to the range within the given entities.\nOutput only one question in each line. For each line, first output the question, then a single “&”, and finally entities involved in the question, still connected by periods if multiple entities are invovled.\n\n# Examples:\n{In context examples}\n\n# Sentence:\n{Input sentence}\n\n# Entities:\n{Input entities}\n\nQuestions:\n\n\n\n\n\n\nNote\n\n\n\nAgain their {In-context examples} can be found on their github. [link]\n\n\n\n\n\nIn this step, the authors employed a pre-trained VQA (Visual question answering) model, which they found to produce fewer hallucination, to answer the question. This step also produces the bounding box that they used to label the items on the image.\n\n\n\n\n\n\nTip\n\n\n\nVQA here is already pre-trained, therefore you need to find a VQA model that is most relevant with the problem at hand. I.e., its not effective to train a medical imaging MMLVM model to have a VQA model trained on regular natural images. For Woodpecker to work, you have to also have a VQA model that is trained using medical imaging data.\n\n\n\n(To be continue)",
    "crumbs": [
      "Home",
      "Posts",
      "Deep Learning",
      "Multi Modal",
      "Hallucinations",
      "[Review] Reducing hallucination: Woodpecker"
    ]
  },
  {
    "objectID": "posts/medical-imaging/intensity-normalization/02_intensity-normalization-mri.html",
    "href": "posts/medical-imaging/intensity-normalization/02_intensity-normalization-mri.html",
    "title": "Intensity normalization (Part 2) - MRI",
    "section": "",
    "text": "Caution\n\n\n\nUnder construction",
    "crumbs": [
      "Home",
      "Posts",
      "Medical Imaging",
      "Intensity Normalization",
      "Intensity normalization (Part 2) - MRI"
    ]
  },
  {
    "objectID": "posts/medical-imaging/intensity-normalization/02_intensity-normalization-mri.html#weighted-sequence",
    "href": "posts/medical-imaging/intensity-normalization/02_intensity-normalization-mri.html#weighted-sequence",
    "title": "Intensity normalization (Part 2) - MRI",
    "section": "Weighted sequence",
    "text": "Weighted sequence\nWeighted sequences are fundamental MRI sequences in radiology. While radiologists examine a variety of different sequences, T1w and T2w scans remain integral components of most imaging protocols. The signal intensities observed in T1/T2-weighted MRI sequences are derived from the T1 and T2 relaxation times of tissues, which are calculated by fitting the signal strength at each location over time. Consequently, these intensity values are not automatically reliable for quantitative analysis due to their generation method.\nThus, intensity normalization is a necessary step for quantitative analysis of MRI sequences (Figure 1).\n\n\n\n\n\n\nMRI intra-scanner repeatability\n\n\n\n\nFigure 1: Image the same salivary gland tumor scanned with two different MRI machine.\n\n\n\n\n\nExtracted from my conference poster “Reproducibility of Parotid Gland Tumor MRI Radiomic Features Across Different MRI Scanners”, AOCNR 2023, Singpore.\n\n\n\n\n\n\nNote\n\n\n\nAs the human eye relies on contrast rather than absolute intensity values for visual interpretation, and this contrast is consistent across different machines, this property of weighted sequences does not really affect radiologists. The requirement to normalize images was most probably raised by neuro-scientists who have extensively investigated the brain with MRI.",
    "crumbs": [
      "Home",
      "Posts",
      "Medical Imaging",
      "Intensity Normalization",
      "Intensity normalization (Part 2) - MRI"
    ]
  },
  {
    "objectID": "posts/medical-imaging/intensity-normalization/02_intensity-normalization-mri.html#quantitative-sequences",
    "href": "posts/medical-imaging/intensity-normalization/02_intensity-normalization-mri.html#quantitative-sequences",
    "title": "Intensity normalization (Part 2) - MRI",
    "section": "Quantitative sequences",
    "text": "Quantitative sequences\nAlthough T1- and T2-weighted images remained the most popular diagnostic sequence used by radiologist, new MRI sequences has been investigated including those that operates in a standard scales.\n\nExamples of quantitative sequences\n\nDiffusion weighted imaging\nProton Density (PD)\nMagnetic Resonance Spectroscopy (MRS)\nArterial Spin Labeling\nFunctional MRI\n\n\n\n\n\n\n\nNote\n\n\n\nWhile the physical factors fosters the inter-scanner reproducibility of intensity values for these sequences, environmental factors and batch effect remains and intensity normalization is still valuable.",
    "crumbs": [
      "Home",
      "Posts",
      "Medical Imaging",
      "Intensity Normalization",
      "Intensity normalization (Part 2) - MRI"
    ]
  },
  {
    "objectID": "posts/medical-imaging/intensity-normalization/02_intensity-normalization-mri.html#neuro",
    "href": "posts/medical-imaging/intensity-normalization/02_intensity-normalization-mri.html#neuro",
    "title": "Intensity normalization (Part 2) - MRI",
    "section": "Neuro",
    "text": "Neuro\nThere is a long history quantitative analysis of the brain using MRI scans. I would say with some confidence that normalization of MRI is most established for MRI brain scans. This is because the white-matter and gray-matter contrast is relatively more consistent between people when compared to other structures such as fat and muscles. Majority of the method developed for normalizing brain scan is based on white- and gray-matter contrast.\n\nNyul Normalization\nOne of the most utilized method in Nero for normalization is Nyúl normalization, which is essentially a piecewise linear transform with reference to the histogram peaks representing white- and gray-matter of neural tissues.\n\n\nNyúl, L. G., Udupa, J. K., & Zhang, X. (2000). New variants of a method of MRI scale standardization. IEEE transactions on medical imaging, 19(2), 143-150.\nThis method works best with a brain mask, but it is not absolutely necessary. Essentially, the histogram is\nFor this purpose I have developed a toolkit MRI Normalization Tools (mnts) to improve repeatability.",
    "crumbs": [
      "Home",
      "Posts",
      "Medical Imaging",
      "Intensity Normalization",
      "Intensity normalization (Part 2) - MRI"
    ]
  },
  {
    "objectID": "posts/MISC/fourier-transform/index.html",
    "href": "posts/MISC/fourier-transform/index.html",
    "title": "Imaging - Convolution between an image and a filtering kernel",
    "section": "",
    "text": "The equivalence of convolution in the image domain and element-wise multiplication in the Fourier domain is a foundational concept in image processing, applicable even within discrete grid spaces. Textbooks often present this relationship as follows:\n\\[\n\\begin{align}\nI(\\vec{x}) = f(\\vec{x}) * g(\\vec{\\tau}) &= \\sum_{\\vec{\\tau}} f(\\vec{x} - \\vec{\\tau}) \\cdot g(\\vec{\\tau}) \\\\\n&= \\mathscr{F}^{-1}\\left\\{\\mathscr{F}[f(\\vec{x})] \\cdot \\mathscr{F}[g(\\vec{\\tau})]\\right\\} \\tag{1}\n\\end{align}\n\\]\nHere, \\(I(\\vec{x})\\) represents the resulting n-dimensional image after convolution, with \\(x_i \\in X\\) and \\(\\tau_i \\in T\\) such that the domain \\(T \\subseteq X\\). This formulation is straightforward when the convolution involves images of the same domain (i.e., identical size). However, the scenario of interest involves the special case where \\(T \\neq X\\), specifically \\(T \\subset X\\). The question then arises: how do we compute this\n\\[\n\\mathscr{F}[f(\\vec{x})] \\cdot \\mathscr{F}[g(\\vec{\\tau})]\n\\]\nPractically when the kernel \\(g(\\vec{\\tau})\\) and the image \\(f(\\vec{x})\\) differ in size?\nIn such cases, direct element-wise multiplication is not feasible. This dilemma is seldom addressed in textbooks, leaving a gap in practical understanding. So what can we do here?\nIn reality, you can thought of the scenario as convolving the image with a kernel that is centered and padded with zeros elsewhere. The most straightforward practical approach is to equalize the domains by padding the kernel \\(g(\\vec{\\tau})\\) with zeros, effectively expanding the domain \\(T\\) to match \\(X\\).\nThis method ensures that the Fourier transform of the padded kernel has the same dimensionality as the Fourier transform of the image, allowing for the required element-wise multiplication in the Fourier space.\n\n\n\n\n'''Import libraries'''\nimport numpy as np\nfrom IPython.display import *\nfrom skimage.color import rgb2gray\nfrom skimage.io import imread\nimport scipy.ndimage as ndimage\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n'''Load Images'''\n# Generate an example 512x512 image and 3x3 kernel\nimage = imread(\"./lena.png\").astype('float32')\nimage = rgb2gray(image[...,:3])\n\nplt.imshow(image,cmap='gray')\nplt.show()\n\n\n\n\nFig.1 Lena.png. We are using this image as our input.\n\n\n\n\ndef out_conv(im, kn):\n    \"\"\"Convolution\"\"\"\n    fftkern = np.fft.fftshift(np.fft.fft2(np.pad(kn,\n                            [[256-1, 256-2], [256-1,256-2]],\n                            constant_values=0,\n                            mode='constant')\n                    ))\n    fftimage = np.fft.fftshift(np.fft.fft2(im))\n\n    # Multiplication\n    fftoutput = fftkern*fftimage\n\n    # Inverse fourier transform\n    out = np.fft.ifftshift(np.fft.ifft2(np.fft.fftshift(fftoutput)))        \n    return np.real(out)\n\n# Define 3 by 3 Kernel\nkern = np.array([[1, 0, -1], [1, 0, -1], [1, 0, -1]], dtype=np.float32)\nioutput = out_conv(image, kern)\n\n# Scipy output as ground truth\nspoutput = ndimage.filters.convolve(image, kern)\n\nfig = plt.figure(figsize=(10, 5))\nax1, ax2 = [fig.add_subplot(i) for i in [121, 122]]\nax1.imshow(ioutput, cmap=\"Greys_r\")\nax2.imshow(spoutput, cmap=\"Greys_r\")\nax1.set_title(r\"$\\mathscr{F}[f(\\vec{x})]$ Result by us\")\nax2.set_title(r\"$\\mathscr{F}[f(\\vec{x})]$ Result by `scipy`\")\n\n\n# calculate and show the difference\ndiff = np.abs(spoutput - ioutput)\nfig = plt.figure(figsize=(5, 5))\nax1 = fig.add_subplot(111)\nax1.imshow(diff, cmap='jet', vmin=-10, vmax=10)\nax1.set_title(\"Real part differences between us and `scipy`\")\n\n# display(Markdown(\"## Results\"))\nplt.show()\n\n\n\n\n\n\n\n\n\n\nSeems like we have repeated the conv function scipy implemented! But did we really? Lets take closer to the edge of the image:\n\n\nCode\nfig, ax = plt.subplots(1, 2, figsize=(10, 5))\nax[0].imshow(spoutput[:25, :25], cmap='jet', vmin=-10, vmax=10)\nax[1].imshow(ioutput[:25, :25], cmap='jet', vmin=-10, vmax=10)\nax[0].set_title(\"`scipy` at the upper left corner\")\nax[1].set_title(\"Ours' at the upper left corner\")\nplt.show()\n\n\n\n\n\nFig.2 Difference between our implementation of convolution and scipy\n\n\n\n\nWow, what happened here? As you can see, our implementation differs with scipy at the edge of the output image. So what is actually happening? To keep things short, this has to do with how the kernel was convolved at the edge of the image. FFT was implemented based on the assumption that the image is periodic outside of the presented discrete domain \\(X\\). Therefore, when the convolve (in \\(x\\) space) reaches the edge, its wrapping back the other end of the image.\n\n\n\n\n\nThis is the most straightforward approach and the one used initially in this workbook, where zeros are added to the borders of the kernel image. Now remember that FFT requires the input to have a dimension as a power of 2, this means that for odd number sized kernels, the padding is always asymetric.\n\n\n\nWhen using FFT for convolution, some libraries and frameworks automatically handle padding. The input signals are implicitly padded with zeros to the next power of two or to a size that allows for the most efficient computation by the FFT algorithm.\n\n\n\nIn some cases, especially in neural networks, convolutions are performed without padding, known as ‘valid’ convolution. This approach results in an output that is smaller than the input because only the regions where the kernel and input fully overlap are computed.\n\n\n\nInstead of padding with zeros, the border values are reflected or mirrored across the edge. This can help reduce the artifacts introduced by the hard boundary of zero-padding and can be more appropriate for certain types of data, such as images.\n\n\n\nThis involves wrapping the signal around, effectively creating a toroidal topology. This is the inherent assumption in FFT-based convolution, but it can also be done explicitly in the spatial domain.\n\n\n\nThe edge values are replicated to the padded regions. This can sometimes produce more natural results than zero-padding, particularly for image processing tasks.\n\n\n\nUsed mainly in image inpainting, partial convolutions involve a mask that indicates which pixels are valid, allowing the convolution to be reweighted to ignore the contribution from missing or padded values.\n\n\n\nAlso known as atrous convolutions, these involve spacing out the kernel elements, which allows for a larger receptive field without increasing the computational complexity. This is not directly a padding method but can be used to reduce the need for padding by controlling the resolution at which feature maps are computed.\n\n\n\nBy skipping input values with a certain stride, these convolutions effectively downsample the input. This can also reduce the need for padding by adjusting the output size.",
    "crumbs": [
      "Home",
      "Posts",
      "MISC",
      "Imaging - Convolution between an image and a filtering kernel"
    ]
  },
  {
    "objectID": "posts/MISC/fourier-transform/index.html#zero-padding-explicit-padding",
    "href": "posts/MISC/fourier-transform/index.html#zero-padding-explicit-padding",
    "title": "Imaging - Convolution between an image and a filtering kernel",
    "section": "",
    "text": "This is the most straightforward approach and the one used initially in this workbook, where zeros are added to the borders of the kernel image. Now remember that FFT requires the input to have a dimension as a power of 2, this means that for odd number sized kernels, the padding is always asymetric.",
    "crumbs": [
      "Home",
      "Posts",
      "MISC",
      "Imaging - Convolution between an image and a filtering kernel"
    ]
  },
  {
    "objectID": "posts/MISC/fourier-transform/index.html#implicit-padding-in-fast-fourier-transform-fft",
    "href": "posts/MISC/fourier-transform/index.html#implicit-padding-in-fast-fourier-transform-fft",
    "title": "Imaging - Convolution between an image and a filtering kernel",
    "section": "",
    "text": "When using FFT for convolution, some libraries and frameworks automatically handle padding. The input signals are implicitly padded with zeros to the next power of two or to a size that allows for the most efficient computation by the FFT algorithm.",
    "crumbs": [
      "Home",
      "Posts",
      "MISC",
      "Imaging - Convolution between an image and a filtering kernel"
    ]
  },
  {
    "objectID": "posts/MISC/fourier-transform/index.html#valid-convolution",
    "href": "posts/MISC/fourier-transform/index.html#valid-convolution",
    "title": "Imaging - Convolution between an image and a filtering kernel",
    "section": "",
    "text": "In some cases, especially in neural networks, convolutions are performed without padding, known as ‘valid’ convolution. This approach results in an output that is smaller than the input because only the regions where the kernel and input fully overlap are computed.",
    "crumbs": [
      "Home",
      "Posts",
      "MISC",
      "Imaging - Convolution between an image and a filtering kernel"
    ]
  },
  {
    "objectID": "posts/MISC/fourier-transform/index.html#reflective-or-symmetric-padding",
    "href": "posts/MISC/fourier-transform/index.html#reflective-or-symmetric-padding",
    "title": "Imaging - Convolution between an image and a filtering kernel",
    "section": "",
    "text": "Instead of padding with zeros, the border values are reflected or mirrored across the edge. This can help reduce the artifacts introduced by the hard boundary of zero-padding and can be more appropriate for certain types of data, such as images.",
    "crumbs": [
      "Home",
      "Posts",
      "MISC",
      "Imaging - Convolution between an image and a filtering kernel"
    ]
  },
  {
    "objectID": "posts/MISC/fourier-transform/index.html#circular-padding",
    "href": "posts/MISC/fourier-transform/index.html#circular-padding",
    "title": "Imaging - Convolution between an image and a filtering kernel",
    "section": "",
    "text": "This involves wrapping the signal around, effectively creating a toroidal topology. This is the inherent assumption in FFT-based convolution, but it can also be done explicitly in the spatial domain.",
    "crumbs": [
      "Home",
      "Posts",
      "MISC",
      "Imaging - Convolution between an image and a filtering kernel"
    ]
  },
  {
    "objectID": "posts/MISC/fourier-transform/index.html#replicate-padding",
    "href": "posts/MISC/fourier-transform/index.html#replicate-padding",
    "title": "Imaging - Convolution between an image and a filtering kernel",
    "section": "",
    "text": "The edge values are replicated to the padded regions. This can sometimes produce more natural results than zero-padding, particularly for image processing tasks.",
    "crumbs": [
      "Home",
      "Posts",
      "MISC",
      "Imaging - Convolution between an image and a filtering kernel"
    ]
  },
  {
    "objectID": "posts/MISC/fourier-transform/index.html#partial-convolutions",
    "href": "posts/MISC/fourier-transform/index.html#partial-convolutions",
    "title": "Imaging - Convolution between an image and a filtering kernel",
    "section": "",
    "text": "Used mainly in image inpainting, partial convolutions involve a mask that indicates which pixels are valid, allowing the convolution to be reweighted to ignore the contribution from missing or padded values.",
    "crumbs": [
      "Home",
      "Posts",
      "MISC",
      "Imaging - Convolution between an image and a filtering kernel"
    ]
  },
  {
    "objectID": "posts/MISC/fourier-transform/index.html#dilated-convolutions",
    "href": "posts/MISC/fourier-transform/index.html#dilated-convolutions",
    "title": "Imaging - Convolution between an image and a filtering kernel",
    "section": "",
    "text": "Also known as atrous convolutions, these involve spacing out the kernel elements, which allows for a larger receptive field without increasing the computational complexity. This is not directly a padding method but can be used to reduce the need for padding by controlling the resolution at which feature maps are computed.",
    "crumbs": [
      "Home",
      "Posts",
      "MISC",
      "Imaging - Convolution between an image and a filtering kernel"
    ]
  },
  {
    "objectID": "posts/MISC/fourier-transform/index.html#strided-convolutions",
    "href": "posts/MISC/fourier-transform/index.html#strided-convolutions",
    "title": "Imaging - Convolution between an image and a filtering kernel",
    "section": "",
    "text": "By skipping input values with a certain stride, these convolutions effectively downsample the input. This can also reduce the need for padding by adjusting the output size.",
    "crumbs": [
      "Home",
      "Posts",
      "MISC",
      "Imaging - Convolution between an image and a filtering kernel"
    ]
  },
  {
    "objectID": "posts/stochastic/incremental-process.html",
    "href": "posts/stochastic/incremental-process.html",
    "title": "Stochastic - Python Example of a Random Walk Process",
    "section": "",
    "text": "An independent incremental process is a type of stochastic process where the increments (or differences) between values at different times are independent random variables. This means that the value of an increment at one time point does not influence the value of another increment at a different time point. In other words, what happens from one step to the next has no memory of what has happened before.\n\n\n\nFor a stochastic process \\(X_T\\) with sample space \\(t_1, t_2, \\dots, t_n \\in T\\) such that \\(t_1 &lt; t_2 &lt; \\dots &lt; t_n\\) then\n\\[\\displaystyle X_{t2} - X_{t1} , X_{t3} - X_{t2} , \\dots , X_{tn} - X_{tn-1}\\]\nare independent.\n\n\n\n\n\nA stationary incremental process, on the other hand, is a stochastic process where the statistical properties of the increments are time-invariant. This means that the probability distribution of the increments does not change over time. For instance, if you take any two increments of the same length at different points in the process, they should have the same probability distribution.\n\n\n\nFor an independent incremental process \\(X_T\\), it is said to be stationary if it satisfy:\n\\[\\displaystyle \\{X_{i + h} - X_{i-1 + h}\\} = \\{X_{i} - X_{i-1}\\};\\forall i, h\\in\\mathbb{Z}^+\\]\nMore formally, \\(\\forall t, s, h \\in \\mathbb{Z}^+\\), where \\(0 \\leq s &lt; t\\), the following holds:\n\\[F_{X_{t+h} - X_{s+h}}(x) = F_{X_t - X_s}(x)\\]\nwhere \\(F_{X_t - X_s}(x)\\) is the cumulative distribution function of the increment \\(X_{t} - X(s)\\).\n\n\n\n\n\n\n\nThe simple random walk process covered here is a classical example of incremental process. Whether or not it is independent and stationary is dictated by whether each steps are independent of each other, and if the distribution from which the steps are sampled is identical for every step.\n\n\n\n\n\n\nA process can be a stationary incremental process without being an independent incremental process. Consider a moving average (MA) process in time series analysis, which is often used to model time series data. In an MA process, the current value is derived from a combination of current and past random shocks (errors), with the random shocks being independent and identically distributed. The increments (changes) in the process could have a constant variance, making them stationary, but they are not independent because the value at time \\(t\\) is directly influenced by the random shock at time \\(t-1\\).\nLet \\(X(t)\\) be the MA and \\(\\epsilon(t)\\) be the shock at time \\(t\\), both being random variables and \\(\\epsilon(t) \\in \\mathscr{N}(0, 1)\\),\n\\[ X(t) = \\epsilon(t) + \\theta \\cdot \\epsilon(t-1) \\]\nEvidently \\(X(t)\\) depends on \\(t\\) because it has memory of \\(X(t-1)\\).\nHere’s a simple example of an MA(1) process, which is a moving average process of order 1:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\nn = 100  # number of points\nerrors = np.random.normal(0, 1, n)  # independent, identically distributed errors\ntheta = 0.5  # parameter of the MA(1) process\n\n# MA(1) process\nY = np.empty(n)\nY[0] = errors[0]\nfor t in range(1, n):\n    Y[t] = errors[t] + theta * errors[t-1]  # dependent increments\n\n# Y[t] - Y[t-1] are the increments and are stationary but not independent because Y[t] depends on errors[t-1]\n\nfig, ax = plt.subplots(1, 1, figsize=(7, 5))\nax.plot(Y)\nplt.show()\n\n\n\n\n\n\n\n\nIn this MA(1) model, the increments are stationary because the distribution of Y[t] - Y[t-1] does not change over time. Each increment has a mean of zero and a constant variance (which can be calculated based on the variance of the errors and the parameter theta). However, the increments are not independent because each Y[t] is dependent on the error term at t-1 (errors[t-1]).\nYou can see how the path tends to return to the average (\\(\\mu = 0\\)) if it has deviate from it.\nThis demonstrates that a process can have stationary increments with a consistent statistical distribution while still exhibiting dependence between those increments.",
    "crumbs": [
      "Home",
      "Posts",
      "Stochastic",
      "Stochastic - Python Example of a Random Walk Process"
    ]
  },
  {
    "objectID": "posts/stochastic/incremental-process.html#independent-incremental-process",
    "href": "posts/stochastic/incremental-process.html#independent-incremental-process",
    "title": "Stochastic - Python Example of a Random Walk Process",
    "section": "",
    "text": "An independent incremental process is a type of stochastic process where the increments (or differences) between values at different times are independent random variables. This means that the value of an increment at one time point does not influence the value of another increment at a different time point. In other words, what happens from one step to the next has no memory of what has happened before.\n\n\n\nFor a stochastic process \\(X_T\\) with sample space \\(t_1, t_2, \\dots, t_n \\in T\\) such that \\(t_1 &lt; t_2 &lt; \\dots &lt; t_n\\) then\n\\[\\displaystyle X_{t2} - X_{t1} , X_{t3} - X_{t2} , \\dots , X_{tn} - X_{tn-1}\\]\nare independent.",
    "crumbs": [
      "Home",
      "Posts",
      "Stochastic",
      "Stochastic - Python Example of a Random Walk Process"
    ]
  },
  {
    "objectID": "posts/stochastic/incremental-process.html#stationary-independent-incremental-process",
    "href": "posts/stochastic/incremental-process.html#stationary-independent-incremental-process",
    "title": "Stochastic - Python Example of a Random Walk Process",
    "section": "",
    "text": "A stationary incremental process, on the other hand, is a stochastic process where the statistical properties of the increments are time-invariant. This means that the probability distribution of the increments does not change over time. For instance, if you take any two increments of the same length at different points in the process, they should have the same probability distribution.\n\n\n\nFor an independent incremental process \\(X_T\\), it is said to be stationary if it satisfy:\n\\[\\displaystyle \\{X_{i + h} - X_{i-1 + h}\\} = \\{X_{i} - X_{i-1}\\};\\forall i, h\\in\\mathbb{Z}^+\\]\nMore formally, \\(\\forall t, s, h \\in \\mathbb{Z}^+\\), where \\(0 \\leq s &lt; t\\), the following holds:\n\\[F_{X_{t+h} - X_{s+h}}(x) = F_{X_t - X_s}(x)\\]\nwhere \\(F_{X_t - X_s}(x)\\) is the cumulative distribution function of the increment \\(X_{t} - X(s)\\).",
    "crumbs": [
      "Home",
      "Posts",
      "Stochastic",
      "Stochastic - Python Example of a Random Walk Process"
    ]
  },
  {
    "objectID": "posts/stochastic/incremental-process.html#random-walk",
    "href": "posts/stochastic/incremental-process.html#random-walk",
    "title": "Stochastic - Python Example of a Random Walk Process",
    "section": "",
    "text": "The simple random walk process covered here is a classical example of incremental process. Whether or not it is independent and stationary is dictated by whether each steps are independent of each other, and if the distribution from which the steps are sampled is identical for every step.",
    "crumbs": [
      "Home",
      "Posts",
      "Stochastic",
      "Stochastic - Python Example of a Random Walk Process"
    ]
  }
]