{
  "hash": "c95149bfdd70821b37e6cbba8f910252",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Intensity normalization - General Info\"\nauthor: \"MLun Wong\"\ndate: \"2024-01-22\"\ncategories: [DICOM, normalization, imaging]\nformat:\n  html:\n    toc: true\n# jupyter: conda-env-vtk-py\n---\n\n# Introduction\n\nThe quantitative analysis of medical images is predicated on a fundamental assumption: the properties being analyzed are consistent across different patients, thereby allowing patterns identified within one patient group to be applicable to another without the need for transformation. In practice, however, this assumption is frequently violated due to a multitude of environmental variables that are challenging to control, including temperature, pressure, exposure to light, level of magnetism, air circulation ...etc. In the field of genomics, the influence of such variations is acknowledged as the \"batch effect,\" referring to the inherent systematic biases in each batch of DNA sequencing data that are unrelated to the histological or pathological features of interest. Although imaging lacks a specific term for this phenomenon, it is subject to a comparable effect, compounded by additional physical constraints inherent to the imaging techniques themselves.\n\nTake, for instance, the MRI weighted sequences. The intensities of MRI T1-weighted and T2-weighted images are described as \"weighted\" precisely because they do not maintain a consistent correlation with a fixed intensity scale. As a result, identical tissue types may exhibit variable intensity values across different scans. Contrastingly, CT imaging does not typically reflect this issue, as the intensities in CT images are anchored to the physical densities of the scanned objects and are denoted in Hounsfield Units (HU). Despite this, CT imaging is not entirely immune to environmental influences.\n\nGiven these challenges, normalizing the intensity of medical images becomes an integral step prior to any quantitative analysis. Such normalization ensures that the patterns discerned from one cohort of patients can be validly extended to another, enhancing the reliability and translatability of the findings.\n\n# General idea for intensity normalization\n\n## Assumption of a reference standard\n\n-   Voxels sharing identical intensity values within a single image should maintain their equivalence in intensity following the normalization process.\n\n-   Identical tissues should exhibit consistent intensity values both within the same scanner and across different scanners when the same or equivalent sequences are used.\n\n\n```{=html}\n<details> <summary><b>Advance discussion</b></summary>\n```\n\n::: callout-tip\nWe mentioned we think that the same type of tissue should show up with the same value in MRI scans, but people are different, and so is their tissue. Some tissues, like brain tissue, don't vary much between people, so brain scan values are pretty consistent and well-adjusted in neuroscience studies. But for other tissues, like tumors, each one is unique. We can't assume they're all the same. We don't have a solid reference to confirm if our assumptions about tissue values are right or wrong. What matters most is if adjusting these values helps us better understand and measure the features we see in scans. So, it's important to be flexible with these assumptions.\n:::\n\n</details>\n\n## Identify a mapping for intensity normalization\n\nFormally, normalization is the process of finding a map between the instance distribution $d[I(x), v]$ to the reference dsitribution $\\mathscr{D}(v)$:\n\n$$\n\\text{Normalizaiton}[I(x)]: d[I(x), v]\\mapsto D(v)\n$$\n\nwhere $d[I(x), v]$ is the distribution of image $I(x)$ and $x\\in X$ with $X$ being the domain where the Image is defined.\n\nNow consider everybody is unique, it is not probably to assume everyone's intensity distribution must fit perfectly to the same reference standard. There should be constrains that is supported by the physical properties of the tissue:\n\n$$\n\\arg\\min_{\\lambda} d[\\text{Normalization}[I(x);\\lambda], v] - D(v) \n$$\n\n$$\n\\text{Subjected to: } f[I(x), \\lambda] = 0\n$$\n\nWhere $\\lambda$ is the normalization parameters and $f(I, \\lambda)$ is arbitrary constrain.\n\n## Histogram-based intensity mapping\n\nMost intensity normalization algorithms complies with the first assumption would aim to create an intensity mapping $N(\\textbf{M}):=I\\mapsto J$, $I$ is the domain (intensity range) of input image batch and $J$ is the normalized intensity domain, $\\textbf{M}$ is the input image batch for normalization.\n\n\n\n::: {#cell-fig-lena-histogram .cell execution_count=2}\n\n::: {.cell-output .cell-output-display}\n![Example of histogram of gray-scale Lena transformed](intensity-normalization-toc_files/figure-html/fig-lena-histogram-output-1.png){#fig-lena-histogram width=615 height=449}\n:::\n:::\n\n\n## Methodologies\n\nThere are really many ways to construct the histogram mapping, most commonly involve linear mapping and piecewise-linear mapping.\n\n### Linear mapping\n\nLinear mapping is easy to understand, it means the the same transform is applied across the entire intensity domain. This normalize the tissue value of the foreground tissues to have a standardized mean of $\\mu$ and a variance of $\\sigma$:\n\n$$\nI'_{ij}=\\frac{I_{ij} - \\mu}{\\sigma}\n$$\n\nwhere $I_{ij}$ is the intensity at index $(i,j)$.\n\n#### Deciding $\\mu$ and $\\sigma$\n\nThe main idea of this linear mapping is to shift, expand or compress the entire histogram. You would have do decide how based on the property of the scanned tissues. Here are some common choices.\n\n##### Z-score normalization\n\nThe mean and variance can be arbitrarily defined. When we set $\\mu=\\text{E}|\\textbf{I}(X)|$ and $\\sigma = \\text{Var}[\\textbf{I}(X)]$; $X\\subset U$ is the domain of foreground tissues in the image, this become **Z-score normalization**, which sets all images' mean and variance to 0 and 1, respectively. This corresponds to the `StandardScaler` in the package `sklearn`. Note that `sklearn` does not deal with foreground tissue masks.\n\nAlternatively, you can also chose to reference the averaged foreground intensity across the entire batch, i.e., calculating $\\mu$ and $\\sigma$ from the entire dataset. This averages the mean and variance across all data.\n\n##### Min-max scaling\n\nThis aims to scale the minimal and maximum intensity value in each image to be a fixed range, say $[a, b]$. Then we can set $\\mu=[\\max(\\textbf{I}) - \\min(\\textbf{I})]/2$ and $\\sigma=\\text{Var}[\\textbf{I}] / (b -a)$.\n\n::: {.callout-caution appearance=\"simple\"}\nThis method is very sensitive to outliers. You should make sure the outliers are cleaned if you want to use this method.\n:::\n\n##### IQR scaling\n\nScales the histogram based on location of the quartiles. Simply set $\\mu=\\text{median}[\\textbf{I}(X)]$ and\n\n$$\n\\sigma=\\frac{\\text{Var}[\\textbf{I}(X)]}{Q[\\textbf{I}(X),0.75] - Q[\\textbf{I}(X),0.25]}\n$$\n\nThis method moves median to 0 and the IQR to $[-0.5, 0.5]$. This is suitable for images with very skewed histogram and with a lot of outliers.\n\n### Piece-wise linear mapping\n\nNow the above linear mappings mainly cater for histogram with roughly a normal distribution. This is often not the case. As you can see in the histogram of lena @fig-lena-histogram, there are plenty of peaks at different locations. Now if we assume these peaks in different people represents the same set of tissues, because of the contrast between tissues is consistent, we can further align the peaks to normalize the image. This requires shifts of the histogram at different peak locations, hence, piece-wise linear mapping means the intensity domain is partitioned into multiple segments, and different linear transform is applied to these segments.\n\n$$\n\\begin{matrix}\nI'(X_i)=N_i[I(X_i)] & \\forall i\\in \\mathbb{Z}^+, x_i\\in X, & \\text{where } x_i \\leq x <x_{i+1}\n\\end{matrix}\n$$\n\n$I'(X_i)$ is the normalized image corresponding to normalization of domain $X_i$, i.e., the $i$-th segment in the intensity profile, and $N_i(.)$ is the normalization operation for the $i$-th segment.\n\nPiece-wise mapping allows for a better\n\n# Modality-specific normalization\n\nThere are no general method for intensity normalization because different medical image modalities has different intrinsic physics invovled.\n\n## MRI\n\nSee [Intensity normalization - MRI](./intensity-normalization-mri.qmd)\n\n",
    "supporting": [
      "intensity-normalization-toc_files"
    ],
    "filters": [],
    "includes": {}
  }
}